<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>7. Run A Program</title><link rel="stylesheet" type="text/css" href="style.css"><meta name="generator" content="DocBook XSL Stylesheets V1.79.1"><link rel="home" href="index.html" title="The Beowulf HOWTO"><link rel="up" href="index.html" title="The Beowulf HOWTO"><link rel="prev" href="ar01s06.html" title="6. Verification"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">7. Run A Program</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="ar01s06.html">Prev</a> </td><th width="60%" align="center"> </th><td width="20%" align="right"> </td></tr></table><hr></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="idm202"></a>7. Run A Program</h2></div></div></div><p>Once you can do all the tests shown above, you should be able to run
    a program. From here on in, the instructions are lam specific.</p><p>Go back to the head node, log in as wolf, and enter the following
    commands:</p><pre class="screen">cat &gt; /nnt/wolf/lamhosts 
wolf01 
wolf02 
wolf03 
wolf04 
&lt;control d&gt;</pre><p>Go to the lam examples directory, and compile "hello.c":</p><pre class="screen">mpicc -o hello hello.c 
cp hello /mnt/wolf </pre><p>Then, as shown in the lam documentation, start up lam:</p><pre class="screen">[wolf@wolf00 wolf]$ lamboot -v lamhosts 
LAM 7.0/MPI 2 C++/ROMIO - Indiana University 
n0&lt;2572&gt; ssi:boot:base:linear: booting n0 (wolf00) 
n0&lt;2572&gt; ssi:boot:base:linear: booting n1 (wolf01) 
n0&lt;2572&gt; ssi:boot:base:linear: booting n2 (wolf02) 
n0&lt;2572&gt; ssi:boot:base:linear: booting n3 (wolf04) 
n0&lt;2572&gt; ssi:boot:base:linear: finished</pre><p>So we are now finally ready to run an app. [Remember, I am using
    lam; your message passing interface may have different syntax].</p><pre class="screen">[wolf@wolf00 wolf]$ mpirun n0-3 /mnt/wolf/hello 
Hello, world! I am 0 of 4 
Hello, world! I am 3 of 4 
Hello, world! I am 2 of 4 
Hello, world! I am 1 of 4 
[wolf@wolf00 wolf]$</pre><p>Recall I mentioned the use of NFS above. I am telling the nodes to
    all use the nfs shared directory, which will bottleneck when using a
    larger number of boxes. You could easily copy the executable to each box,
    and in the mpirun command, specify node local directories: mpirun n0-3
    /home/wolf/hello. The prerequisite for this is to have all the files
    available locally. In fact I have done this, and it worked better than
    using the nfs shared executable. Of course this theory breaks down if my
    cluster application needs to modify a file shared across the
    cluster.</p></div><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="ar01s06.html">Prev</a> </td><td width="20%" align="center"> </td><td width="40%" align="right"> </td></tr><tr><td width="40%" align="left" valign="top">6. Verification </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> </td></tr></table></div></body></html>
