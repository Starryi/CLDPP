<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Traffic Control HOWTO</title><link rel="stylesheet" type="text/css" href="style.css"><meta name="generator" content="DocBook XSL Stylesheets V1.79.1"><meta name="description" content="Traffic control encompasses the sets of mechanisms and operations by which packets are queued for transmission/reception on a network interface. The operations include enqueuing, policing, classifying, scheduling, shaping and dropping. This HOWTO provides an introduction and overview of the capabilities and implementation of traffic control under Linux."></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div lang="en" class="article"><div class="titlepage"><div><div><h2 class="title"><a name="idm1"></a>Traffic Control HOWTO</h2></div><div><h3 class="subtitle"><i>Version 1.1
</i></h3></div><div><div class="authorgroup"><div class="author"><h3 class="author"><span class="firstname">Martin</span> <span class="othername">A.</span> <span class="surname">Brown</span></h3><div class="affiliation"><span class="orgname">
          <a class="ulink" href="http://linux-ip.net/" target="_top">linux-ip.net</a>
          <br></span><div class="address"><p><code class="email">&lt;<a class="email" href="mailto:martin@linux-ip.net">martin@linux-ip.net</a>&gt;</code></p></div></div></div><div class="author"><h3 class="author"><span class="firstname">Federico</span> <span class="surname">Bolelli</span></h3><div class="affiliation"><span class="orgname">
                 <a class="ulink" href="http://www.unimore.it/" target="_top">Unimore</a>
            <br></span><div class="address"><p><code class="email">&lt;<a class="email" href="mailto:167439@studenti.unimore.it">167439@studenti.unimore.it</a>&gt;</code></p></div></div></div><div class="author"><h3 class="author"><span class="firstname">Natale</span> <span class="surname">Patriciello</span></h3><div class="affiliation"><span class="orgname">
                 <a class="ulink" href="http://www.unimore.it/" target="_top">Unimore</a>
            <br></span><div class="address"><p><code class="email">&lt;<a class="email" href="mailto:natale.patriciello@unimore.it">natale.patriciello@unimore.it</a>&gt;</code></p></div></div></div></div></div><div><div class="legalnotice"><a name="legalnotice"></a><p>© 2016, 2006, Martin A. Brown</p><div class="blockquote"><blockquote class="blockquote"><p>
          Permission is granted to copy, distribute and/or modify this
          document under the terms of the GNU Free Documentation License,
          Version 1.1 or any later version published by the Free Software
          Foundation; with no invariant sections, with no Front-Cover Texts,
          with no Back-Cover Text.  A copy of the license is located at
          <a class="ulink" href="http://www.gnu.org/licenses/fdl.html" target="_top">http://www.gnu.org/licenses/fdl.html</a>.
        </p></blockquote></div></div></div><div><p class="pubdate">2016-03-11
</p></div><div><div class="revhistory"><table style="border-style:solid; width:100%;" summary="Revision History"><tr><th align="left" valign="top" colspan="3"><b>Revision History</b></th></tr><tr><td align="left">Revision 1.1.0</td><td align="left">2016-01-30</td><td align="left">MAB</td></tr><tr><td align="left" colspan="3">Incorporate new qdisc description sections written by Federico Bollelli and Natale
        Patriciello</td></tr><tr><td align="left">Revision 1.0.2</td><td align="left">2006-10-28</td><td align="left">MAB</td></tr><tr><td align="left" colspan="3">Add references to HFSC, alter author email addresses</td></tr><tr><td align="left">Revision 1.0.1</td><td align="left">2003-11-17</td><td align="left">MAB</td></tr><tr><td align="left" colspan="3">Added link to Leonardo Balliache's documentation</td></tr><tr><td align="left">Revision 1.0</td><td align="left">2003-09-24</td><td align="left">MAB</td></tr><tr><td align="left" colspan="3">reviewed and approved by TLDP</td></tr><tr><td align="left">Revision 0.7</td><td align="left">2003-09-14</td><td align="left">MAB</td></tr><tr><td align="left" colspan="3">incremental revisions, proofreading, ready for TLDP</td></tr><tr><td align="left">Revision 0.6</td><td align="left">2003-09-09</td><td align="left">MAB</td></tr><tr><td align="left" colspan="3">minor editing, corrections from Stef Coene</td></tr><tr><td align="left">Revision 0.5</td><td align="left">2003-09-01</td><td align="left">MAB</td></tr><tr><td align="left" colspan="3">HTB section mostly complete, more diagrams, LARTC pre-release</td></tr><tr><td align="left">Revision 0.4</td><td align="left">2003-08-30</td><td align="left">MAB</td></tr><tr><td align="left" colspan="3">added diagram</td></tr><tr><td align="left">Revision 0.3</td><td align="left">2003-08-29</td><td align="left">MAB</td></tr><tr><td align="left" colspan="3">substantial completion of classless, software, rules,
          elements and components sections</td></tr><tr><td align="left">Revision 0.2</td><td align="left">2003-08-23</td><td align="left">MAB</td></tr><tr><td align="left" colspan="3">major work on overview, elements, components and
          software sections</td></tr><tr><td align="left">Revision 0.1</td><td align="left">2003-08-15</td><td align="left">MAB</td></tr><tr><td align="left" colspan="3">initial revision (outline complete)</td></tr></table></div></div><div><div class="abstract"><p class="title"><b>Abstract</b></p><p>
        Traffic control encompasses the sets of mechanisms and operations by
        which packets are queued for transmission/reception on a network
        interface.  The operations include enqueuing, policing, classifying,
        scheduling, shaping and dropping.  This HOWTO provides an introduction
        and overview of the capabilities and implementation of traffic control
        under Linux.
      </p></div></div></div><hr></div><div class="toc"><p><b>Table of Contents</b></p><dl class="toc"><dt><span class="section"><a href="#intro">1. Introduction to Linux Traffic Control</a></span></dt><dd><dl><dt><span class="section"><a href="#i-assumptions">1.1. Target audience and assumptions about the reader</a></span></dt><dt><span class="section"><a href="#i-conventions">1.2. Conventions</a></span></dt><dt><span class="section"><a href="#i-recommendation">1.3. Recommended approach</a></span></dt><dt><span class="section"><a href="#i-missing">1.4. Missing content, corrections and feedback</a></span></dt></dl></dd><dt><span class="section"><a href="#overview">2. Overview of Concepts</a></span></dt><dd><dl><dt><span class="section"><a href="#o-what-is">2.1. What is it?</a></span></dt><dt><span class="section"><a href="#o-why-use">2.2. Why use it?</a></span></dt><dt><span class="section"><a href="#o-advantages">2.3. Advantages</a></span></dt><dt><span class="section"><a href="#o-disadvantages">2.4. Disdvantages</a></span></dt><dt><span class="section"><a href="#o-queues">2.5. Queues</a></span></dt><dt><span class="section"><a href="#o-flows">2.6. Flows</a></span></dt><dt><span class="section"><a href="#o-tokens">2.7. Tokens and buckets</a></span></dt><dt><span class="section"><a href="#o-packets">2.8. Packets and frames</a></span></dt><dt><span class="section"><a href="#o-nic">2.9. NIC, Network Interface Controller</a></span></dt><dt><span class="section"><a href="#o-starv-lat">2.10. Starvation and Latency</a></span></dt><dt><span class="section"><a href="#o-throughput-latency">2.11. Relationship between throughput and latency</a></span></dt></dl></dd><dt><span class="section"><a href="#elements">3. Traditional Elements of Traffic Control</a></span></dt><dd><dl><dt><span class="section"><a href="#e-shaping">3.1. Shaping</a></span></dt><dt><span class="section"><a href="#e-scheduling">3.2. Scheduling</a></span></dt><dt><span class="section"><a href="#e-classifying">3.3. Classifying</a></span></dt><dt><span class="section"><a href="#e-policing">3.4. Policing</a></span></dt><dt><span class="section"><a href="#e-dropping">3.5. Dropping</a></span></dt><dt><span class="section"><a href="#e-marking">3.6. Marking</a></span></dt></dl></dd><dt><span class="section"><a href="#components">4. Components of Linux Traffic Control</a></span></dt><dd><dl><dt><span class="section"><a href="#c-qdisc">4.1. <code class="constant">qdisc</code></a></span></dt><dt><span class="section"><a href="#c-class">4.2. <code class="constant">class</code></a></span></dt><dt><span class="section"><a href="#c-filter">4.3. <code class="constant">filter</code></a></span></dt><dt><span class="section"><a href="#c-classifier">4.4. classifier</a></span></dt><dt><span class="section"><a href="#c-police">4.5. policer</a></span></dt><dt><span class="section"><a href="#c-drop">4.6. <code class="constant">drop</code></a></span></dt><dt><span class="section"><a href="#c-handle">4.7. <code class="constant">handle</code></a></span></dt><dt><span class="section"><a href="#c-txqueuelen">4.8. <code class="constant">txqueuelen</code></a></span></dt><dt><span class="section"><a href="#c-driver-queue">4.9. Driver Queue (aka ring buffer)</a></span></dt><dt><span class="section"><a href="#c-bql">4.10. Byte Queue Limits (<acronym class="acronym">BQL</acronym>)</a></span></dt></dl></dd><dt><span class="section"><a href="#software">5. Software and Tools</a></span></dt><dd><dl><dt><span class="section"><a href="#s-kernel">5.1. Kernel requirements</a></span></dt><dt><span class="section"><a href="#s-iproute2">5.2. <span class="command"><strong>iproute2</strong></span> tools (<span class="command"><strong>tc</strong></span>)</a></span></dt><dt><span class="section"><a href="#s-tcng">5.3. <span class="command"><strong>tcng</strong></span>, Traffic Control Next Generation</a></span></dt><dt><span class="section"><a href="#s-netfilter">5.4. Netfilter</a></span></dt><dt><span class="section"><a href="#s-imq">5.5. IMQ, Intermediate Queuing device</a></span></dt><dt><span class="section"><a href="#s-ethtool">5.6. <code class="constant">ethtool,</code> Driver Queue</a></span></dt></dl></dd><dt><span class="section"><a href="#classless-qdiscs">6. Classless Queuing Disciplines (<code class="constant">qdisc</code>s)</a></span></dt><dd><dl><dt><span class="section"><a href="#qs-fifo">6.1. FIFO, First-In First-Out (<code class="constant">pfifo</code> and <code class="constant">bfifo</code>)</a></span></dt><dt><span class="section"><a href="#qs-pfifo_fast">6.2. <code class="constant">pfifo_fast</code>, the default Linux qdisc</a></span></dt><dt><span class="section"><a href="#qs-sfq">6.3. SFQ, Stochastic Fair Queuing</a></span></dt><dt><span class="section"><a href="#qs-esfq">6.4. ESFQ, Extended Stochastic Fair Queuing</a></span></dt><dt><span class="section"><a href="#qs-red">6.5. RED,Random Early Drop</a></span></dt><dt><span class="section"><a href="#qs-gred">6.6. GRED, Generic Random Early Drop</a></span></dt><dt><span class="section"><a href="#qs-tbf">6.7. TBF, Token Bucket Filter</a></span></dt></dl></dd><dt><span class="section"><a href="#classful-qdiscs">7. Classful Queuing Disciplines (<code class="constant">qdisc</code>s)</a></span></dt><dd><dl><dt><span class="section"><a href="#qc-htb">7.1. HTB, Hierarchical Token Bucket</a></span></dt><dt><span class="section"><a href="#qc-hfsc">7.2. HFSC, Hierarchical Fair Service Curve</a></span></dt><dt><span class="section"><a href="#qc-prio">7.3. PRIO, priority scheduler</a></span></dt><dt><span class="section"><a href="#qc-cbq">7.4. CBQ, Class Based Queuing (<acronym class="acronym">CBQ</acronym>)</a></span></dt><dt><span class="section"><a href="#qc-wrr">7.5. WRR, Weighted Round Robin</a></span></dt></dl></dd><dt><span class="section"><a href="#rules">8. Rules, Guidelines and Approaches</a></span></dt><dd><dl><dt><span class="section"><a href="#r-general">8.1. General Rules of Linux Traffic Control</a></span></dt><dt><span class="section"><a href="#r-known-bandwidth">8.2. Handling a link with a known bandwidth</a></span></dt><dt><span class="section"><a href="#r-unknown-bandwidth">8.3. Handling a link with a variable (or unknown) bandwidth</a></span></dt><dt><span class="section"><a href="#r-sharing-flows">8.4. Sharing/splitting bandwidth based on flows</a></span></dt><dt><span class="section"><a href="#r-sharing-ips">8.5. Sharing/splitting bandwidth based on IP</a></span></dt></dl></dd><dt><span class="section"><a href="#scripts">9. Scripts for use with QoS/Traffic Control</a></span></dt><dd><dl><dt><span class="section"><a href="#sc-wondershaper">9.1. wondershaper</a></span></dt><dt><span class="section"><a href="#sc-myshaper">9.2. ADSL Bandwidth HOWTO script (<code class="filename">myshaper</code>)</a></span></dt><dt><span class="section"><a href="#sc-htb.init">9.3. <code class="filename">htb.init</code></a></span></dt><dt><span class="section"><a href="#sc-tcng.init">9.4. <code class="filename">tcng.init</code></a></span></dt><dt><span class="section"><a href="#sc-cbq.init">9.5. <code class="filename">cbq.init</code></a></span></dt></dl></dd><dt><span class="section"><a href="#diagram">10. Diagram</a></span></dt><dd><dl><dt><span class="section"><a href="#d-general">10.1. General diagram</a></span></dt></dl></dd><dt><span class="section"><a href="#links">11. Annotated Traffic Control Links</a></span></dt></dl></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="intro"></a>1. Introduction to Linux Traffic Control</h2></div></div></div><p>
    Linux offers a very rich set of tools for managing and manipulating the
    transmission of packets.  The larger Linux community is very familiar with
    the tools available under Linux for packet mangling and firewalling
    (netfilter, and before that, ipchains) as well as hundreds of network
    services which can run on the operating system.  Few inside the community
    and fewer outside the Linux community are aware of the tremendous power of
    the traffic control subsystem which has grown and matured under kernels
    2.2 and 2.4.
  </p><p>
    This HOWTO purports to introduce the
    <a class="link" href="#overview" title="2. Overview of Concepts">concepts of traffic control</a>,
    <a class="link" href="#elements" title="3. Traditional Elements of Traffic Control">the traditional elements (in general)</a>,
    <a class="link" href="#components" title="4. Components of Linux Traffic Control">the components of the Linux traffic control
    implementation</a> and provide some
    <a class="link" href="#rules" title="8. Rules, Guidelines and Approaches">guidelines</a>
    .
    This HOWTO represents the collection, amalgamation and synthesis of the
    <a class="ulink" href="http://lartc.org/howto/" target="_top">LARTC HOWTO</a>, documentation from individual projects and importantly
    the <a class="ulink" href="http://vger.kernel.org/vger-lists.html#lartc" target="_top">LARTC 
         mailing list</a> over a period of study.
  </p><p>
    The impatient soul, who simply wishes to experiment right now, is
    recommended to the <a class="ulink" href="http://tldp.org/HOWTO/Traffic-Control-tcng-HTB-HOWTO/" target="_top">
           Traffic Control using tcng and HTB HOWTO</a> and <a class="ulink" href="http://lartc.org/howto/" target="_top">LARTC HOWTO</a> for
    immediate satisfaction.
  </p><p>
  </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="i-assumptions"></a>1.1. Target audience and assumptions about the reader</h3></div></div></div><p>
      The target audience for this HOWTO is the network administrator or savvy
      home user who desires an introduction to the field of traffic control
      and an overview of the tools available under Linux for implementing
      traffic control.
    </p><p>
      I assume that the reader is comfortable with UNIX concepts and the
      command line and has a basic knowledge of IP networking.  Users who wish
      to implement traffic control may require the ability to patch, compile
      and install a kernel or software package
      <a href="#ftn.idm113" class="footnote" name="idm113"><sup class="footnote">[1]</sup></a>.  For users with newer kernels
      (2.4.20+, see also
      <a class="xref" href="#s-kernel" title="5.1. Kernel requirements">Section 5.1, &#8220;Kernel requirements&#8221;</a>), however, the ability to install and use
      software may be all that is required.
    </p><p>
      Broadly speaking, this HOWTO was written with a sophisticated user in
      mind, perhaps one who has already had experience with traffic control
      under Linux.  I assume that the reader may have
      no prior traffic control experience.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="i-conventions"></a>1.2. Conventions</h3></div></div></div><p>
      This text was written in
      <a class="ulink" href="http://www.docbook.org/" target="_top">DocBook</a>
      (<a class="ulink" href="http://www.docbook.org/xml/4.2/index.html" target="_top">version 4.2</a>)
      with
      <a class="ulink" href="http://vim.sourceforge.net/" target="_top"><span class="command"><strong>vim</strong></span></a>.
      All formatting has been applied by
      <a class="ulink" href="http://xmlsoft.org/XSLT/" target="_top">xsltproc</a> based on
      <a class="ulink" href="http://docbook.sourceforge.net/projects/xsl/" target="_top">DocBook
      XSL</a> and
      <a class="ulink" href="http://www.tldp.org/LDP/LDP-Author-Guide/usingldpxsl.html" target="_top">LDP
      XSL</a> stylesheets.  Typeface formatting and display conventions
      are similar to most printed and electronically distributed technical
      documentation.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="i-recommendation"></a>1.3. Recommended approach</h3></div></div></div><p>
      I strongly recommend to the eager reader making a first foray into the
      discipline of traffic control, to become only casually familiar with the
      <a class="link" href="#s-iproute2-tc"><span class="command"><strong>tc</strong></span></a> command line utility, before concentrating on <a class="link" href="#s-tcng" title="5.3. tcng, Traffic Control Next Generation"><span class="command"><strong>tcng</strong></span></a>.  The
      <span class="command"><strong>tcng</strong></span> software package defines an entire language for describing
      traffic control structures.
      At first, this language may seem daunting, but mastery of these basics
      will quickly provide the user with a much wider ability to employ (and
      deploy) traffic control configurations than the direct use of <span class="command"><strong>tc</strong></span>
      would afford.
    </p><p>
      Where possible, I'll try to prefer describing the behaviour of
      the Linux traffic control system in an abstract manner, although in
      many cases I'll need to supply the syntax of one or the other common
      systems for defining these structures.  I may not supply examples in
      both the <span class="command"><strong>tcng</strong></span> language and the <span class="command"><strong>tc</strong></span> command line, so the wise user
      will have some familiarity with both.
    </p><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="i-missing"></a>1.4. Missing content, corrections and feedback</h3></div></div></div><p>
      There is content yet missing from this HOWTO.  In particular, the
      following items will be added at some point to this documentation.
    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
          A section of examples.
        </p></li><li class="listitem"><p>
          A section detailing the classifiers.
        </p></li><li class="listitem"><p>
          A section discussing the techniques for measuring traffic.
        </p></li><li class="listitem"><p>
          A section covering meters.
        </p></li><li class="listitem"><p>
          More details on <span class="command"><strong>tcng</strong></span>.
        </p></li><li class="listitem"><p>
          Descriptions of newer qdiscs, specifically, Controlled Delay
          (codel), Fair Queue Controlled Delay (fq_codel), Proportional
          Integrated controller Enhanced (pie), Stochastic Fair Blue (sfb),
          Heavy Hitter Filter (hhf), Choke (choke).
        </p></li></ul></div><p>
      I welcome suggestions, corrections and feedback at <code class="email">&lt;<a class="email" href="mailto:martin@linux-ip.net">martin@linux-ip.net</a>&gt;</code>.  All errors
      and omissions are strictly my fault.  Although I have made every effort
      to verify the factual correctness of the content presented herein, I
      cannot accept any responsibility for actions taken under the influence
      of this documentation.
    </p><p>
    </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="overview"></a>2. Overview of Concepts</h2></div></div></div><p>
    This section will
    <a class="link" href="#o-what-is" title="2.1. What is it?">introduce traffic control</a> and
    <a class="link" href="#o-why-use" title="2.2. Why use it?">examine reasons for it</a>,
    identify a few
    <a class="link" href="#o-advantages" title="2.3. Advantages">advantages</a> and
    <a class="link" href="#o-disadvantages" title="2.4. Disdvantages">disadvantages</a> and
    introduce key concepts used in traffic control.
  </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="o-what-is"></a>2.1. What is it?</h3></div></div></div><p>
      Traffic control is the name given to the sets of queuing systems and
      mechanisms by which packets are received and transmitted on a router.
      This includes deciding (if and) which packets to accept at what
      rate on the input of an interface and determining which packets to
      transmit in what order at what rate on the output of an interface.
    </p><p>
      In the simplest possible model, traffic control consists of
      a single queue which collects entering packets and dequeues them as
      quickly as the hardware (or underlying device) can accept them.  This
      sort of queue is a FIFO.  This is like a single toll booth for
      entering a highway.  Every car must stop and pay the toll.  Other cars
      wait their turn.
    </p><p>
      Linux provides this simplest traffic control tool (FIFO), and
      in addition offers a wide variety of other tools that allow all sorts of
      control over packet handling.
    </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
      The default qdisc under Linux is the <a class="link" href="#qs-pfifo_fast" title="6.2. pfifo_fast, the default Linux qdisc"><code class="constant">pfifo_fast</code></a>, which is
      slightly more complex than the <a class="link" href="#qs-fifo" title="6.1. FIFO, First-In First-Out (pfifo and bfifo)">FIFO</a>.
      </p></div><p>
      There are examples of queues in all sorts of software.  The queue is a
      way of organizing the pending tasks or data (see also
      <a class="xref" href="#o-queues" title="2.5. Queues">Section 2.5, &#8220;Queues&#8221;</a>).  Because network links typically carry data
      in a serialized fashion, a queue is required to manage the outbound
      data packets.
    </p><p>
      In the case of a desktop machine and an efficient webserver sharing the
      same uplink to the Internet, the following contention for bandwidth may
      occur.  The web server may be able to fill up the output queue on the
      router faster than the data can be transmitted across the link, at which
      point the router starts to drop packets (its buffer is full!).  Now, the
      desktop machine (with an interactive application user) may be faced with
      packet loss and high latency.  Note that high latency sometimes leads to
      screaming users!  By separating the internal queues used to service
      these two different classes of application, there can be better sharing
      of the network resource between the two applications.
    </p><p>
      Traffic control is a set of tools allowing an administrator granular
      control over these queues and the queuing mechanisms of a
      networked device.  The power to rearrange traffic flows and packets with
      these tools is tremendous and can be complicated, but is no substitute
      for adequate bandwidth.
    </p><p>
      The term Quality of Service (QoS) is often used as a synonym for traffic
      control at an IP-layer.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="o-why-use"></a>2.2. Why use it?</h3></div></div></div><p>
      Traffic control tools allow the implementer to apply preferences,
      organizational or business policies to packets or network flows
      transmitted into a network.  This control allows stewardship over the
      network resources such as throughput or latency.
    </p><p>
      Fundamentally, traffic control becomes a necessity because of packet
      switching in networks.  
    </p><div class="sidebar"><div class="titlepage"><div><div><p class="title"><b></b></p></div></div></div><p>
      For a brief digression, to explain the novelty and cleverness
      of packet switching, think about the circuit-switched telephone networks
      that were built over the entire 20th century.  In order to set up a
      call, the network gear knew rules about call establishment and when a
      caller tried to connect, the network employed the rules to reserve a
      circuit for the entire duration of the call or connection.  While one
      call was engaged, using that resource, no other call or caller could use
      that resource.  This meant many individual pieces of equipment could
      block call setup because of resource unavailability.
    </p><p>
      Let's return to packet-switched networks, a mid-20th century invention,
      later in wide use, and nearly ubiquitous in the 21st century.
      Packet-switched networks differ from circuit based networks in one very
      important regard.  The unit of data handled by the network gear is not a
      circuit, but rather a small chunk of data called a packet.  Inexactly
      speaking, the packet is a letter in an envelope with a destination
      address.  The packet-switched network had only a very small amount of
      work to do, reading the destination identifier and transmitting the
      packet.
    </p><p>
      Sometimes, packet-switched networks are described as stateless because
      they do not need to track all of the flows (analogy to a circuit) that
      are active in the network.  In order to be function, the packet-handling
      machines must know how to reach the destinations addresses.  One analogy
      is a package-handling service like your postal service, UPS or DHL.
    </p><p>
      If there's a sudden influx of packets into a packet-switched network
      (or, by analogy, the increase of cards and packages sent by mail and
      other carriers at Christmas), the network can become slow or
      unresponsive.  Lack of differentiation between importance of specific
      packets or network flows is, therefore, a weakness of such
      packet-switched networks.  The network can be overloaded with data
      packets all competing.
    </p></div><p>
      In simplest terms, the traffic control tools allow somebody to enqueue
      packets into the network differently based on attributes of the packet.
      The various different tools each solve a different problem and many can
      be combined, to implement complex rules to meet a preference or business
      goal.
    </p><p>
      There are many practical reasons to consider traffic control, and many
      scenarios in which using traffic control makes sense.  Below are some
      examples of common problems which can be solved or at least ameliorated
      with these tools.
    </p><p>
      The list below is not an exhaustive list of the sorts of solutions
      available to users of traffic control, but shows the
      types of common problems that can be solved by using traffic control
      tools to maximize the usability of the network.
    </p><div class="itemizedlist"><p class="title"><b>Common traffic control solutions</b></p><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
          Limit total bandwidth to a known rate; <a class="link" href="#qs-tbf" title="6.7. TBF, Token Bucket Filter">TBF</a>,
          <a class="link" href="#qc-htb" title="7.1. HTB, Hierarchical Token Bucket">HTB</a> with child class(es).
        </p></li><li class="listitem"><p>
          Limit the bandwidth of a particular user, service or client;
          <a class="link" href="#qc-htb" title="7.1. HTB, Hierarchical Token Bucket">HTB</a> classes and <a class="link" href="#e-classifying" title="3.3. Classifying">classifying</a> with a
          <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a>.
        </p></li><li class="listitem"><p>
          Maximize TCP throughput on an asymmetric link; prioritize
          transmission of ACK packets, <a class="link" href="#sc-wondershaper" title="9.1. wondershaper">wondershaper</a>.
        </p></li><li class="listitem"><p>
          Reserve bandwidth for a particular application or user;
          <a class="link" href="#qc-htb" title="7.1. HTB, Hierarchical Token Bucket">HTB</a> with children classes and <a class="link" href="#e-classifying" title="3.3. Classifying">classifying</a>.
        </p></li><li class="listitem"><p>
          Prefer latency sensitive traffic; <a class="link" href="#qc-prio" title="7.3. PRIO, priority scheduler">PRIO</a> inside an
          <a class="link" href="#qc-htb" title="7.1. HTB, Hierarchical Token Bucket">HTB</a> class.
        </p></li><li class="listitem"><p>
          Managed oversubscribed bandwidth; <a class="link" href="#qc-htb" title="7.1. HTB, Hierarchical Token Bucket">HTB</a> with borrowing.
        </p></li><li class="listitem"><p>
          Allow equitable distribution of unreserved bandwidth; <a class="link" href="#qc-htb" title="7.1. HTB, Hierarchical Token Bucket">HTB</a>
          with borrowing.
        </p></li><li class="listitem"><p>
          Ensure that a particular type of traffic is dropped; <a class="link" href="#c-police" title="4.5. policer"><code class="constant">policer</code></a>
          attached to a <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a> with a <a class="link" href="#c-drop" title="4.6. drop"><code class="constant">drop</code></a> action.
        </p></li></ul></div><p>
      Remember that, sometimes, it is simply better to purchase more
      bandwidth.  Traffic control does not solve all problems!
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="o-advantages"></a>2.3. Advantages</h3></div></div></div><p>
      When properly employed, traffic control should lead to more predictable
      usage of network resources and less volatile contention for these
      resources.  The network then meets the goals of the traffic control
      configuration.  Bulk download traffic can be allocated a reasonable
      amount of bandwidth even as higher priority interactive traffic is
      simultaneously
      serviced.  Even low priority data transfer such as mail can
      be allocated bandwidth without tremendously affecting the other classes
      of traffic.
    </p><p>
      In a larger picture, if the traffic control configuration represents
      policy which has been communicated to the users, then users (and,
      by extension, applications) know what to expect from the network.
    </p><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="o-disadvantages"></a>2.4. Disdvantages</h3></div></div></div><p>
    </p><p>
      Complexity is easily one of the most significant disadvantages of using
      traffic control.  There are ways to become familiar with traffic control
      tools which ease the learning curve about traffic control and its
      mechanisms, but identifying a traffic control misconfiguration can be
      quite a challenge.
    </p><p>
      Traffic control when used appropriately can lead to more equitable
      distribution of network resources.  It can just as easily be installed
      in an inappropriate manner leading to further and more divisive
      contention for resources.
    </p><p>
      The computing resources required on a router to support a traffic
      control scenario need to be capable of handling the increased cost of
      maintaining the traffic control structures.  Fortunately, this is a
      small incremental cost, but can become more significant as the
      configuration grows in size and complexity.
    </p><p>
      For personal use, there's no training cost associated with the use of
      traffic control, but a company may find that purchasing more bandwidth
      is a simpler solution than employing traffic control.  Training
      employees  and ensuring depth of knowledge may be more costly than
      investing in more bandwidth.
    </p><p>
      Although
      traffic control on packet-switched networks covers a larger conceptual
      area, you can think of traffic control as a way to provide [some of] the
      statefulness of a circuit-based network to a packet-switched.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="o-queues"></a>2.5. Queues</h3></div></div></div><p>
      Queues form the backdrop for all of traffic control and are the integral
      concept behind scheduling.  A queue is a location (or buffer) containing
      a finite number of items waiting for an action or service.  In
      networking, a queue is the place where packets (our units) wait to be
      transmitted by the hardware (the service).  In the simplest model,
      packets are transmitted in a first-come first-serve basis
      <a href="#ftn.idm249" class="footnote" name="idm249"><sup class="footnote">[2]</sup></a>.
      In the discipline of computer networking (and more generally
      computer science), this sort of a queue is known as a FIFO.
    </p><p>
      Without any other mechanisms, a queue doesn't offer any promise for
      traffic control.  There are only two interesting actions in a queue.
      Anything entering a queue is enqueued into the queue.  To remove an item
      from a queue is to dequeue that item.
    </p><p>
      A queue becomes much more interesting when coupled with other mechanisms
      which can delay packets, rearrange, drop and prioritize packets in
      multiple queues.  A queue can also use subqueues, which allow for
      complexity of behaviour in a scheduling operation.
    </p><p>
      From the perspective of the higher layer software, a packet is simply
      enqueued for transmission, and the manner and order in which the
      enqueued packets are transmitted is immaterial to the higher layer.  So,
      to the higher layer, the entire traffic control system may appear as a
      single queue
      <a href="#ftn.idm254" class="footnote" name="idm254"><sup class="footnote">[3]</sup></a>.
      It is only by examining the internals of this layer that
      the traffic control structures become exposed and available.
    </p><p>
      In the image below a simplified high level overview of the queues on
      the transmit path of the Linux network stack:
    </p><div class="mediaobject"><a name="img-Figure1"></a><object type="image/svg+xml" data="images/Figure1.svg"></object><div class="caption"><p><span class="command"><strong>Figure 1: </strong></span><span class="emphasis"><em>Simplified high level overview of the queues on the transmit path of the Linux network stack</em></span>.
              </p></div></div><p>
      See <a class="link" href="#o-nic" title="2.9. NIC, Network Interface Controller">2.9</a> for details about NIC interface and <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">4.9</a>
      for details about <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">driver queue</a>.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="o-flows"></a>2.6. Flows</h3></div></div></div><p>
      A flow is a distinct connection or conversation between two hosts.  Any
      unique set of packets between two hosts can be regarded as a flow.
      Under TCP the concept of a connection with a source IP and port and
      destination IP and port represents a flow.  A UDP flow can be similarly
      defined.
    </p><p>
      Traffic control mechanisms frequently separate traffic into classes of
      flows which can be aggregated and transmitted as an aggregated flow
      (consider DiffServ).  Alternate mechanisms may attempt to divide
      bandwidth equally based on the individual flows.
    </p><p>
      Flows become important when attempting to divide bandwidth equally among
      a set of competing flows, especially when some applications deliberately
      build a large number of flows.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="o-tokens"></a>2.7. Tokens and buckets</h3></div></div></div><a name="o-buckets"></a><p>
      Two of the key underpinnings of a <a class="link" href="#e-shaping" title="3.1. Shaping">shaping</a> mechanisms are
      the interrelated concepts of tokens and buckets.
    </p><p>
      In order to control the rate of dequeuing, an implementation can count
      the number of packets or bytes dequeued as each item is dequeued,
      although this requires complex usage of timers and measurements to limit
      accurately.  Instead of calculating the current usage and time, one
      method, used widely in traffic control, is to generate tokens at a
      desired rate, and only dequeue packets or bytes if a token is available.
    </p><p>
      Consider the analogy of an amusement park ride with a queue of people
      waiting to experience the ride.  Let's imagine a track on which carts
      traverse a fixed track.  The carts arrive at the head of the queue at a
      fixed rate.  In order to enjoy the ride, each person must wait for an
      available cart.  The cart is analogous to a token and the person is
      analogous to a packet.  Again, this mechanism is a rate-limiting or
      <a class="link" href="#e-shaping" title="3.1. Shaping">shaping</a> mechanism.  Only a certain number of people can
      experience the ride in a particular period.
    </p><p>
      To extend the analogy, imagine an empty line for the amusement park
      ride and a large number of carts sitting on the track ready to carry
      people.  If a large number of people entered the line together many
      (maybe all) of them could experience the ride because of the carts
      available and waiting.  The number of carts available is a concept
      analogous to the bucket.  A bucket contains a number of tokens and can
      use all of the tokens in bucket without regard for passage of time.
    </p><p>
      And to complete the analogy, the carts on the amusement park ride (our
      tokens) arrive at a fixed rate and are only kept available up to the
      size of the bucket.  So, the bucket is filled with tokens according to
      the rate, and if the tokens are not used, the bucket can fill up.  If
      tokens are used the bucket will not fill up.  Buckets are a key concept
      in supporting bursty traffic such as HTTP.
    </p><p>
      The <a class="link" href="#qs-tbf" title="6.7. TBF, Token Bucket Filter">TBF</a> qdisc is a classical example of a shaper (the section
      on TBF includes a diagram which may help to visualize the token
      and bucket concepts).  The TBF generates <em class="parameter"><code>rate</code></em> tokens and
      only transmits packets when a token is available.  Tokens are a generic
      shaping concept.
    </p><p>
      In the case that a queue does not need tokens immediately, the tokens
      can be collected until they are needed.  To collect tokens indefinitely
      would negate any benefit of shaping so tokens are collected until a
      certain number of tokens has been reached.  Now, the queue has tokens
      available for a large number of packets or bytes which need to be
      dequeued.  These intangible tokens are stored in an intangible bucket,
      and the number of tokens that can be stored depends on the size of the
      bucket.
    </p><p>
      This also means that a bucket full of tokens may be available at any
      instant.  Very predictable regular traffic can be handled by small
      buckets.  Larger buckets may be required for burstier traffic, unless
      one of the desired goals is to reduce the burstiness of the
      <a class="link" href="#o-flows" title="2.6. Flows">flows</a>.
    </p><p>
      In summary, tokens are generated at rate, and a maximum of a bucket's
      worth of tokens may be collected.  This allows bursty traffic to be
      handled, while smoothing and shaping the transmitted traffic.
    </p><p>
      The concepts of tokens and buckets are closely interrelated and are used
      in both <a class="link" href="#qs-tbf" title="6.7. TBF, Token Bucket Filter">TBF</a> (one of the <a class="link" href="#classless-qdiscs" title="6. Classless Queuing Disciplines (qdiscs)">classless qdiscs</a>) and
      <a class="link" href="#qc-htb" title="7.1. HTB, Hierarchical Token Bucket">HTB</a> (one of the <a class="link" href="#classful-qdiscs" title="7. Classful Queuing Disciplines (qdiscs)">classful qdiscs</a>).
      Within the <span class="command"><strong>tcng</strong></span> language, the use of two- and three-color meters is
      indubitably a token and bucket concept.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="o-packets"></a>2.8. Packets and frames</h3></div></div></div><a name="o-frames"></a><p>
      The terms for data sent across network changes depending on the layer
      the user is examining.  This document will rather impolitely (and
      incorrectly) gloss over the technical distinction between
      packets and frames although they are outlined here.
    </p><p>
      The word frame is typically used to describe a layer 2 (data link) unit
      of data to be forwarded to the next recipient.  Ethernet interfaces, PPP
      interfaces, and T1 interfaces all name their layer 2 data unit a frame.
      The frame is actually the unit on which traffic control is performed.
    </p><p>
      A packet, on the other hand, is a higher layer concept, representing
      layer 3 (network) units.  The term packet is preferred in this
      documentation, although it is slightly inaccurate.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="o-nic"></a>2.9. NIC, Network Interface Controller</h3></div></div></div><p>
         A network interface controller is a computer hardware component,
         differently from previous ones thar are software components, that
         connects a computer to a computer network. The network controller
         implements the electronic circuitry required to communicate using a
         specific data link layer and physical layer standard such as
         Ethernet, Fibre Channel, Wi-Fi or Token Ring.  Traffic control must
         deal with the physical constraints and characteristics of the NIC
         interface.
      </p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="o-huge-packet"></a>2.9.1. Huge Packets from the Stack</h4></div></div></div><p>
              Most NICs have a fixed maximum transmission unit
              (<acronym class="acronym">MTU</acronym>) which is the biggest frame which can be
              transmitted by the physical medium.  For Ethernet the default MTU
              is 1500 bytes but some Ethernet networks support Jumbo Frames
              of up to 9000 bytes.  Inside the IP network stack, the MTU can
              manifest as a limit on the size of the packets which are sent to
              the device for transmission. For example, if an application
              writes 2000 bytes to a TCP socket then the IP stack needs to
              create two IP packets to keep the packet size less than or equal
              to a 1500 byte MTU. For large data transfers the comparably small
              MTU causes a large number of small packets to be created and
              transferred through the <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">driver
              queue</a>.
          </p><p>
              In order to avoid the overhead associated with a large number of packets on the transmit path, the Linux kernel implements several optimizations: TCP segmentation offload (TSO), UDP fragmentation offload (UFO) and generic segmentation offload (GSO). All of these optimizations allow the IP stack to create packets which are larger than the MTU of the outgoing NIC. For IPv4, packets as large as the IPv4 maximum of 65,536 bytes can be created and queued to the <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">driver queue</a>. In the case of TSO and UFO, the NIC hardware takes responsibility for breaking the single large packet into packets small enough to be transmitted on the physical interface. For NICs without hardware support, GSO performs the same operation in software immediately before queueing to the <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">driver queue</a>.
          </p><p>
              Recall from earlier that the <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">driver queue</a> contains a fixed number of descriptors which each point to packets of varying sizes, Since TSO, UFO and GSO allow for much larger packets these optimizations have the side effect of greatly increasing the number of bytes which can be queued in the <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">driver queue</a>. Figure 3 illustrates this concept.
          </p><div class="mediaobject"><a name="img-Figure2"></a><object type="image/svg+xml" data="images/Figure2.svg"></object><div class="caption"><p><span class="command"><strong>Figure 2:</strong></span><span class="emphasis"><em> Large packets can be sent to the NIC when TSO, UFO or GSO are enabled. This can greatly increase the number of bytes in the <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">driver queue</a>.</em></span>
                  </p></div></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="o-starv-lat"></a>2.10. Starvation and Latency</h3></div></div></div><p>
          The queue between the IP stack and the hardware (see <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">chapter 4.2</a> for detail about <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">driver queue</a> or see <a class="link" href="#s-ethtool" title="5.6. ethtool, Driver Queue">chapter 5.5</a> for how manage it) introduces two problems: starvation and latency.
      </p><p>
          If the NIC driver wakes to pull packets off of the queue for transmission and the queue is empty the hardware will miss a transmission opportunity thereby reducing the throughput of the system. This is referred to as starvation. Note that an empty queue when the system does not have anything to transmit is not starvation &#8211; this is normal. The complication associated with avoiding starvation is that the IP stack which is filling the queue and the hardware driver draining the queue run asynchronously. Worse, the duration between fill or drain events varies with the load on the system and external conditions such as the network interface&#8217;s physical medium. For example, on a busy system the IP stack will get fewer opportunities to add packets to the buffer which increases the chances that the hardware will drain the buffer before more packets are queued. For this reason it is advantageous to have a very large buffer to reduce the probability of starvation and ensures high throughput.
      </p><p>
          While a large queue is necessary for a busy system to maintain high throughput, it has the downside of allowing for the introduction of a large amount of latency.
      </p><div class="mediaobject"><a name="img-Figure3"></a><object type="image/svg+xml" data="images/Figure3.svg"></object><div class="caption"><p><span class="command"><strong>Figure 3:</strong></span> <span class="emphasis"><em>Interactive packet (yellow) behind bulk flow packets (blue)</em></span>
              </p></div></div><p>
          Figure 3 shows a <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">driver queue</a> which is almost full with TCP segments for a single high bandwidth, bulk traffic flow (blue). Queued last is a packet from a VoIP or gaming flow (yellow). Interactive applications like VoIP or gaming typically emit small packets at fixed intervals which are latency sensitive while a high bandwidth data transfer generates a higher packet rate and larger packets. This higher packet rate can fill the buffer between interactive packets causing the transmission of the interactive packet to be delayed. To further illustrate this behaviour consider a scenario based on the following assumptions:
      </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                 A network interface which is capable of transmitting at 5 Mbit/sec or 5,000,000 bits/sec.
              </p></li><li class="listitem"><p>
                 Each packet from the bulk flow is 1,500 bytes or 12,000 bits.
              </p></li><li class="listitem"><p>
                  Each packet from the interactive flow is 500 bytes.
              </p></li><li class="listitem"><p>
                 The depth of the queue is 128 descriptors
              </p></li><li class="listitem"><p>
                  There are 127 bulk data packets and 1 interactive packet queued last.
              </p></li></ul></div><p>
            Given the above assumptions, the time required to drain the 127 bulk packets and create a transmission opportunity for the interactive packet is (127 * 12,000) / 5,000,000 = 0.304 seconds (304 milliseconds for those who think of latency in terms of ping results). This amount of latency is well beyond what is acceptable for interactive applications and this does not even represent the complete round trip time &#8211; it is only the time required transmit the packets queued before the interactive one. As described earlier, the size of the packets in the <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">driver queue</a> can be larger than 1,500 bytes if TSO, UFO or GSO are enabled. This makes the latency problem correspondingly worse.
        </p><p>
          Choosing the correct size for the <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">driver queue</a> is a Goldilocks problem &#8211; it can&#8217;t be too small or throughput suffers, it can&#8217;t be too big or latency suffers.
        </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="o-throughput-latency"></a>2.11. Relationship between throughput and latency</h3></div></div></div><p>
      In all traffic control systems, there is a relationship between
      throughput and latency.  The maximum information rate of a network link
      is termed bandwidth, but for the user of a network the actually achieved
      bandwidth has a dedicated term, throughput.
    </p><div class="variablelist"><dl class="variablelist"><dt><a name="otl-latency"></a><span class="term">latency</span></dt><dd><p>
            the delay in time between a sender's transmission and the
            recipient's decoding or receiving of the data; always non-negative
            and non-zero (time doesn't move backwards, then)
          </p><p>
            in principle, latency is unidirectional, however almost the entire
            Internet networking community talks about bidirectional delay
            &#8212;the delay in time between a sender's transmission of data
            and some sort of acknowledgement of receipt of that data; cf.
            <span class="command"><strong>ping</strong></span>
          </p><p>
            measured in milliseconds (ms); on Ethernet, latencies are
            typically between 0.3 and 1.0 ms and on wide-area networks (i.e.
            to your ISP, across a large campus or to a remote server) between
            5 to 300 ms
          </p></dd><dt><a name="otl-throughput"></a><span class="term">throughput</span></dt><dd><p>
            a measure of the total amount of data that can be transmitted
            successfully between a sender and receiver
          </p><p>
            measured in bits per second; the measurement most often
            quoted by complaining users after buying a 10Mbit/s package from
            their provider and receiving 8.2Mbit/s.
          </p></dd></dl></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
        Latency and throughput are general computing terms.  For example,
        application developers speak of user-perceived latency when trying to
        build responsive tools.  Database and filesystem people speak about
        disk throughput.  And, above the network layer, latency of a website
        name lookup in DNS is a major contributor to the perceived performance
        of a website.  The remainder of this document concerns latency in the
        network domain, specifically the IP network layer.
      </p></div><p>
      During the millenial fin de siècle, many developed world network service
      providers had learned that users were interested in the highest possible
      download throughput (the above mentioned 10Mbit/s bandwidth figure).
    </p><p>
      In order to maximize this download throughput, gear vendors and
      providers commonly tuned their equipment to hold a large number of data
      packets.  When the network was ready to accept another packet, the
      network gear was certain to have one in its queue and could simply send
      another packet.  In practice, this meant that the user, who was
      measuring download throughput, would receive a high number and was
      likely to be happy.  This was desirable for the provider because the
      delivered throughput could more likely meet the advertised number.
    </p><p>
      This technique effectively maximized throughput, at the cost of latency.
      Imagine that a high priority packet is waiting at the end of the big
      queue of packets mentioned above.  Perhaps, the theoretical latency of
      the packet on this network might be 100ms, but it needs to wait its turn
      in the queue to be transmitted.  
    </p><p>
      While the decision to maximize
      throughput has been wildly successful, the effect on latency is
      significant.
    </p><a name="o-bufferbloat"></a><p>
      Despite a general warning from Stuart Cheshire in the mid-1990s called
      <a class="ulink" href="http://www.stuartcheshire.org/rants/Latency.html" target="_top">It's the Latency, Stupid</a>,
      it took the novel term, bufferbloat, widely publicized about 15
      years later by Jim Getty in an ACM Queue article
      <a class="ulink" href="http://queue.acm.org/detail.cfm?id=2071893" target="_top">Bufferbloat: Dark Buffers in the Internet</a>
      and a
      <a class="ulink" href="https://gettys.wordpress.com/bufferbloat-faq/" target="_top">Bufferbloat FAQ</a>
      in his blog, to bring some focus onto the choice for maximizing
      throughput that both gear vendors and providers preferred.
    </p><p>
      The relationship (tension) between latency and throughput in
      packet-switched networks have been well-known in the academic,
      networking and Linux development community.  Linux traffic control core
      data structures date back to the 1990s and have been continuously
      developed and extended with new schedulers and features.
    </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="elements"></a>3. Traditional Elements of Traffic Control</h2></div></div></div><p>
  </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="e-shaping"></a>3.1. Shaping</h3></div></div></div><p>
      Shapers delay packets to meet a desired rate.
    </p><p>
      Shaping is the mechanism by which packets are delayed before
      transmission in an output queue to meet a desired output rate.  This is
      one of the most common desires of users seeking bandwidth control
      solutions.  The act of delaying a packet as part of a traffic control
      solution makes every shaping mechanism into a non-work-conserving
      mechanism, meaning roughly:  "Work is required in order to delay
      packets."
    </p><p>
      Viewed in reverse, a non-work-conserving queuing mechanism is performing
      a shaping function.  A work-conserving queuing mechanism (see
      <a class="link" href="#qc-prio" title="7.3. PRIO, priority scheduler">PRIO</a>) would not be capable of delaying a packet.
    </p><p>
      Shapers attempt to limit or ration traffic to meet but not exceed a
      configured rate (frequently measured in packets per second or bits/bytes
      per second).  As a side effect, shapers can smooth out bursty traffic
      <a href="#ftn.idm412" class="footnote" name="idm412"><sup class="footnote">[4]</sup></a>.
      One of the advantages of shaping bandwidth is the ability to control
      latency of packets.  The underlying mechanism for shaping to a rate is
      typically a token and bucket mechanism.  See also
      <a class="xref" href="#o-tokens" title="2.7. Tokens and buckets">Section 2.7, &#8220;Tokens and buckets&#8221;</a> for further detail on tokens and buckets.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="e-scheduling"></a>3.2. Scheduling</h3></div></div></div><p>
      Schedulers arrange and/or rearrange packets for output.
    </p><p>
      Scheduling is the mechanism by which packets are arranged (or
      rearranged) between input and output of a particular queue.  The
      overwhelmingly most common scheduler is the FIFO (first-in first-out)
      scheduler.  From a larger perspective, any set of traffic control
      mechanisms on an output queue can be regarded as a scheduler, because
      packets are arranged for output.
    </p><p>
      Other generic scheduling mechanisms attempt to compensate for various
      networking conditions.  A fair queuing algorithm (see <a class="link" href="#qs-sfq" title="6.3. SFQ, Stochastic Fair Queuing">SFQ</a>)
      attempts to prevent any single client or flow from dominating the
      network usage.  A round-robin algorithm (see <a class="link" href="#qc-wrr" title="7.5. WRR, Weighted Round Robin">WRR</a>) gives each
      flow or client a turn to dequeue packets.  Other sophisticated
      scheduling algorithms attempt to prevent backbone overload (see
      <a class="link" href="#qs-gred" title="6.6. GRED, Generic Random Early Drop">GRED</a>) or refine other scheduling mechanisms (see
      <a class="link" href="#qs-esfq" title="6.4. ESFQ, Extended Stochastic Fair Queuing">ESFQ</a>).
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="e-classifying"></a>3.3. Classifying</h3></div></div></div><p>
      Classifiers sort or separate traffic into queues.
    </p><p>
      Classifying is the mechanism by which packets are separated for
      different treatment, possibly different output queues.  During the
      process of accepting, routing and transmitting a packet, a networking
      device can classify the packet a number of different ways.
      Classification can include
      <a class="link" href="#e-marking" title="3.6. Marking">marking</a> the packet, which usually
      happens on the boundary of a network under a single administrative
      control or classification can occur on each hop individually.
    </p><p>
      The Linux model (see
      <a class="xref" href="#c-filter" title="4.3. filter">Section 4.3, &#8220;<code class="constant">filter</code>&#8221;</a>) allows for a packet to cascade across a
      series of classifiers in a traffic control structure and to be
      classified in conjunction with
      <a class="link" href="#e-policing" title="3.4. Policing">policers</a> (see also
      <a class="xref" href="#c-police" title="4.5. policer">Section 4.5, &#8220;policer&#8221;</a>).
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="e-policing"></a>3.4. Policing</h3></div></div></div><p>
      Policers measure and limit traffic in a particular queue.
    </p><p>
      Policing, as an element of traffic control, is simply
      a mechanism by which traffic can be limited.  Policing is most
      frequently used on the network border to ensure that a peer is not
      consuming more than its allocated bandwidth.  A policer will accept
      traffic to a certain rate, and then perform an action on traffic
      exceeding this rate.  A rather harsh solution is to
      <a class="link" href="#e-dropping" title="3.5. Dropping">drop</a> the traffic, although the
      traffic could be
      <a class="link" href="#e-classifying" title="3.3. Classifying">reclassified</a> instead of being
      dropped.
    </p><p>
      A policer is a yes/no question about the rate at which traffic is
      entering a queue.  If the packet is about to enter a queue below a given
      rate, take one action (allow the enqueuing).  If the packet is about to
      enter a queue above a given rate, take another action.  Although the
      policer uses a token bucket mechanism internally, it does not have the
      capability to delay a packet as a <a class="link" href="#e-shaping" title="3.1. Shaping">shaping</a> mechanism does.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="e-dropping"></a>3.5. Dropping</h3></div></div></div><p>
      Dropping discards an entire packet, flow or classification.
    </p><p>
      Dropping a packet is a mechanism by which a packet is discarded.
    </p><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="e-marking"></a>3.6. Marking</h3></div></div></div><p>
      Marking is a mechanism by which the packet is altered.
    </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
      This is not <code class="constant">fwmark</code>.  The <span class="command"><strong>iptables</strong></span> target <code class="constant">MARK</code> and the
      <span class="command"><strong>ipchains</strong></span> <code class="option">--mark</code> are used to modify packet metadata, not the packet
      itself.
      </p></div><p>
      Traffic control marking mechanisms install a DSCP on the packet
      itself, which is then used and respected by other routers inside an
      administrative domain (usually for DiffServ).
    </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="components"></a>4. Components of Linux Traffic Control</h2></div></div></div><p>
  </p><p>
  </p><p>
  </p><div class="table"><a name="tb-c-components-correlation"></a><p class="title"><b>Table 1. Correlation between traffic control elements and Linux
      components</b></p><div class="table-contents"><table class="table" summary="Correlation between traffic control elements and Linux
      components" border="1"><colgroup><col align="left" class="elem"><col align="left" class="comp"></colgroup><thead><tr><th align="left">traditional element</th><th align="left">Linux component</th></tr></thead><tbody><tr><td align="left">enqueuing;</td><td align="left">FIXME: receiving packets from userspace and
          network.</td></tr><tr><td align="left"><a class="link" href="#e-shaping" title="3.1. Shaping">shaping</a></td><td align="left">The <a class="link" href="#c-class" title="4.2. class"><code class="constant">class</code></a> offers shaping capabilities.</td></tr><tr><td align="left"><a class="link" href="#e-scheduling" title="3.2. Scheduling">scheduling</a></td><td align="left">A <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a> is a scheduler.  Schedulers
                                can be simple such as the FIFO or
                                complex, containing classes and other
                                qdiscs, such as HTB.</td></tr><tr><td align="left"><a class="link" href="#e-classifying" title="3.3. Classifying">classifying</a></td><td align="left">The <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a> object performs the
                                classification through the agency of a
                                <a class="link" href="#c-classifier" title="4.4. classifier"><code class="constant">classifier</code></a> object.  Strictly speaking,
                                Linux classifiers cannot exist outside
                                of a filter.</td></tr><tr><td align="left"><a class="link" href="#e-policing" title="3.4. Policing">policing</a></td><td align="left">A <a class="link" href="#c-police" title="4.5. policer"><code class="constant">policer</code></a> exists in the Linux traffic
                                control implementation only as part of a
                                <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a>.</td></tr><tr><td align="left"><a class="link" href="#e-dropping" title="3.5. Dropping">dropping</a></td><td align="left">To <a class="link" href="#c-drop" title="4.6. drop"><code class="constant">drop</code></a> traffic requires a
                                <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a> with a <a class="link" href="#c-police" title="4.5. policer"><code class="constant">policer</code></a> which
                                uses <span class="quote">&#8220;<span class="quote">drop</span>&#8221;</span> as an action.</td></tr><tr><td align="left"><a class="link" href="#e-marking" title="3.6. Marking">marking</a></td><td align="left">The <code class="constant">dsmark</code> <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a> is used for
                                marking.</td></tr><tr><td align="left">enqueuing;</td><td align="left">Between the scheduler's <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a> and the <a class="link" href="#o-nic" title="2.9. NIC, Network Interface Controller">network interface controller (NIC)</a> lies the <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">driver queue</a>. The <a class="link" href="#c-driver-queue" title="4.9. Driver Queue (aka ring buffer)">driver queue</a> gives the higher layers (IP stack and traffic control subsystem) a location to queue data asynchronously for the operation of the hardware.  The size of that queue is automatically set by <a class="xref" href="#c-bql" title="4.10. Byte Queue Limits (BQL)">Byte Queue Limits (BQL)</a>.</td></tr></tbody></table></div></div><br class="table-break"><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="c-qdisc"></a>4.1. <code class="constant">qdisc</code></h3></div></div></div><p>
      Simply put, a qdisc is a scheduler
      (<a class="xref" href="#e-scheduling" title="3.2. Scheduling">Section 3.2, &#8220;Scheduling&#8221;</a>).  Every output interface needs a
      scheduler of some kind, and the default scheduler is a FIFO.
      Other qdiscs available under Linux will rearrange the packets entering
      the scheduler's queue in accordance with that scheduler's rules.
    </p><p>
      The qdisc is the major building block on which all of Linux traffic
      control is built, and is also called a queuing discipline.
    </p><p>
      The <a class="link" href="#classful-qdiscs" title="7. Classful Queuing Disciplines (qdiscs)">classful qdiscs</a> can contain <a class="link" href="#c-class" title="4.2. class"><code class="constant">class</code></a>es, and provide a handle
      to which to attach <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a>s.  There is no prohibition on using a
      classful qdisc without child classes, although this will usually consume
      cycles and other system resources for no benefit.
    </p><p>
      The <a class="link" href="#classless-qdiscs" title="6. Classless Queuing Disciplines (qdiscs)">classless qdiscs</a> can contain no classes, nor is it possible to
      attach filter to a classless qdisc.  Because a classless qdisc contains
      no children of any kind, there is no utility to <a class="link" href="#e-classifying" title="3.3. Classifying">classifying</a>.
      This means that no filter can be attached to a classless qdisc.
    </p><p>
      A source of terminology confusion is the usage of the terms
      <code class="constant">root</code> qdisc and <code class="constant">ingress</code> qdisc.  These are not
      really queuing disciplines, but rather locations onto which traffic
      control structures can be attached for egress (outbound traffic) and
      ingress (inbound traffic).
    </p><p>
      Each interface contains both. The primary and more common is the
      egress qdisc, known as the <code class="constant">root</code> qdisc.  It can contain any
      of the queuing disciplines (<a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a>s) with potential
      <a class="link" href="#c-class" title="4.2. class"><code class="constant">class</code></a>es and class structures.  The overwhelming majority of
      documentation applies to the <code class="constant">root</code> qdisc and its children.  Traffic
      transmitted on an interface traverses the egress or <code class="constant">root</code> qdisc.
    </p><p>
      For traffic accepted on an interface, the <code class="constant">ingress</code> qdisc is traversed.
      With its limited utility, it allows no child <a class="link" href="#c-class" title="4.2. class"><code class="constant">class</code></a> to be
      created, and only exists as an object onto which a <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a> can be
      attached.  For practical purposes, the <code class="constant">ingress</code> qdisc is merely a
      convenient object onto which to attach a <a class="link" href="#c-police" title="4.5. policer"><code class="constant">policer</code></a> to limit the
      amount of traffic accepted on a network interface.
    </p><p>
      In short, you can do much more with an egress qdisc because it contains
      a real qdisc and the full power of the traffic control system.  An
      <code class="constant">ingress</code> qdisc can only support a policer.  The remainder of the
      documentation will concern itself with traffic control structures
      attached to the <code class="constant">root</code> qdisc unless otherwise specified.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="c-class"></a>4.2. <code class="constant">class</code></h3></div></div></div><p>
      Classes only exist inside a classful <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a> (<span class="foreignphrase"><em class="foreignphrase">e.g.</em></span>, <a class="link" href="#qc-htb" title="7.1. HTB, Hierarchical Token Bucket">HTB</a>
      and <a class="link" href="#qc-cbq" title="7.4. CBQ, Class Based Queuing (CBQ)">CBQ</a>).  Classes are immensely flexible and can always
      contain either multiple children classes or a single child qdisc
      <a href="#ftn.idm577" class="footnote" name="idm577"><sup class="footnote">[5]</sup></a>.
      There is no prohibition against a class containing a classful qdisc
      itself, which facilitates tremendously complex traffic control
      scenarios.
    </p><p>
      Any class can also have an arbitrary number of <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a>s attached
      to it, which allows the selection of a child class or the use of a
      filter to reclassify or drop traffic entering a particular class.
    </p><p>
      A leaf class is a terminal class in a qdisc.  It contains a qdisc
      (default <a class="link" href="#qs-fifo" title="6.1. FIFO, First-In First-Out (pfifo and bfifo)">FIFO</a>) and will never contain a child class.  Any
      class which contains a child class is an inner class (or root class) and
      not a leaf class.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="c-filter"></a>4.3. <code class="constant">filter</code></h3></div></div></div><p>
      The filter is the most complex component in the Linux
      traffic control system.  The filter provides a convenient mechanism for
      gluing together several of the key elements of traffic control.  The
      simplest and most obvious role of the filter is to classify
      (see <a class="xref" href="#e-classifying" title="3.3. Classifying">Section 3.3, &#8220;Classifying&#8221;</a>) packets.  Linux filters allow the
      user to classify packets into an output queue with either several
      different filters or a single filter.
    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
          A filter must contain a <a class="link" href="#c-classifier" title="4.4. classifier"><code class="constant">classifier</code></a> phrase.
        </p></li><li class="listitem"><p>
          A filter may contain a <a class="link" href="#c-police" title="4.5. policer"><code class="constant">policer</code></a> phrase.
        </p></li></ul></div><p>
      Filters can be attached either to classful <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a>s or to
      <a class="link" href="#c-class" title="4.2. class"><code class="constant">class</code></a>es, however the enqueued packet always enters the root
      qdisc first.  After the filter attached to the root qdisc has been
      traversed, the packet may be directed to any subclasses (which can have
      their own filters) where the packet may undergo further classification.
    </p><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="c-classifier"></a>4.4. classifier</h3></div></div></div><p>
      Filter objects, which can be manipulated using <a class="link" href="#s-iproute2-tc"><span class="command"><strong>tc</strong></span></a>, can use several
      different classifying mechanisms, the most common of which is the
      <code class="constant">u32</code> classifier.  The <code class="constant">u32</code> classifier allows the user to
      select packets based on attributes of the packet.
    </p><p>
      The classifiers are tools which can be used as part of a <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a>
      to identify characteristics of a packet or a packet's metadata.  The
      Linux classfier object is a direct analogue to the basic operation and
      elemental mechanism of traffic control <a class="link" href="#e-classifying" title="3.3. Classifying">classifying</a>.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="c-police"></a>4.5. policer</h3></div></div></div><p>
      This elemental mechanism is only used in Linux traffic control as part
      of a <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a>.  A policer calls one action above and another
      action below the specified rate.  Clever use of policers can simulate
      a three-color meter.  See also
      <a class="xref" href="#diagram" title="10. Diagram">Section 10, &#8220;Diagram&#8221;</a>.
    </p><p>
      Although both <a class="link" href="#e-policing" title="3.4. Policing">policing</a> and <a class="link" href="#e-shaping" title="3.1. Shaping">shaping</a> are basic
      elements of traffic control for limiting bandwidth usage a policer will
      never delay traffic.  It can only perform an action based on specified
      criteria.  See also
      <a class="xref" href="#ex-s-iproute2-tc-filter" title="Example 5. tc filter">Example 5, &#8220;<span class="command">tc</span> <code class="constant">filter</code>&#8221;</a>.
    </p><p>
    </p><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="c-drop"></a>4.6. <code class="constant">drop</code></h3></div></div></div><p>
      This basic traffic control mechanism is only used in Linux traffic
      control as part of a <a class="link" href="#c-police" title="4.5. policer"><code class="constant">policer</code></a>.  Any policer attached to
      any <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a> could have a <a class="link" href="#c-drop" title="4.6. drop"><code class="constant">drop</code></a> action.
    </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
      The only place in the Linux traffic control system where a packet can be
      explicitly dropped is a policer.  A policer can limit packets enqueued
      at a specific rate, or it can be configured to drop all traffic matching
      a particular pattern
      <a href="#ftn.idm639" class="footnote" name="idm639"><sup class="footnote">[6]</sup></a>.
      </p></div><p>
      There are, however, places within the traffic control system where a
      packet may be dropped as a side effect.  For example, a packet will be
      dropped if the scheduler employed uses this method to control flows as
      the <a class="link" href="#qs-gred" title="6.6. GRED, Generic Random Early Drop">GRED</a> does.
    </p><p>
      Also, a shaper or scheduler which runs out of its allocated buffer space
      may have to drop a packet during a particularly bursty or overloaded
      period.
    </p><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="c-handle"></a>4.7. <code class="constant">handle</code></h3></div></div></div><p>
      Every <a class="link" href="#c-class" title="4.2. class"><code class="constant">class</code></a> and classful <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a> (see also
      <a class="xref" href="#classful-qdiscs" title="7. Classful Queuing Disciplines (qdiscs)">Section 7, &#8220;Classful Queuing Disciplines (<code class="constant">qdisc</code>s)&#8221;</a>) requires a unique identifier within
      the traffic control structure.  This unique identifier is known as a
      handle and has two constituent members, a major number and a minor
      number.  These numbers can be assigned arbitrarily by the user in
      accordance with the following rules
      <a href="#ftn.idm661" class="footnote" name="idm661"><sup class="footnote">[7]</sup></a>.
    </p><p>
    </p><div class="variablelist"><p class="title"><b>The numbering of handles for classes and qdiscs</b></p><dl class="variablelist"><dt><span class="term"><em class="parameter"><code>major</code></em></span></dt><dd><p>
            This parameter is completely free of meaning to the kernel.  The
            user may use an arbitrary numbering scheme, however all objects in
            the traffic control structure with the same parent must share a
            <em class="parameter"><code>major</code></em> handle number.  Conventional
            numbering schemes start at 1 for objects attached directly to the
            <code class="constant">root</code> qdisc.
          </p></dd><dt><span class="term"><em class="parameter"><code>minor</code></em></span></dt><dd><p>
            This parameter unambiguously identifies the object as a qdisc if
            <em class="parameter"><code>minor</code></em> is 0.  Any other value identifies the
            object as a class.  All classes sharing a parent must have unique
            <em class="parameter"><code>minor</code></em> numbers.
          </p></dd></dl></div><p>
      The special handle ffff:0 is reserved for the <code class="constant">ingress</code> qdisc.
    </p><p>
      The handle is used as the target in <em class="parameter"><code>classid</code></em> and
      <em class="parameter"><code>flowid</code></em> phrases of <span class="command"><strong>tc</strong></span> <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a> statements.
      These handles are external identifiers for the objects, usable by
      userland applications.  The kernel maintains internal identifiers for
      each object.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="c-txqueuelen"></a>4.8. <code class="constant">txqueuelen</code></h3></div></div></div><p>
          The current size of the transmission queue can be obtained from the
          <span class="command"><strong>ip</strong></span> and <span class="command"><strong>ifconfig</strong></span> commands.
          Confusingly, these commands name the transmission queue length
          differently (emphasized text below):
      </p><pre class="programlisting">
$ifconfig eth0

eth0      Link encap:Ethernet  HWaddr 00:18:F3:51:44:10
          inet addr:69.41.199.58  Bcast:69.41.199.63 Mask:255.255.255.248
          inet6 addr: fe80::218:f3ff:fe51:4410/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:435033 errors:0 dropped:0 overruns:0 frame:0
          TX packets:429919 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 <span class="emphasis"><em>txqueuelen:1000</em></span>
          RX bytes:65651219 (62.6 MiB)  TX bytes:132143593 (126.0 MiB)
          Interrupt:23
      </pre><pre class="programlisting">
$ip link

1: lo:  mtu 16436 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0:  mtu 1500 qdisc pfifo_fast state UP <span class="emphasis"><em>qlen 1000</em></span>
    link/ether 00:18:f3:51:44:10 brd ff:ff:ff:ff:ff:ff
      </pre><p>
          The length of the transmission queue in Linux defaults to 1000
          packets which could represent a large amount of buffering especially
          at low bandwidths. (To understand why, see the discussion on latency
          and throughput, specifically 
          <a class="link" href="#o-bufferbloat">bufferbloat</a>).</p><p>
          More interestingly, <code class="constant">txqueuelen</code> is only used as a default queue
          length for these queueing disciplines.
      </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                  <code class="constant">pfifo_fast</code> (Linux default queueing discipline)
              </p></li><li class="listitem"><p>
                  <code class="constant">sch_fifo</code>
              </p></li><li class="listitem"><p>
                    <code class="constant">sch_gred</code>
              </p></li><li class="listitem"><p>
                    <code class="constant">sch_htb</code> (only for the default queue)
              </p></li><li class="listitem"><p>
                    <code class="constant">sch_plug</code>
              </p></li><li class="listitem"><p>
                    <code class="constant">sch_sfb</code>
              </p></li><li class="listitem"><p>
                    <code class="constant">sch_teql</code>
              </p></li></ul></div><p>
          Looking back at Figure 1, the txqueuelen parameter controls the size of the queues in the Queueing Discipline box for the QDiscs listed above. For most of these queueing disciplines, the &#8220;limit&#8221; argument on the tc command line overrides the txqueuelen default. In summary, if you do not use one of the above queueing disciplines or if you override the queue length then the txqueuelen value is meaningless.
      </p><p>
         The length of the transmission queue is configured with the ip or ifconfig commands.
      </p><pre class="programlisting">
ip link set txqueuelen 500 dev eth0
      </pre><p>
          Notice that the ip command uses &#8220;txqueuelen&#8221; but when displaying the interface details it uses &#8220;qlen&#8221;.
      </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="c-driver-queue"></a>4.9. Driver Queue (aka ring buffer)</h3></div></div></div><p>
              Between the IP stack and the network interface controller (NIC) lies the driver queue. This queue is typically implemented as a first-in, first-out (FIFO) ring buffer &#8211; just think of it as a fixed sized buffer. The driver queue does not contain packet data. Instead it consists of descriptors which point to other data structures called socket kernel buffers (SKBs) which hold the packet data and are used throughout the kernel.
          </p><div class="mediaobject"><a name="img-Figure4"></a><object type="image/svg+xml" data="images/Figure4.svg"></object><div class="caption"><p><span class="command"><strong>Figure 4: </strong></span><span class="emphasis"><em>Partially full driver queue with descriptors pointing to SKBs</em></span></p></div></div><p>
              The input source for the driver queue is the IP stack which queues complete IP packets. The packets may be generated locally or received on one NIC to be routed out another when the device is functioning as an IP router. Packets added to the driver queue by the IP stack are dequeued by the hardware driver and sent across a data bus to the NIC hardware for transmission.
          </p><p>
              The reason the driver queue exists is to ensure that whenever the system has data to transmit, the data is available to the NIC for immediate transmission. That is, the driver queue gives the IP stack a location to queue data asynchronously from the operation of the hardware. One alternative design would be for the NIC to ask the IP stack for data whenever the physical medium is ready to transmit. Since responding to this request cannot be instantaneous this design wastes valuable transmission opportunities resulting in lower throughput. The opposite approach would be for the IP stack to wait after a packet is created until the hardware is ready to transmit. This is also not ideal because the IP stack cannot move on to other work.
          </p><p>
              For detail how to set driver queue see <a class="link" href="#s-ethtool" title="5.6. ethtool, Driver Queue">chapter 5.5</a>.
          </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="c-bql"></a>4.10. Byte Queue Limits (<acronym class="acronym">BQL</acronym>)</h3></div></div></div><p>
        Byte Queue Limits (BQL) is a new feature in recent Linux kernels (&gt; 3.3.0) which attempts to solve the problem of driver queue sizing automatically. This is accomplished by adding a layer which enables and disables queuing to the driver queue based on calculating the minimum buffer size required to avoid <a class="link" href="#o-starv-lat" title="2.10. Starvation and Latency">starvation</a> under the current system conditions. Recall from earlier that the smaller the amount of queued data, the lower the maximum <a class="link" href="#o-starv-lat" title="2.10. Starvation and Latency">latency</a> experienced by queued packets.
    </p><p>
        It is key to understand that the actual size of the driver queue is not changed by BQL. Rather BQL calculates a limit of how much data (in bytes) can be queued at the current time. Any bytes over this limit must be held or dropped by the layers above the driver queue..
    </p><p>
    The BQL mechanism operates when two events occur: when packets are enqueued to the driver queue and when a transmission to the wire has completed. A simplified version of the BQL algorithm is outlined below. LIMIT refers to the value calculated by BQL.
    </p><pre class="programlisting">
****
** After adding packets to the queue
****

if the number of queued bytes is over the current LIMIT value then
        disable the queueing of more data to the driver queue
    </pre><p>
        Notice that the amount of queued data can exceed LIMIT because data is queued before the LIMIT check occurs. Since a large number of bytes can be queued in a single operation when TSO, UFO or GSO (see chapter 2.9.1 aggiungi link for details) are enabled these throughput optimizations have the side effect of allowing a higher than desirable amount of data to be queued. If you care about <a class="link" href="#o-starv-lat" title="2.10. Starvation and Latency">latency</a> you probably want to disable these features.
    </p><p>
       The second stage of BQL is executed after the hardware has completed a transmission (simplified pseudo-code):
    </p><pre class="programlisting">
****
** When the hardware has completed sending a batch of packets
** (Referred to as the end of an interval)
****

if the hardware was starved in the interval
    increase LIMIT

else if the hardware was busy during the entire interval (not starved) and there are bytes to transmit
    decrease LIMIT by the number of bytes not transmitted in the interval

if the number of queued bytes is less than LIMIT
    enable the queueing of more data to the buffer
    </pre><p>
        As you can see, BQL is based on testing whether the device was starved. If it was starved, then LIMIT is increased allowing more data to be queued which reduces the chance of <a class="link" href="#o-starv-lat" title="2.10. Starvation and Latency">starvation</a>. If the device was busy for the entire interval and there are still bytes to be transferred in the queue then the queue is bigger than is necessary for the system under the current conditions and LIMIT is decreased to constrain the <a class="link" href="#o-starv-lat" title="2.10. Starvation and Latency">latency</a>.
    </p><p>
        A real world example may help provide a sense of how much BQL affects the amount of data which can be queued. On one of my servers the driver queue size defaults to 256 descriptors. Since the Ethernet MTU is 1,500 bytes this means up to 256 * 1,500 = 384,000 bytes can be queued to the driver queue (TSO, GSO etc are disabled or this would be much higher). However, the limit value calculated by BQL is 3,012 bytes. As you can see, BQL greatly constrains the amount of data which can be queued.
    </p><p>
        An interesting aspect of BQL can be inferred from the first word in the name &#8211; byte. Unlike the size of the driver queue and most other packet queues, BQL operates on bytes. This is because the number of bytes has a more direct relationship with the time required to transmit to the physical medium than the number of packets or descriptors since the later are variably sized.
    </p><p>
        BQL reduces network <a class="link" href="#o-starv-lat" title="2.10. Starvation and Latency">latency</a> by limiting the amount of queued data to the minimum required to avoid <a class="link" href="#o-starv-lat" title="2.10. Starvation and Latency">starvation</a>. It also has the very important side effect of moving the point where most packets are queued from the driver queue which is a simple FIFO to the queueing discipline (QDisc) layer which is capable of implementing much more complicated queueing strategies. The next section introduces the Linux QDisc layer.
    </p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="idm769"></a>4.10.1. Set BQL</h4></div></div></div><p>
            The BQL algorithm is self tuning so you probably don&#8217;t need to mess with this too much. However, if you are concerned about optimal latencies at low bitrates then you may want override the upper limit on the calculated LIMIT value. BQL state and configuration can be found in a /sys directory based on the location and name of the NIC. On my server the directory for eth0 is:
        </p><pre class="programlisting">
/sys/devices/pci0000:00/0000:00:14.0/net/eth0/queues/tx-0/byte_queue_limits
        </pre><p>
            The files in this directory are:
        </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                    <span class="emphasis"><em>hold_time:</em></span> Time between modifying LIMIT in milliseconds.
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>inflight:</em></span> The number of queued but not yet transmitted bytes.
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>limit:</em></span> The LIMIT value calculated by BQL. 0 if BQL is not supported in the NIC driver.
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>limit_max:</em></span> A configurable maximum value for LIMIT. Set this value lower to optimize for <a class="link" href="#o-starv-lat" title="2.10. Starvation and Latency">latency</a>.
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>limit_min:</em></span> A configurable minimum value for LIMIT. Set this value higher to optimize for throughput.
                </p></li></ul></div><p>
            To place a hard upper limit on the number of bytes which can be queued write the new value to the limit_max fie.
        </p><pre class="programlisting">
echo "3000" &gt; limit_max
        </pre></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="software"></a>5. Software and Tools</h2></div></div></div><p>
  </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="s-kernel"></a>5.1. Kernel requirements</h3></div></div></div><p>
      Many distributions provide kernels with modular or monolithic support
      for traffic control (Quality of Service).  Custom kernels may not
      already provide support (modular or not) for the required features.  If
      not, this is a very brief listing of the required kernel options.
    </p><p>
      The user who has little or no experience compiling a kernel is
      recommended to <a class="ulink" href="http://tldp.org/HOWTO/Kernel-HOWTO/" target="_top">Kernel
         HOWTO</a>.  Experienced kernel compilers should
      be able to determine which of the below options apply to the desired
      configuration, after reading a bit more about traffic control and
      planning.
    </p><div class="example"><a name="ex-s-kernel-options"></a><p class="title"><b>Example 1. Kernel compilation options
        <a href="#ftn.idm803" class="footnote" name="idm803"><sup class="footnote">[8]</sup></a>
      </b></p><div class="example-contents"><pre class="programlisting">
#
# QoS and/or fair queueing
#
CONFIG_NET_SCHED=y
CONFIG_NET_SCH_CBQ=m
CONFIG_NET_SCH_HTB=m
CONFIG_NET_SCH_CSZ=m
CONFIG_NET_SCH_PRIO=m
CONFIG_NET_SCH_RED=m
CONFIG_NET_SCH_SFQ=m
CONFIG_NET_SCH_TEQL=m
CONFIG_NET_SCH_TBF=m
CONFIG_NET_SCH_GRED=m
CONFIG_NET_SCH_DSMARK=m
CONFIG_NET_SCH_INGRESS=m
CONFIG_NET_QOS=y
CONFIG_NET_ESTIMATOR=y
CONFIG_NET_CLS=y
CONFIG_NET_CLS_TCINDEX=m
CONFIG_NET_CLS_ROUTE4=m
CONFIG_NET_CLS_ROUTE=y
CONFIG_NET_CLS_FW=m
CONFIG_NET_CLS_U32=m
CONFIG_NET_CLS_RSVP=m
CONFIG_NET_CLS_RSVP6=m
CONFIG_NET_CLS_POLICE=y
      </pre></div></div><br class="example-break"><p>
      A kernel compiled with the above set of options will provide modular
      support for almost everything discussed in this documentation.  The user
      may need to <span class="command"><strong>modprobe
      <em class="replaceable"><code>module</code></em></strong></span> before using a given
      feature.  Again, the confused user is recommended to the
      <a class="ulink" href="http://tldp.org/HOWTO/Kernel-HOWTO/" target="_top">Kernel
         HOWTO</a>, as this document cannot adequately address questions
      about the use of the Linux kernel.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="s-iproute2"></a>5.2. <span class="command"><strong>iproute2</strong></span> tools (<span class="command"><strong>tc</strong></span>)</h3></div></div></div><p>
      <span class="command"><strong>iproute2</strong></span> is a suite of command line utilities which
      manipulate kernel structures for IP networking
      configuration on a machine.  For technical documentation on these tools,
      see the <a class="ulink" href="http://linux-ip.net/gl/ip-cref/" target="_top">iproute2
         documentation</a> and for a more expository discussion, the
      documentation at <a class="ulink" href="http://linux-ip.net/" target="_top">linux-ip.net</a>.  Of the tools in the <span class="command"><strong>iproute2</strong></span>
      package, the binary <span class="command"><strong>tc</strong></span> is the only one used for traffic control.  This
      HOWTO will ignore the other tools in the suite.
    </p><a name="s-iproute2-tc"></a><p>
      Because it interacts with the kernel to direct the creation, deletion
      and modification of traffic control structures, the <span class="command"><strong>tc</strong></span> binary needs to
      be compiled with support for all of the <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a>s you wish to use.
      In particular, the HTB qdisc is not supported yet in the upstream
      <span class="command"><strong>iproute2</strong></span> package.  See
      <a class="xref" href="#qc-htb" title="7.1. HTB, Hierarchical Token Bucket">Section 7.1, &#8220;HTB, Hierarchical Token Bucket&#8221;</a> for more information.
    </p><p>
      The <span class="command"><strong>tc</strong></span> tool performs all of the configuration of the kernel structures
      required to support traffic control.  As a result of its many uses, the
      command syntax can be described (at best) as arcane.  The utility takes
      as its first non-option argument one of three Linux traffic control
      components, <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a>, <a class="link" href="#c-class" title="4.2. class"><code class="constant">class</code></a> or <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a>.
    </p><div class="example"><a name="ex-s-iproute2-tc"></a><p class="title"><b>Example 2. <span class="command">tc</span> command usage</b></p><div class="example-contents"><pre class="programlisting">
<code class="prompt">[root@leander]# </code><strong class="userinput"><code>tc</code></strong>
<code class="computeroutput">Usage: tc [ OPTIONS ] OBJECT { COMMAND | help }
where  OBJECT := { qdisc | class | filter }
       OPTIONS := { -s[tatistics] | -d[etails] | -r[aw] }</code>
      </pre></div></div><br class="example-break"><p>
      Each object accepts further and different options, and will be
      incompletely described and documented below.  The hints in the examples
      below are designed to introduce the vagaries of <span class="command"><strong>tc</strong></span> command line
      syntax.  For more examples, consult the <a class="ulink" href="http://lartc.org/howto/" target="_top">LARTC HOWTO</a>.  For even
      better understanding, consult the kernel and <span class="command"><strong>iproute2</strong></span> code.
    </p><div class="example"><a name="ex-s-iproute2-tc-qdisc"></a><p class="title"><b>Example 3. <span class="command">tc</span> <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a></b></p><div class="example-contents"><pre class="programlisting">
<code class="prompt">[root@leander]# </code><strong class="userinput"><code>tc qdisc add    \</code></strong> <a class="co" name="ex-s-itcq-tc" href="#ex-s-itcq-tc-text"><span><img src="images/callouts/1.png" alt="1" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 dev eth0     \</code></strong> <a class="co" name="ex-s-itcq-dev" href="#ex-s-itcq-dev-text"><span><img src="images/callouts/2.png" alt="2" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 root         \</code></strong> <a class="co" name="ex-s-itcq-root" href="#ex-s-itcq-root-text"><span><img src="images/callouts/3.png" alt="3" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 handle 1:0   \</code></strong> <a class="co" name="ex-s-itcq-handle" href="#ex-s-itcq-handle-text"><span><img src="images/callouts/4.png" alt="4" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 htb</code></strong>            <a class="co" name="ex-s-itcq-qdisc" href="#ex-s-itcq-qdisc-text"><span><img src="images/callouts/5.png" alt="5" border="0"></span></a>
      </pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcq-tc-text"></a><a href="#ex-s-itcq-tc"><span><img src="images/callouts/1.png" alt="1" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            Add a queuing discipline.  The verb could also be
            <code class="constant">del</code>.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcq-dev-text"></a><a href="#ex-s-itcq-dev"><span><img src="images/callouts/2.png" alt="2" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            Specify the device onto which we are attaching the new queuing
            discipline.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcq-root-text"></a><a href="#ex-s-itcq-root"><span><img src="images/callouts/3.png" alt="3" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            This means <span class="quote">&#8220;<span class="quote">egress</span>&#8221;</span> to <span class="command"><strong>tc</strong></span>.  The word
            <code class="constant">root</code> must be used, however.  Another
            qdisc with limited functionality, the <code class="constant">ingress</code> qdisc can be
            attached to the same device.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcq-handle-text"></a><a href="#ex-s-itcq-handle"><span><img src="images/callouts/4.png" alt="4" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            The <a class="link" href="#c-handle" title="4.7. handle"><code class="constant">handle</code></a> is a user-specified number of the form
            <em class="replaceable"><code>major</code></em>:<em class="replaceable"><code>minor</code></em>.
            The minor number for any queueing discipline handle must always be
            zero (0).  An acceptable shorthand for a <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a> handle is
            the syntax "1:", where the minor number is assumed to be zero (0)
            if not specified.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcq-qdisc-text"></a><a href="#ex-s-itcq-qdisc"><span><img src="images/callouts/5.png" alt="5" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            This is the queuing discipline to attach, HTB in this
            example.  Queuing discipline specific parameters will follow this.
            In the example here, we add no qdisc-specific parameters.
          </p></td></tr></table></div></div></div><br class="example-break"><p>
      Above was the simplest use of the <span class="command"><strong>tc</strong></span> utility for adding a queuing
      discipline to a device.  Here's an example of the use of <span class="command"><strong>tc</strong></span> to add a
      class to an existing parent class.
    </p><div class="example"><a name="ex-s-iproute2-tc-class"></a><p class="title"><b>Example 4. <span class="command">tc</span> <a class="link" href="#c-class" title="4.2. class"><code class="constant">class</code></a></b></p><div class="example-contents"><pre class="programlisting">
<code class="prompt">[root@leander]# </code><strong class="userinput"><code>tc class add    \</code></strong> <a class="co" name="ex-s-itcc-tc" href="#ex-s-itcc-tc-text"><span><img src="images/callouts/1.png" alt="1" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 dev eth0     \</code></strong> <a class="co" name="ex-s-itcc-dev" href="#ex-s-itcc-dev-text"><span><img src="images/callouts/2.png" alt="2" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 parent 1:1   \</code></strong> <a class="co" name="ex-s-itcc-parent" href="#ex-s-itcc-parent-text"><span><img src="images/callouts/3.png" alt="3" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 classid 1:6  \</code></strong> <a class="co" name="ex-s-itcc-classid" href="#ex-s-itcc-classid-text"><span><img src="images/callouts/4.png" alt="4" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 htb          \</code></strong> <a class="co" name="ex-s-itcc-classtype" href="#ex-s-itcc-classtype-text"><span><img src="images/callouts/5.png" alt="5" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 rate 256kbit \</code></strong> <a class="co" name="ex-s-itcc-htb-rate" href="#ex-s-itcc-htb-only-text"><span><img src="images/callouts/6.png" alt="6" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 ceil 512kbit</code></strong>   <a class="co" name="ex-s-itcc-htb-ceil" href="#ex-s-itcc-htb-only-text"><span><img src="images/callouts/7.png" alt="7" border="0"></span></a>
      </pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcc-tc-text"></a><a href="#ex-s-itcc-tc"><span><img src="images/callouts/1.png" alt="1" border="0"></span></a> </p></td><td valign="top" align="left"><p>
             Add a class. The verb could also be <code class="constant">del</code>.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcc-dev-text"></a><a href="#ex-s-itcc-dev"><span><img src="images/callouts/2.png" alt="2" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            Specify the device onto which we are attaching the new class.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcc-parent-text"></a><a href="#ex-s-itcc-parent"><span><img src="images/callouts/3.png" alt="3" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            Specify the parent <a class="link" href="#c-handle" title="4.7. handle"><code class="constant">handle</code></a> to which we are attaching the new class.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcc-classid-text"></a><a href="#ex-s-itcc-classid"><span><img src="images/callouts/4.png" alt="4" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            This is a unique <a class="link" href="#c-handle" title="4.7. handle"><code class="constant">handle</code></a>
            (<em class="replaceable"><code>major</code></em>:<em class="replaceable"><code>minor</code></em>)
            identifying this class.  The minor number must be any non-zero (0)
            number.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcc-classtype-text"></a><a href="#ex-s-itcc-classtype"><span><img src="images/callouts/5.png" alt="5" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            Both of the <a class="link" href="#classful-qdiscs" title="7. Classful Queuing Disciplines (qdiscs)">classful qdiscs</a> require that any children classes be
            classes of the same type as the parent.  Thus an HTB qdisc
            will contain HTB classes.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcc-htb-only-text"></a><a href="#ex-s-itcc-htb-rate"><span><img src="images/callouts/6.png" alt="6" border="0"></span></a> <a href="#ex-s-itcc-htb-ceil"><span><img src="images/callouts/7.png" alt="7" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            This is a class specific parameter.  Consult
            <a class="xref" href="#qc-htb" title="7.1. HTB, Hierarchical Token Bucket">Section 7.1, &#8220;HTB, Hierarchical Token Bucket&#8221;</a> for more detail on these parameters.
          </p></td></tr></table></div></div></div><br class="example-break"><p>
    </p><div class="example"><a name="ex-s-iproute2-tc-filter"></a><p class="title"><b>Example 5. <span class="command">tc</span> <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a></b></p><div class="example-contents"><pre class="programlisting">
<code class="prompt">[root@leander]# </code><strong class="userinput"><code>tc filter add               \</code></strong> <a class="co" name="ex-s-itcf-tc" href="#ex-s-itcf-tc-text"><span><img src="images/callouts/1.png" alt="1" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 dev eth0                 \</code></strong> <a class="co" name="ex-s-itcf-dev" href="#ex-s-itcf-dev-text"><span><img src="images/callouts/2.png" alt="2" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 parent 1:0               \</code></strong> <a class="co" name="ex-s-itcf-parent" href="#ex-s-itcf-parent-text"><span><img src="images/callouts/3.png" alt="3" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 protocol ip              \</code></strong> <a class="co" name="ex-s-itcf-protocol" href="#ex-s-itcf-protocol-text"><span><img src="images/callouts/4.png" alt="4" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 prio 5                   \</code></strong> <a class="co" name="ex-s-itcf-prio" href="#ex-s-itcf-prio-text"><span><img src="images/callouts/5.png" alt="5" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 u32                      \</code></strong> <a class="co" name="ex-s-itcf-classifier" href="#ex-s-itcf-classifier-text"><span><img src="images/callouts/6.png" alt="6" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 match ip port 22 0xffff  \</code></strong> <a class="co" name="ex-s-itcf-match-port" href="#ex-s-itcf-match-text"><span><img src="images/callouts/7.png" alt="7" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 match ip tos 0x10 0xff   \</code></strong> <a class="co" name="ex-s-itcf-match-tos" href="#ex-s-itcf-match-text"><span><img src="images/callouts/8.png" alt="8" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 flowid 1:6               \</code></strong> <a class="co" name="ex-s-itcf-flowid" href="#ex-s-itcf-flowid-text"><span><img src="images/callouts/9.png" alt="9" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 police                   \</code></strong> <a class="co" name="ex-s-itcf-police" href="#ex-s-itcf-police-text"><span><img src="images/callouts/10.png" alt="10" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 rate 32000bps            \</code></strong> <a class="co" name="ex-s-itcf-prate" href="#ex-s-itcf-prate-text"><span><img src="images/callouts/11.png" alt="11" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 burst 10240              \</code></strong> <a class="co" name="ex-s-itcf-burst" href="#ex-s-itcf-burst-text"><span><img src="images/callouts/12.png" alt="12" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 mpu 0                    \</code></strong> <a class="co" name="ex-s-itcf-mpu" href="#ex-s-itcf-mpu-text"><span><img src="images/callouts/13.png" alt="13" border="0"></span></a>
<code class="prompt">&gt; </code><strong class="userinput"><code>                 action drop/continue</code></strong>       <a class="co" name="ex-s-itcf-action" href="#ex-s-itcf-action-text"><span><img src="images/callouts/14.png" alt="14" border="0"></span></a>
      </pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcf-tc-text"></a><a href="#ex-s-itcf-tc"><span><img src="images/callouts/1.png" alt="1" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            Add a filter.  The verb could also be <code class="constant">del</code>.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcf-dev-text"></a><a href="#ex-s-itcf-dev"><span><img src="images/callouts/2.png" alt="2" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            Specify the device onto which we are attaching the new filter.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcf-parent-text"></a><a href="#ex-s-itcf-parent"><span><img src="images/callouts/3.png" alt="3" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            Specify the parent handle to which we are attaching the new
            filter.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcf-protocol-text"></a><a href="#ex-s-itcf-protocol"><span><img src="images/callouts/4.png" alt="4" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            This parameter is required.  It's use should be obvious, although
            I don't know more.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcf-prio-text"></a><a href="#ex-s-itcf-prio"><span><img src="images/callouts/5.png" alt="5" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            The <em class="parameter"><code>prio</code></em> parameter allows a given filter to
            be preferred above another.  The <em class="parameter"><code>pref</code></em> is a
            synonym.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcf-classifier-text"></a><a href="#ex-s-itcf-classifier"><span><img src="images/callouts/6.png" alt="6" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            This is a <a class="link" href="#c-classifier" title="4.4. classifier"><code class="constant">classifier</code></a>, and is a required phrase in every
            <span class="command"><strong>tc</strong></span> <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a> command.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcf-match-text"></a><a href="#ex-s-itcf-match-port"><span><img src="images/callouts/7.png" alt="7" border="0"></span></a> <a href="#ex-s-itcf-match-tos"><span><img src="images/callouts/8.png" alt="8" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            These are parameters to the classifier.  In this case, packets
            with a type of service flag (indicating interactive usage) and
            matching port 22 will be selected by this statement.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcf-flowid-text"></a><a href="#ex-s-itcf-flowid"><span><img src="images/callouts/9.png" alt="9" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            The <em class="parameter"><code>flowid</code></em> specifies the <a class="link" href="#c-handle" title="4.7. handle"><code class="constant">handle</code></a> of
            the target class (or qdisc) to which a matching filter should send
            its selected packets.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcf-police-text"></a><a href="#ex-s-itcf-police"><span><img src="images/callouts/10.png" alt="10" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            This is the <a class="link" href="#c-police" title="4.5. policer"><code class="constant">policer</code></a>, and is an optional phrase in every
            <span class="command"><strong>tc</strong></span> <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a> command.
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcf-prate-text"></a><a href="#ex-s-itcf-prate"><span><img src="images/callouts/11.png" alt="11" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            The policer will perform one action above this rate, and another
            action below (see
            <a class="xref" href="#ex-s-itcf-action-text">action parameter</a>).
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcf-burst-text"></a><a href="#ex-s-itcf-burst"><span><img src="images/callouts/12.png" alt="12" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            The <em class="parameter"><code>burst</code></em> is an exact analog to <em class="parameter"><code>burst</code></em> in
            <a class="link" href="#qc-htb" title="7.1. HTB, Hierarchical Token Bucket">HTB</a> (<em class="parameter"><code>burst</code></em> is a <a class="link" href="#o-buckets">buckets</a> concept).
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcf-mpu-text"></a><a href="#ex-s-itcf-mpu"><span><img src="images/callouts/13.png" alt="13" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            The minimum policed unit.  To count all traffic, use an
            <em class="parameter"><code>mpu</code></em> of zero (0).
          </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a name="ex-s-itcf-action-text"></a><a href="#ex-s-itcf-action"><span><img src="images/callouts/14.png" alt="14" border="0"></span></a> </p></td><td valign="top" align="left"><p>
            The <em class="parameter"><code>action</code></em> indicates what should be done if
            the <em class="parameter"><code>rate</code></em> based on the attributes of the policer.  The
            first word specifies the action to take if the policer has been
            exceeded.  The second word specifies action to take otherwise.
          </p></td></tr></table></div></div></div><br class="example-break"><p>
      As evidenced above, the <span class="command"><strong>tc</strong></span> command line utility has an arcane and
      complex syntax, even for simple operations such as these examples show.
      It should come as no surprised to the reader that there exists an easier
      way to configure Linux traffic control.  See the next section,
      <a class="xref" href="#s-tcng" title="5.3. tcng, Traffic Control Next Generation">Section 5.3, &#8220;<span class="command"><strong>tcng</strong></span>, Traffic Control Next Generation&#8221;</a>.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="s-tcng"></a>5.3. <span class="command"><strong>tcng</strong></span>, Traffic Control Next Generation</h3></div></div></div><p>
      FIXME; sing the praises of tcng.  See also <a class="ulink" href="http://tldp.org/HOWTO/Traffic-Control-tcng-HTB-HOWTO/" target="_top">
           Traffic Control using tcng and HTB HOWTO</a> and
      <a class="ulink" href="http://linux-ip.net/gl/tcng/" target="_top">tcng
         documentation</a>.
    </p><p>
      Traffic control next generation (hereafter, <span class="command"><strong>tcng</strong></span>) provides all of the
      power of traffic control under Linux with twenty percent of the
      headache.
    </p><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="s-netfilter"></a>5.4. Netfilter</h3></div></div></div><p>
         Netfilter is a framework provided by the Linux kernel that allows various networking-related operations to be implemented in the form of customized handlers. Netfilter offers various functions and operations for packet filtering, network address translation, and port translation, which provide the functionality required for directing packets through a network, as well as for providing ability to prohibit packets from reaching sensitive locations within a computer network.
      </p><p>
          Netfilter represents a set of hooks inside the Linux kernel, allowing specific kernel modules to register callback functions with the kernel's networking stack. Those functions, usually applied to the traffic in form of filtering and modification rules, are called for every packet that traverses the respective hook within the networking stack.
      </p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="s-iptables"></a>5.4.1. <code class="constant">iptables</code></h4></div></div></div><p>
              iptables is a user-space application program that allows a system administrator to configure the tables provided by the Linux kernel firewall (implemented as different Netfilter modules) and the chains and rules it stores. Different kernel modules and programs are currently used for different protocols; iptables applies to IPv4, ip6tables to IPv6, arptables to ARP, and ebtables to Ethernet frames.
          </p><p>
              iptables requires elevated privileges to operate and must be executed by user root, otherwise it fails to function. On most Linux systems, iptables is installed as /usr/sbin/iptables and documented in its man pages, which can be opened using man iptables when installed. It may also be found in /sbin/iptables, but since iptables is more like a service rather than an "essential binary", the preferred location remains /usr/sbin.
          </p><p>
              The term iptables is also commonly used to inclusively refer to the kernel-level components. x_tables is the name of the kernel module carrying the shared code portion used by all four modules that also provides the API used for extensions; subsequently, Xtables is more or less used to refer to the entire firewall (v4, v6, arp, and eb) architecture.
          </p><p>
              Xtables allows the system administrator to define tables containing chains of rules for the treatment of packets. Each table is associated with a different kind of packet processing. Packets are processed by sequentially traversing the rules in chains. A rule in a chain can cause a goto or jump to another chain, and this can be repeated to whatever level of nesting is desired. (A jump is like a &#8220;call&#8221;, i.e. the point that was jumped from is remembered.) Every network packet arriving at or leaving from the computer traverses at least one chain.
          </p><div class="mediaobject"><a name="img-Figure5"></a><object type="image/svg+xml" data="images/Figure5.svg"></object><div class="caption"><p><span class="command"><strong>Figure 5: </strong></span><span class="emphasis"><em>Packet flow paths. Packets start at a given box and will flow along a certain path, depending on the circumstances.</em></span>
                  </p></div></div><p>
              The origin of the packet determines which chain it traverses initially. There are five predefined chains (mapping to the five available Netfilter hooks, see figure 5), though a table may not have all chains.
          </p><div class="mediaobject"><a name="img-Figure6"></a><object type="image/svg+xml" data="images/Figure6.svg"></object><div class="caption"><p><span class="command"><strong>Figure 6: </strong></span><span class="emphasis"><em>netfilter&#8217;s hook</em></span>
                  </p></div></div><p>
              Predefined chains have a policy, for example DROP, which is applied to the packet if it reaches the end of the chain. The system administrator can create as many other chains as desired. These chains have no policy; if a packet reaches the end of the chain it is returned to the chain which called it. A chain may be empty.
          </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                      <code class="constant">PREROUTING:</code> Packets will enter this chain before a routing decision is made (point 1 in Figure 6).
                  </p></li><li class="listitem"><p>
                      <code class="constant">INPUT:</code> Packet is going to be locally delivered. It does not have anything to do with processes having an opened socket; local delivery is controlled by the "local-delivery" routing table: ip route show table local (point 2 Figure 6).
                  </p></li><li class="listitem"><p>
                      <code class="constant">FORWARD:</code> All packets that have been routed and were not for local delivery will traverse this chain (point 3 in Figure 6).
                  </p></li><li class="listitem"><p>
                      <code class="constant">OUTPUT:</code> Packets sent from the machine itself will be visiting this chain (point 5 in Figure 6)
                  </p></li><li class="listitem"><p>
                      <code class="constant">POSTROUTING:</code> Routing decision has been made. Packets enter this chain just before handing them off to the hardware (point 4 in Figure 6).
                  </p></li></ul></div><p>
              Each rule in a chain contains the specification of which packets it matches. It may also contain a target (used for extensions) or verdict (one of the built-in decisions). As a packet traverses a chain, each rule in turn is examined. If a rule does not match the packet, the packet is passed to the next rule. If a rule does match the packet, the rule takes the action indicated by the target/verdict, which may result in the packet being allowed to continue along the chain or it may not. Matches make up the large part of rulesets, as they contain the conditions packets are tested for. These can happen for about any layer in the OSI model, as with e.g. the --mac-source and -p tcp --dport parameters, and there are also protocol-independent matches, such as -m time.
          </p><p>
              The packet continues to traverse the chain until either
          </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                      a rule matches the packet and decides the ultimate fate of the packet, for example by calling one of the <code class="constant">ACCEPT</code> or <code class="constant">DROP</code>, or a module returning such an ultimate fate; or
                  </p></li><li class="listitem"><p>
                      a rule calls the <code class="constant">RETURN</code> verdict, in which case processing returns to the calling chain; or
                  </p></li><li class="listitem"><p>
                     the end of the chain is reached; traversal either continues in the parent chain (as if RETURN was used), or the base chain policy, which is an ultimate fate, is used.
                  </p></li></ul></div><p>
              Targets also return a verdict like ACCEPT (NAT modules will do this) or DROP (e.g. the REJECT module), but may also imply CONTINUE (e.g. the LOG module; CONTINUE is an internal name) to continue with the next rule as if no target/verdict was specified at all.
          </p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="s-imq"></a>5.5. IMQ, Intermediate Queuing device</h3></div></div></div><p>
        The Intermediate queueing device is not a qdisc but its usage is tightly bound to qdiscs. Within linux, qdiscs are attached to network devices and everything that is queued to the device is first queued to the qdisc and then to driver queue. From this concept, two limitations arise:
    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                Only egress shaping is possible (an ingress qdisc exists, but its possibilities are very limited compared to classful qdiscs, as seen before).
            </p></li><li class="listitem"><p>
                A qdisc can only see traffic of one interface, global limitations can't be placed.
            </p></li></ul></div><p>
      IMQ is there to help solve those two limitations. In short, you can put everything you choose in a qdisc. Specially marked packets get intercepted in netfilter NF_IP_PRE_ROUTING and NF_IP_POST_ROUTING hooks and pass through the qdisc attached to an imq device. An iptables target is used for marking the packets.
    </p><p>
        This enables you to do ingress shaping as you can just mark packets coming in from somewhere and/or treat interfaces as classes to set global limits. You can also do lots of other stuff like just putting your http traffic in a qdisc, put new connection requests in a qdisc, exc.
    </p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="s-sample-configuration"></a>5.5.1. Sample configuration</h4></div></div></div><p>
            The first thing that might come to mind is use ingress shaping to give yourself a high guaranteed bandwidth. Configuration is just like with any other interface:
        </p><pre class="programlisting">
tc qdisc add dev imq0 root handle 1: htb default 20

tc class add dev imq0 parent 1: classid 1:1 htb rate 2mbit burst 15k

tc class add dev imq0 parent 1:1 classid 1:10 htb rate 1mbit
tc class add dev imq0 parent 1:1 classid 1:20 htb rate 1mbit

tc qdisc add dev imq0 parent 1:10 handle 10: pfifo
tc qdisc add dev imq0 parent 1:20 handle 20: sfq
        </pre><pre class="programlisting">
tc filter add dev imq0 parent 10:0 protocol ip prio 1 u32 match \ ip dst 10.0.0.230/32 flowid 1:10
        </pre><p>
            In this example u32 is used for classification. Other classifiers should work as expected. Next traffic has to be selected and marked to be enqueued to imq0.
        </p><pre class="programlisting">
iptables -t mangle -A PREROUTING -i eth0 -j IMQ --todev 0

ip link set imq0 up
        </pre><p>
            The IMQ iptables targets is valid in the PREROUTING and POSTROUTING chains of the mangle table. It's syntax is
        </p><pre class="programlisting">
IMQ [ --todev n ]	n : number of imq device
        </pre><p>
            An ip6tables target is also provided.
        </p><p>
            Please note traffic is not enqueued when the target is hit but afterwards. The exact location where traffic enters the imq device depends on the direction of the traffic (in/out). These are the predefined netfilter hooks used by iptables:
        </p><pre class="programlisting">
enum nf_ip_hook_priorities {
            NF_IP_PRI_FIRST = INT_MIN,
            NF_IP_PRI_CONNTRACK = -200,
            NF_IP_PRI_MANGLE = -150,
            NF_IP_PRI_NAT_DST = -100,
            NF_IP_PRI_FILTER = 0,
            NF_IP_PRI_NAT_SRC = 100,
            NF_IP_PRI_LAST = INT_MAX,
            };
        </pre><p>
            For ingress traffic, imq registers itself with NF_IP_PRI_MANGLE + 1 priority which means packets enter the imq device directly after the mangle PREROUTING chain has been passed.
        </p><p>
          For egress imq uses NF_IP_PRI_LAST which honours the fact that packets dropped by the filter table won't occupy bandwidth.
        </p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="s-ethtool"></a>5.6. <code class="constant">ethtool,</code> Driver Queue</h3></div></div></div><p>
 The ethtool command is used to control the driver queue size for Ethernet devices. ethtool also provides low level interface statistics as well as the ability to enable and disable IP stack and driver features.
    </p><p>
        The -g flag to ethtool displays the driver queue (ring) parameters (see Figure 1) :
    </p><pre class="programlisting">
$ethtool -g eth0

    Ring parameters for eth0:
    Pre-set maximums:
    RX:        16384
    RX Mini:    0
    RX Jumbo:    0
    TX:        16384
    Current hardware settings:
    RX:        512
    RX Mini:    0
    RX Jumbo:    0
    TX:        256
    </pre><p>
        You can see from the above output that the driver for this NIC defaults to 256 descriptors in the transmission queue. It was often recommended to reduce the size of the driver queue in order to reduce latency. With the introduction of BQL (assuming your NIC driver supports it) there is no longer any reason to modify the driver queue size (see the below for how to configure BQL).
    </p><p>
        Ethtool also allows you to manage optimization features such as <a class="link" href="#o-huge-packet" title="2.9.1. Huge Packets from the Stack">TSO, UFO and GSO</a>. The -k flag displays the current offload settings and -K modifies them.
    </p><pre class="programlisting">
$ethtool -k eth0

    Offload parameters for eth0:
    rx-checksumming: off
    tx-checksumming: off
    scatter-gather: off
    tcp-segmentation-offload: off
    udp-fragmentation-offload: off
    generic-segmentation-offload: off
    generic-receive-offload: on
    large-receive-offload: off
    rx-vlan-offload: off
    tx-vlan-offload: off
    ntuple-filters: off
    receive-hashing: off
    </pre><p>
        Since <a class="link" href="#o-huge-packet" title="2.9.1. Huge Packets from the Stack">TSO, GSO, UFO</a> and GRO greatly increase the number of bytes which can be queued in the driver queue you should disable these optimizations if you want to optimize for latency over throughput. It&#8217;s doubtful you will notice any CPU impact or throughput decrease when disabling these features unless the system is handling very high data rates.
    </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="classless-qdiscs"></a>6. Classless Queuing Disciplines (<a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a>s)</h2></div></div></div><p>
    Each of these queuing disciplines can be used as the primary qdisc on an
    interface, or can be used inside a leaf class of a <a class="link" href="#classful-qdiscs" title="7. Classful Queuing Disciplines (qdiscs)">classful qdiscs</a>.
    These are the fundamental schedulers used under Linux.  Note that the
    default scheduler is the <a class="link" href="#qs-pfifo_fast" title="6.2. pfifo_fast, the default Linux qdisc"><code class="constant">pfifo_fast</code></a>.
  </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="qs-fifo"></a>6.1. FIFO, First-In First-Out (<code class="constant">pfifo</code> and <code class="constant">bfifo</code>)</h3></div></div></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
      Although a FIFO is one of the simplest elements of a queueing system,
      neither <code class="constant">pfifo</code> nor <code class="constant">bfifo</code> is the default qdisc on Linux
      interfaces.  Be certain to see
      <a class="xref" href="#qs-pfifo_fast" title="6.2. pfifo_fast, the default Linux qdisc">Section 6.2, &#8220;<code class="constant">pfifo_fast</code>, the default Linux qdisc&#8221;</a> for the full details on the default
      (<code class="constant">pfifo_fast</code>) qdisc.
      </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qs-fifo-algorithm"></a>6.1.1. <code class="constant">pfifo, bfifo</code> Algorithm</h4></div></div></div><p>
      The FIFO algorithm forms the underlying basis for the default qdisc on
      all Linux
      network interfaces (<a class="link" href="#qs-pfifo_fast" title="6.2. pfifo_fast, the default Linux qdisc"><code class="constant">pfifo_fast</code></a>).  It performs no shaping or
      rearranging of packets.  It simply transmits packets as soon as it can
      after receiving and queuing them.  This is also the qdisc used inside
      all newly created classes until another qdisc or a class replaces the
      FIFO.
    </p><div class="mediaobject"><a name="img-qs-fifo"></a><object type="image/svg+xml" data="images/fifo-qdisc.svg"></object><div class="caption"><p><span class="command"><strong>Figure 7:</strong></span> <span class="emphasis"><em>FIFO qdisc</em></span></p></div></div><p>
        A list  of  packets is maintained, when a packet is enqueued it gets inserted at the tail of a list. When a packet needs to be sent  out  to the network, it is taken from the head of the list.
    </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="idm1196"></a>6.1.2. <code class="constant">limit</code> parameter</h4></div></div></div><p>
      A real FIFO qdisc must, however, have a size limit (a buffer size) to
      prevent it from overflowing in case it is unable to dequeue packets as
      quickly as it receives them.  Linux implements two basic FIFO
      <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a>s, one based on bytes, and one on packets.  Regardless of
      the type of FIFO used, the size of the queue is defined by the parameter
      <em class="parameter"><code>limit</code></em>.  For a <code class="constant">pfifo</code> <em class="parameter"><code>(Packet limited First In, First Out queue)</code></em>  the unit is understood
      to be packets and for a <code class="constant">bfifo</code><em class="parameter"><code> (Byte limited First In, First Out queue) </code></em> the unit is understood to be bytes.
    </p><p>
        For pfifo, defaults to the interface <a class="link" href="#c-txqueuelen" title="4.8. txqueuelen">txqueuelen</a> , as specified with <a class="link" href="#c-txqueuelen" title="4.8. txqueuelen">ifconfig</a> or <a class="link" href="#c-txqueuelen" title="4.8. txqueuelen">ip</a>.  The range for this  parameter is [0, UINT32_MAX].
    </p><p>
        For bfifo, it  defaults to the <a class="link" href="#c-txqueuelen" title="4.8. txqueuelen">txqueuelen</a> multiplied by the interface MTU.  The range for this parameter is [0,  UINT32_MAX] bytes. Note: The link layer header was considered when counting packet length.
    </p><div class="example"><a name="ex-qs-fifo-limit"></a><p class="title"><b>Example 6. Specifying a <em class="parameter"><code>limit</code></em> for a packet
        or byte FIFO</b></p><div class="example-contents"><pre class="programlisting">
<code class="prompt">[root@leander]# </code><strong class="userinput"><code>cat bfifo.tcc</code></strong>
<code class="computeroutput">/*
 * make a FIFO on eth0 with 10kbyte queue size
 *
 */

dev eth0 {
    egress {
        fifo (limit 10kB );
    }
}</code>
<code class="prompt">[root@leander]# </code><strong class="userinput"><code>tcc &lt; bfifo.tcc</code></strong>
<code class="computeroutput"># ================================ Device eth0 ================================

tc qdisc add dev eth0 handle 1:0 root dsmark indices 1 default_index 0
tc qdisc add dev eth0 handle 2:0 parent 1:0 bfifo limit 10240</code>
<code class="prompt">[root@leander]# </code><strong class="userinput"><code>cat pfifo.tcc</code></strong>
<code class="computeroutput">/*
 * make a FIFO on eth0 with 30 packet queue size
 *
 */

dev eth0 {
    egress {
        fifo (limit 30p );
    }
}</code>
<code class="prompt">[root@leander]# </code><strong class="userinput"><code>tcc &lt; pfifo.tcc</code></strong>
<code class="computeroutput"># ================================ Device eth0 ================================

tc qdisc add dev eth0 handle 1:0 root dsmark indices 1 default_index 0
tc qdisc add dev eth0 handle 2:0 parent 1:0 pfifo limit 30</code>
      </pre></div></div><br class="example-break"></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qd-pfifo-ls"></a>6.1.3. tc &#8211;s qdisc ls</h4></div></div></div><p>
         The output of tc -s qdisc ls contains the limit, either in packets or in bytes, and the number of bytes and packets actually sent. An unsent and dropped packet only appears between braces and is  not  counted  as 'Sent'.
     </p><p>
         In this example, the queue length is 100 packets, 45894 bytes were sentover 681 packets.  No packets were dropped, and as the pfifo queue does not slow down packets, there were also no overlimits:
     </p><pre class="programlisting">
$ tc -s qdisc ls dev eth0

    qdisc pfifo 8001: dev eth0 limit 100p
    Sent 45894 bytes 681 pkts (dropped 0, overlimits 0)
     </pre><p>
          If a backlog occurs, this is displayed as well.
     </p><p>
         <code class="constant">pfifo</code> and <code class="constant">bfifo</code>, like all non-default qdiscs, maintain statistics. This might be a reason to prefer that over the default.
     </p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="qs-pfifo_fast"></a>6.2. <code class="constant">pfifo_fast</code>, the default Linux qdisc</h3></div></div></div><p>
        The <code class="constant">pfifo_fast</code> <em class="parameter"><code>(three-band first in, first out queue)</code></em> qdisc is the default qdisc for all interfaces under
        Linux. Whenever an interface is created, the pfifo_fast qdisc is automatically used as a queue. If another qdisc is attached, it preempts the default pfifo_fast, which  automatically  returns to function when an existing qdisc is detached.
    </p><div class="mediaobject"><a name="img-qs-pfifo_fast"></a><object type="image/svg+xml" data="images/pfifo_fast-qdisc.svg"></object><div class="caption"><p><span class="command"><strong>Figure 8:</strong></span> <span class="emphasis"><em>pfifo_fast qdisc</em></span></p></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qd-pfifo_fast-algorithm"></a>6.2.1. <code class="constant">pfifo_fast</code> algorithm</h4></div></div></div><p>
            Based on a conventional <a class="link" href="#qs-fifo" title="6.1. FIFO, First-In First-Out (pfifo and bfifo)">FIFO</a> <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a>, this qdisc also provides some prioritization. It provides three different bands (individual FIFOs) for separating traffic. The highest priority traffic (interactive flows) are placed into band 0 and are always serviced first. Similarly, band 1 is always emptied of pending packets before band 2 is dequeued.
            </p><p>
                The algorithm is very similar to that of the classful 
                <a class="link" href="#qc-prio" title="7.3. PRIO, priority scheduler">prio qdisc</a>.  The pfifo_fast qdisc is like
                three <code class="constant">pfifo</code> queues side by side, where an individual packet will
                be enqueued in one of the three FIFOs based on its Type of
                Service (<acronym class="acronym">ToS</acronym>) bits.
                Not all three bands are dequeued simultaneously - as long as
                lower-numbered (higher priority) bands have traffic,
                higher-numbered bands are never dequeued.  This can be used to
                prioritize interactive traffic or penalize 'lowest cost'
                traffic. Each band can be txqueuelen packets long, as
                configured with 
                <a class="link" href="#c-txqueuelen" title="4.8. txqueuelen">ifconfig</a> or 
                <a class="link" href="#c-txqueuelen" title="4.8. txqueuelen">ip</a>.  Any additional
                packets arriving, once a specific band is full, are not
                enqueued but are instead dropped.
            </p><p>
                See <a class="link" href="#qd-pfifo_fast-bugs" title="6.2.3. Bugs">chapter 6.2.3</a> for complete details on how ToS bits are translated into bands.
            </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qd-pfifo_fast-parameter"></a>6.2.2. <code class="constant">txqueuelen</code> parameter</h4></div></div></div><p>
                    The length of the three bands depends on the interface <a class="link" href="#c-txqueuelen" title="4.8. txqueuelen">txqueuelen</a>, as specified with <a class="link" href="#c-txqueuelen" title="4.8. txqueuelen">ifconfig</a> or <a class="link" href="#c-txqueuelen" title="4.8. txqueuelen">ip</a>.
                </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qd-pfifo_fast-bugs"></a>6.2.3. Bugs</h4></div></div></div><p>
                    There is nothing configurable to the end user about the pfifo_fast qdisc. This is how it is configured by default:
                </p><p>
                    <code class="constant">priomap</code> determines how packet priorities, as assigned by the kernel, map to bands. Mapping occurs based on the ToS octet of the packet, which looks like this:
                </p><div class="mediaobject"><a name="bugs1"></a><object type="image/svg+xml" data="images/bugs1.svg"></object></div><p>
                    The four ToS bits (the 'ToS field') are defined as:
                </p><div class="mediaobject"><a name="bugs2"></a><object type="image/svg+xml" data="images/bugs2.svg"></object></div><p>
                    As there is 1 bit to the right of these four bits, the actual value of the ToS field is double the value of the ToS bits. Running <span class="command"><strong>tcpdump -v -v</strong></span> shows you the value of the entire ToS field, not just the four bits. It is the value you see in the first column of this table:

                </p><div class="mediaobject"><a name="bugs3"></a><object type="image/svg+xml" data="images/bugs3.svg"></object></div><p>
                    Lots of numbers. The second column contains the value of the relevant four ToS bits, followed by their translated meaning. For example, 15 stands for a packet wanting Minimal Monetary Cost, Maximum Reliability, Maximum Throughput AND Minimum Delay.
                </p><p>
                    The fourth column lists the way the Linux kernel interprets the ToS bits, by showing to which Priority they are mapped.
                </p><p>
                    The last column shows the result of the default priomap. On the command line, the default priomap looks like this:
                </p><p>
                    1, 2, 2, 2, 1, 2, 0, 0 , 1, 1, 1, 1, 1, 1, 1, 1
                </p><p>
                    This means that priority 4, for example, gets mapped to band number 1. The priomap also allows you to list higher priorities (&gt; 7) which do not correspond to ToS mappings, but which are set by other means.
                </p><p>
                    Moreover, differently from other non standard qdisc, pfifo_fast does not maintain statistics and does not show up in tc qdisc ls. This is because it is the automatic default in the absence of  a  configured qdisc.
                </p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="qs-sfq"></a>6.3. SFQ, Stochastic Fair Queuing</h3></div></div></div><p>
      Stochastic Fairness Queueing is a classless queueing discipline available for traffic control with the tc command. SFQ does not shape traffic but only schedules the transmission of packets, based on 'flows'.  The goal is to ensure fairness so that each flow is able to send data in turn, thus preventing any single flow from drowning out the rest. It accomplishes this by using a hash function to separate the traffic into separate (internally maintained) FIFOs which are dequeued in a round-robin fashion. Because there is the possibility for unfairness to manifest in the choice of hash function, this function is altered periodically (<a class="link" href="#qs-sfq-algorithm" title="6.3.1. SFQ Algorithm">see 6.3.1 algorithm</a>). Perturbation (the parameter <span class="emphasis"><em>perturb</em></span>) sets this periodicity (<a class="link" href="#qd-sfq-parameters" title="6.3.3. Parameters">see 6.3.3 parameters</a>). This may in fact have some effect in mitigating a Denial of Service attempt. SFQ is work-conserving and therefore always delivers a packet if it has one available.
    </p><p>
        So, the SFQ qdisc attempts to fairly distribute opportunity to transmit data to the network among an arbitrary number of flows.
    </p><div class="mediaobject"><a name="img-qs-sfq"></a><object type="image/svg+xml" data="images/sfq-qdisc.svg"></object><div class="caption"><p><span class="command"><strong>Figure 9:</strong></span> <span class="emphasis"><em> Stochastic Fair Queuing (SFQ)</em></span> </p></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qs-sfq-algorithm"></a>6.3.1. SFQ Algorithm</h4></div></div></div><p>
         On enqueueing, each packet is assigned to a hash bucket, based on the packets hash value.  This hash value is either obtained from an external flow classifier (use tc filter to set them), or a default internal classifier if no external classifier has been configured. When the internal classifier is used, sfq uses
        </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                    Source address;
                </p></li><li class="listitem"><p>
                    Destination address;
                </p></li><li class="listitem"><p>
                    Source and Destination port;
                </p></li></ul></div><p>
            if these are available.
        </p><p>
            SFQ knows about ipv4 and ipv6 and also UDP, TCP and ESP.  Packets with other protocols are hashed based on the  32bits representation  of  their  destination  and  source. A flow corresponds mostly to a TCP/IP connection.
        </p><p>
            Each of these buckets should represent a unique flow. Because multiple flows may  get  hashed to the same bucket, sfqs internal hashing algorithm may be perturbed at configurable intervals so that the unfairness lasts only for a short while. Perturbation may however cause some inadvertent packet reordering to occur. After linux-3.3, there is no packet reordering  problem, but  possible packet drops if rehashing hits one limit (number of flows or packets per flow)
        </p><p>
            When dequeuing, each hashbucket with data is queried in a round robin fashion.
        </p><p>
            Before  linux-3.3,  the  compile  time maximum length of the SFQ is 128 packets, which can be spread over at most 128 buckets  of  1024  available.  In case  of  overflow,  tail-drop  is  performed on the fullest bucket, thus maintaining fairness.
        </p><p>
            After linux-3.3, maximum length of SFQ is 65535 packets,  and  divisor limit  is  65536.   In case of overflow, tail-drop is performed on the fullest bucket, unless headdrop was requested.
        </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qd-sfq-synopsis"></a>6.3.2. Synopsis</h4></div></div></div><pre class="programlisting">
tc  qdisc ... [divisor hashtablesize] [limit packets] [perturb seconds] [quantum bytes] [flows number] [depth number] [head drop] [redflowlimit bytes] [min bytes] [max bytes] [avpkt bytes] [burst packets] [probability P] [ecn] [harddrop]
        </pre><p>
        </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qd-sfq-parameters"></a>6.3.3. Parameters</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                    <span class="emphasis"><em>divisor:</em></span> can be used to set a different hash table size,  available  from kernel 2.6.39 onwards.  The specified divisor must be a power of two and cannot be larger than 65536.  Default value is 1024.
                </p></li><li class="listitem"><p>

                    <span class="emphasis"><em>limit:</em></span>  upper limit of the SFQ. Can be used to reduce the default length of 127 packets.  After linux-3.3, it can be raised.

                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>depth:</em></span>  limit of packets per flow (after linux-3.3). Default to 127 and can be lowered.

                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>perturb:</em></span> interval in seconds for queue algorithm perturbation.  Defaults to 0,  which  means that no perturbation occurs. Do not set too low for each perturbation may cause some packet reordering or losses. Advised value is 60. This value has no effect when external flow classification is used.  Its  better to  increase  divisor value to lower risk of hash collisions.

                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>quantum:</em></span> amount  of  bytes a flow is allowed to dequeue during a round of the round robin process.  Defaults to the MTU of the interface which is also the advised value and the minimum value.

                </p></li><li class="listitem"><p>

                    <span class="emphasis"><em>flows:</em></span>  after  linux-3.3,  it is possible to change the default limit of flows.  Default value is 127.
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>headdrop:</em></span> default SFQ behavior is to perform tail-drop of packets  from  a flow. You can ask a headdrop instead, as this is known to provide a better feedback for TCP flows
                </p></li><li class="listitem"><p><span class="emphasis"><em>redflowlimit:</em></span> configure the optional RED module on top of each SFQ flow.  Random  Early  Detection  principle  is  to perform packet marks or drops in a probabilistic way. Redflowlimit configures the hard limit on the real (not average) queue size per SFQ flow in bytes.
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>min:</em></span> average  queue  size  at  which  marking  becomes a possibility. Defaults to max /3.
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>max:</em></span>  at this average queue size, the marking probability is  maximal. Defaults to redflowlimit /4.
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>probability:</em></span> maximum   probability   for   marking,  specified  as a floating point number from 0.0 to 1.0. Default value is 0.02.
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>avpkt:</em></span>  specified in bytes. Used with burst to determine the  time  constant for average queue size calculations. Default value is 1000.
                </p></li><li class="listitem"><p><span class="emphasis"><em>burst:</em></span>  used  for  determining how fast the average queue size is influenced by the real queue size. Default value is: (2 * min + max) / (3 * avpkt).
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>ecn:</em></span> RED can either 'mark' or 'drop'. Explicit Congestion  Notification  allows RED to notify remote hosts that their rate exceeds the amount of bandwidth available.  Non-ECN capable  hosts  can only  be  notified  by  dropping a packet.  If this parameter is specified, packets which indicate that  their  hosts  honor  ECN will  only be marked and not dropped, unless the queue size hits depth packets.
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>harddrop:</em></span> if average flow queue size is above max  bytes,  this  parameter forces a drop instead of ecn marking.
                </p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="idm1402"></a>6.3.4. Example and Usage</h4></div></div></div><p>
        To attach to device ppp0:
    </p><pre class="programlisting">
$ tc qdisc add dev ppp0 root sfq
    </pre><p>
        Please note that SFQ, like all non-shaping (work-conserving) qdiscs, is only useful if it owns the queue.  This is the case when the link speed equals  the  actually available bandwidth. This holds for regular phone modems, ISDN connections and direct non-switched ethernet links.
    </p><p>
        Most often, cable modems and DSL devices do not fall  into this category. The same holds for when connected to a switch and trying to send data to a congested segment also connected to the switch. In this case, the effective queue does not reside within Linux and is therefore not available for scheduling. Embed SFQ in a classful qdisc to make sure it owns the queue.
    </p><p>
        It is possible to use external classifiers with sfq, for example to hash traffic based only on source/destination ip addresses
    </p><pre class="programlisting">
$ tc filter add ... flow hash keys src,dst perturb 30 divisor 1024
    </pre><p>
       Note that the given divisor should match the one used by sfq.  If  you have  changed  the sfq default of 1024, use the same value for the flow hash filter, too.
    </p><div class="example"><a name="ex-qs-sfq-bos"></a><p class="title"><b>Example 7. SFQ with optional RED mode</b></p><div class="example-contents"><pre class="programlisting">
<code class="prompt">[root@leander]# </code><strong class="userinput"><code>tc qdisc add dev eth0 parent 1:1 handle 10: sfq limit 3000 flows 512 divisor 16384 redflowlimit 100000 min 8000 max 60000 probability 0.20 ecn headdrop</code></strong>
        </pre></div></div><br class="example-break"><div class="example"><a name="ex-qs-sfq"></a><p class="title"><b>Example 8. Creating an SFQ</b></p><div class="example-contents"><pre class="programlisting">
<code class="prompt">[root@leander]# </code><strong class="userinput"><code>cat sfq.tcc</code></strong>
<code class="computeroutput">/*
 * make an SFQ on eth0 with a 10 second perturbation
 *
 */

dev eth0 {
    egress {
        sfq( perturb 10s );
    }
}</code>
<code class="prompt">[root@leander]# </code><strong class="userinput"><code>tcc &lt; sfq.tcc</code></strong>
<code class="computeroutput"># ================================ Device eth0 ================================

tc qdisc add dev eth0 handle 1:0 root dsmark indices 1 default_index 0
tc qdisc add dev eth0 handle 2:0 parent 1:0 sfq perturb 10</code>
      </pre></div></div><br class="example-break"><p>
      Unfortunately, some clever software (<span class="foreignphrase"><em class="foreignphrase">e.g.</em></span> Kazaa and eMule among others)
      obliterate the benefit of this attempt at fair queuing by opening as
      many TCP sessions (<a class="link" href="#o-flows" title="2.6. Flows">flows</a>) as can be sustained.  In many
      networks, with well-behaved users, SFQ can adequately distribute
      the network resources to the contending flows, but other measures may be
      called for when obnoxious applications have invaded the network.
    </p><p>
      See also
      <a class="xref" href="#qs-esfq" title="6.4. ESFQ, Extended Stochastic Fair Queuing">Section 6.4, &#8220;ESFQ, Extended Stochastic Fair Queuing&#8221;</a> for an SFQ qdisc with more exposed
      parameters for the user to manipulate.
    </p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="qs-esfq"></a>6.4. ESFQ, Extended Stochastic Fair Queuing</h3></div></div></div><p>
      Conceptually, this qdisc is no different than SFQ although it
      allows the user to control more parameters than its simpler cousin.
      This qdisc was conceived to overcome the shortcoming of SFQ
      identified above.  By allowing the user to control which hashing
      algorithm is used for distributing access to network bandwidth, it
      is possible for the user to reach a fairer real distribution of
      bandwidth.
    </p><div class="example"><a name="ex-qs-esfq-usage"></a><p class="title"><b>Example 9. ESFQ usage</b></p><div class="example-contents"><pre class="programlisting">
Usage: ... esfq [ perturb SECS ] [ quantum BYTES ] [ depth FLOWS ]
        [ divisor HASHBITS ] [ limit PKTS ] [ hash HASHTYPE]

Where:
HASHTYPE := { classic | src | dst }
      </pre></div></div><br class="example-break"><p>
      FIXME; need practical experience and/or attestation here.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="qs-red"></a>6.5. RED,Random Early Drop</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qs-red-descriptiom"></a>6.5.1. Description</h4></div></div></div><p>
        Random Early  Detection  is  a classless qdisc which manages its queue size smartly. Regular queues simply drop packets from the  tail  when they  are  full,  which may not be the optimal behaviour. RED also performs tail drop, but does so in a more gradual way.

    </p><p>
        Once the queue hits a certain average length, packets enqueued  have  a configurable  chance  of  being  marked  (which may mean dropped). This chance increases linearly up to a point called the max  average  queue length, although the queue might get bigger.
    </p><p>
        This has a host of benefits over simple taildrop, while not being processor intensive. It prevents synchronous retransmits after a burst in traffic, which cause further retransmits, etc. The goal is to have a small queue size, which is good for interactivity while not disturbing TCP/IP traffic with too many sudden drops after  a burst of traffic.
    </p><p>
        Depending on if  ECN  is configured, marking either means dropping or purely marking a packet as overlimit.
    </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qs-red-algorithm"></a>6.5.2. Algorithm</h4></div></div></div><p>
The average queue size is used for determining the marking probability. This is calculated using an Exponential Weighted Moving Average, which can be more or less sensitive to bursts. When the average queue size is below min bytes, no packet will ever be marked.  When  it  exceeds min, the probability of doing so climbs linearly up to probability, until the average queue size hits  max  bytes. Because probability is normally not set to 100%, the queue size might conceivably rise above max bytes, so the limit parameter is provided to set a hard maximum for the size of the queue.
               </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qs-red-synopsis"></a>6.5.3. Synopsis</h4></div></div></div><pre class="programlisting">
$ tc  qdisc ... red limit bytes [min bytes] [max bytes] avpkt bytes [burst packets] [ecn] [harddrop] [bandwidth rate]  [probability chance] [adaptive]
                </pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qs-red-parameters"></a>6.5.4. Parameters</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                            <span class="emphasis"><em>min:</em></span>  average  queue  size  at  which  marking  becomes a possibility. Defaults to max/3.
                        </p></li><li class="listitem"><p>
                            <span class="emphasis"><em>max:</em></span>  at this average queue size, the marking probability is  maximal. Should be at least twice min to prevent synchronous retransmits, higher for low min.  Default to limit/4.
                        </p></li><li class="listitem"><p>
                            <span class="emphasis"><em>probability:</em></span> maximum probability for marking, specified as a  floating  point number  from 0.0 to 1.0. Suggested values are 0.01 or 0.02 (1 or 2%, respectively). Default is 0.02.
                        </p></li><li class="listitem"><p>
                            <span class="emphasis"><em>limit:</em></span>  hard limit on the real (not average) queue size in  bytes.  Further  packets  are dropped. Should be set higher than max+burst. It is advised to set this a few times higher than max.
                        </p></li><li class="listitem"><p>
                            <span class="emphasis"><em>burst:</em></span> used for determining how fast the average queue size  is  influenced by the real queue size. Larger values make the calculation more sluggish, allowing longer bursts of traffic before  marking starts.  Real life experiments support the following guideline (min+min+max)/(3*avpkt).
                        </p></li><li class="listitem"><p>
                            <span class="emphasis"><em>Avpkt:</em></span>  specified in bytes. Used with burst to determine the time constant for average queue size calculations. 1000 is a good value.
                        </p></li><li class="listitem"><p>
                            <span class="emphasis"><em>bandwidth:</em></span> this  rate  is used for calculating the average queue size after some idle time. Should be set to the bandwidth  of  your  interface.  Does  not  mean  that  RED  will shape for you! Optional. Default is 10Mbit.
                        </p></li><li class="listitem"><p>
                            <span class="emphasis"><em>ecn:</em></span> as mentioned before, RED can either 'mark' or  'drop'.  Explicit Congestion  Notification  allows RED to notify remote hosts that their rate exceeds the amount of  bandwidth  available.  Non-ECN capable  hosts  can  only  be notified by dropping a packet.  If this parameter is specified, packets which indicate  that  their hosts  honor ECN will only be marked and not dropped, unless the queue size hits limit bytes. Recommended.
                        </p></li><li class="listitem"><p>
                            <span class="emphasis"><em>harddrop:</em></span> If average flow queue size is above max  bytes,  this  parameterv forces a drop instead of ecn marking.
                        </p></li><li class="listitem"><p>
                            <span class="emphasis"><em>adaptive:</em></span> (added  in  linux-3.3) Sets RED in adaptive mode as described in http://icir.org/floyd/papers/adaptiveRed.pdf. Goal of Adaptive RED is to make 'probability' dynamic value between 1% and 50% to reach the target average queue: (max - min) / 2.
                        </p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qs-red-example"></a>6.5.5. Example</h4></div></div></div><pre class="programlisting">
# tc qdisc add dev eth0 parent 1:1 handle 10: red limit 400000 min 30000 max 90000 avpkt 1000 burst 55 ecn adaptive bandwidth 10Mbit
            </pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="qs-gred"></a>6.6. GRED, Generic Random Early Drop</h3></div></div></div><p>
      Generalized RED is used in DiffServ implementation and it has virtual queue (VQ) within physical queue. Currently, the number of virtual queues is limited to 16.
    </p><p>
        GRED is configured in two steps. First the generic parameters are configured to select the number of virtual queues DPs and whether to turn on the RIO-like buffer sharing scheme. Also at this point, a default virtual queue is selected.
    </p><p>
        The second step is used to set parameters for individual virtual queues.
    </p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qs-gred-synopsis"></a>6.6.1. Synopsis</h4></div></div></div><pre class="programlisting">
... gred DP drop-probability limit BYTES min BYTES max BYTES avpkt BYTES burst PACKETS probability PROBABILITY bandwidth KBPS [prio value]

OR

... gred setup DPs "num of DPs" default "default DP" [grio]
        </pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qs-gred-parameter"></a>6.6.2. Parameter</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                    <span class="emphasis"><em>setup:</em></span> identifies that this is a generic setup for GRED;
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>DPs:</em></span> is the number of virtual queues;
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>default:</em></span> specifies default virtual queue;
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>grio:</em></span> turns on the RIO-like buffering scheme;
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>limit:</em></span> defines the virtual queue &#8220;physical&#8221; limit in bytes;
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>min:</em></span> defines the minimum threshold value in bytes;
                </p></li><li class="listitem"><p>
                     <span class="emphasis"><em>max:</em></span> defines the maximum threshold value in bytes;
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>avpkt:</em></span> is the average packet size in bytes;
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>bandwidth:</em></span> is the wire-speed of the interface;
                </p></li><li class="listitem"><p>
                     <span class="emphasis"><em>burst:</em></span> is the number of average-sized packets allowed to burst;
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>probability:</em></span> defines the drop probability in the range (0&#8230;);
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>DP:</em></span> identifies the virtual queue assigned to these parameters;
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>prio:</em></span> identifies the virtual queue priority if grio was set in general parameters;
                </p></li></ul></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="qs-tbf"></a>6.7. TBF, Token Bucket Filter</h3></div></div></div><p>
      This qdisc is built on <a class="link" href="#o-tokens" title="2.7. Tokens and buckets">tokens</a> and <a class="link" href="#o-buckets">buckets</a>.  It
      simply shapes traffic transmitted on an interface.  To limit the speed
      at which packets will be dequeued from a particular interface, the
      TBF qdisc is the perfect solution.  It simply slows down
      transmitted traffic to the specified rate.
    </p><p>
      Packets are only transmitted if there are sufficient tokens available.
      Otherwise, packets are deferred.  Delaying packets in this fashion will
      introduce an artificial latency into the packet's round trip time.
    </p><div class="mediaobject"><a name="img-qs-tbf"></a><object type="image/svg+xml" data="images/tbf-qdisc.svg"></object><div class="caption"><p><span class="command"><strong>Figure 10:</strong></span> <span class="emphasis"><em>Token Bucket Filter (TBF)</em></span></p></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qs-tbf-algorithm"></a>6.7.1. Algorithm</h4></div></div></div><p>
            As the name implies, traffic is filtered based on the expenditure of tokens (non-work-conserving). Tokens roughly correspond to bytes, with the additional  constraint  that  each packet consumes some tokens, no matter how small it is. This reflects the fact that even a zero-sized packet occupies the link for some time. On creation,  the  TBF  is stocked with tokens which correspond to the amount of traffic that can be burst in  one  go.  Tokens  arrive at a steady rate, until the bucket is full. If no tokens are available, packets are queued, up to a configured limit. The TBF now calculates the token deficit, and throttles until the first packet in the queue can be sent. If  it  is  not  acceptable  to  burst  out packets at maximum speed, a peakrate can be configured to limit the speed at which the bucket  empties.  This  peakrate is implemented as a second TBF with a very small bucket, so that it doesn't burst.
        </p><p>
        </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qs-tbf-parameters"></a>6.7.2. Parameters</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                    <span class="emphasis"><em>limit or latency:</em></span> limit  is  the  number  of  bytes that can be queued waiting for tokens to become available. You can also specify this the other way around by setting the latency parameter, which specifies the maximum amount of time a packet can sit in the TBF. The  latter calculation  takes into account the size of the bucket, the rate and possibly the peakrate (if set).  These  two  parameters are mutually exclusive.
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>Burst:</em></span> also known as buffer or maxburst.  Size of the bucket, in bytes. This is the maximum amount of bytes that tokens can be available for instantaneously.  In general, larger shaping rates require a larger buffer. For 10mbit/s on Intel, you need at least  10kbyte buffer if you want to reach your configured rate. If your buffer is too small, packets may be dropped because more tokens arrive per timer tick than fit in your bucket. The minimum buffer size can be calculated by dividing the rate by HZ.
                </p><p>
                    Token usage calculations are performed using a table which by default has a resolution of 8 packets. This resolution can be changed by specifying the cell size with the burst. For example, to specify a 6000 byte buffer with a 16 byte cell  size,  set  a burst of 6000/16. You will probably never have to set this. Must be an integral power of 2.
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>Mpu:</em></span> a zero-sized packet does not use zero bandwidth.  For ethernet, no packet uses  less  than  64  bytes. The Minimum Packet Unit determines the minimal token usage (specified in bytes) for a packet. Defaults to zero.
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>Rate:</em></span> the speed knob. Furthermore, if a peakrate is desired, the  following  parameters  are available:
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>peakrate:</em></span> maximum  depletion  rate  of  the bucket.  The peakrate does not need to be set, it is only  necessary  if  perfect  millisecond timescale shaping is required.
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>mtu/minburst:</em></span> specifies the size of the peakrate bucket. For perfect accuracy, should be set to the MTU of the interface.  If a peakrate is needed,  but  some  burstiness  is  acceptable, this size can be raised. A 3000 byte minburst allows around 3mbit/s of peakrate, given 1000 byte packets. Like the regular burstsize you can also specify a cell size.
                </p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qs-tbf-example"></a>6.7.3. Example</h4></div></div></div><div class="example"><a name="ex-qs-tbf"></a><p class="title"><b>Example 10. Creating a 256kbit/s TBF</b></p><div class="example-contents"><pre class="programlisting">
<code class="prompt">[root@leander]# </code><strong class="userinput"><code>cat tbf.tcc</code></strong>
<code class="computeroutput">/*
 * make a 256kbit/s TBF on eth0
 *
 */

dev eth0 {
    egress {
        tbf( rate 256 kbps, burst 20 kB, limit 20 kB, mtu 1514 B );
    }
}</code>
<code class="prompt">[root@leander]# </code><strong class="userinput"><code>tcc &lt; tbf.tcc</code></strong>
<code class="computeroutput"># ================================ Device eth0 ================================

tc qdisc add dev eth0 handle 1:0 root dsmark indices 1 default_index 0
tc qdisc add dev eth0 handle 2:0 parent 1:0 tbf burst 20480 limit 20480 mtu 1514 rate 32000bps</code>
      </pre></div></div><br class="example-break"><p>
    </p></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="classful-qdiscs"></a>7. Classful Queuing Disciplines (<a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a>s)</h2></div></div></div><p>
    The flexibility and control of Linux traffic control can be unleashed
    through the agency of the classful qdiscs.  Remember that the classful
    queuing disciplines can have filters attached to them, allowing packets to
    be directed to particular classes and subqueues.
  </p><p>
    There are several common terms to describe classes directly attached to
    the <code class="constant">root</code> qdisc and terminal classes.  Classess attached to the
    <code class="constant">root</code> qdisc are known as root classes, and more generically inner
    classes.  Any terminal class in a particular queuing discipline is known
    as a leaf class by analogy to the tree structure of the classes.  Besides
    the use of figurative language depicting the structure as a tree, the
    language of family relationships is also quite common.
  </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="qc-htb"></a>7.1. HTB, Hierarchical Token Bucket</h3></div></div></div><p>
        HTB is meant as a more understandable and intuitive replacement for the CBQ (<a class="link" href="#qc-cbq" title="7.4. CBQ, Class Based Queuing (CBQ)">see chapter 7.4</a>) qdisc in Linux. Both CBQ and HTB help you to control the use of the outbound bandwidth on a given link. Both allow you to use one physical link to simulate several slower links and to send  different  kinds  oftraffic  on different simulated links. In both cases, you have to specify how to divide the physical link into simulated  links  and  how  to decide which simulated link to use for a given packet to be sent.
    </p><p>
      HTB uses the concepts of tokens and buckets
      along with the class-based system and <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a>s to allow for
      complex and granular control over traffic.  With a complex
      <a class="link" href="#qc-htb-borrowing" title="7.1.3. Borrowing">borrowing model</a>, HTB can perform a variety of sophisticated
      traffic control techniques.  One of the easiest ways to use HTB
      immediately is that of <a class="link" href="#qc-htb-borrowing" title="7.1.3. Borrowing">shaping</a>.
    </p><p>
      By understanding <a class="link" href="#o-tokens" title="2.7. Tokens and buckets">tokens</a> and <a class="link" href="#o-buckets">buckets</a> or by grasping
      the function of <a class="link" href="#qs-tbf" title="6.7. TBF, Token Bucket Filter">TBF</a>, HTB should be merely a logical
      step.  This queuing discipline allows the user to define the
      characteristics of the tokens and bucket used and allows the user to
      nest these buckets in an arbitrary fashion.  When coupled with a
      <a class="link" href="#e-classifying" title="3.3. Classifying">classifying</a> scheme, traffic can be controlled in a very
      granular fashion.
    </p><p>
    </p><p>
      Below is example output of the syntax for HTB on the command line
      with the <a class="link" href="#s-iproute2-tc"><span class="command"><strong>tc</strong></span></a> tool.  Although the syntax for <a class="link" href="#s-tcng" title="5.3. tcng, Traffic Control Next Generation"><span class="command"><strong>tcng</strong></span></a> is a
      language of its own, the rules for HTB are the same.
    </p><div class="example"><a name="ex-qc-htb-usage"></a><p class="title"><b>Example 11. <span class="command">tc</span> usage for HTB</b></p><div class="example-contents"><pre class="programlisting">
Usage: ... qdisc add ... htb [default N] [r2q N]
 default  minor id of class to which unclassified packets are sent {0}
 r2q      DRR quantums are computed as rate in Bps/r2q {10}
 debug    string of 16 numbers each 0-3 {0}

... class add ... htb rate R1 burst B1 [prio P] [slot S] [pslot PS]
                      [ceil R2] [cburst B2] [mtu MTU] [quantum Q]
 rate     rate allocated to this class (class can still borrow)
 burst    max bytes burst which can be accumulated during idle period {computed}
 ceil     definite upper class rate (no borrows) {rate}
 cburst   burst but for ceil {computed}
 mtu      max packet size we create rate map for {1600}
 prio     priority of leaf; lower are served first {0}
 quantum  how much bytes to serve from leaf at once {use r2q}

TC HTB version 3.3
      </pre></div></div><br class="example-break"><p>
    </p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-htb-software"></a>7.1.1. Software requirements</h4></div></div></div><p>
        Unlike almost all of the other software discussed, HTB is a
        newer queuing discipline and your distribution may not have all of the
        tools and capability you need to use HTB.  The kernel must
        support HTB; kernel version 2.4.20 and later support it in the
        stock distribution, although earlier kernel versions require patching.
        To enable userland support for HTB, see <a class="ulink" href="http://luxik.cdi.cz/~devik/qos/htb/" target="_top">HTB</a> for an
        <span class="command"><strong>iproute2</strong></span> patch to <span class="command"><strong>tc</strong></span>.
      </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-htb-shaping"></a>7.1.2. Shaping</h4></div></div></div><p>
        One of the most common applications of HTB involves shaping
        transmitted traffic to a specific rate.
      </p><p>
        All shaping occurs in leaf classes.  No shaping occurs in inner or
        root classes as they only exist to suggest how the
        <a class="link" href="#qc-htb-borrowing" title="7.1.3. Borrowing">borrowing model</a> should distribute available tokens.
      </p><p>
      </p><p>
      </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-htb-borrowing"></a>7.1.3. Borrowing</h4></div></div></div><p>
        A fundamental part of the HTB qdisc is the borrowing mechanism.
        Children classes borrow tokens from their parents once they have
        exceeded <a class="link" href="#vl-qc-htb-params-rate"><em class="parameter"><code>rate</code></em></a>.  A child class will continue to
        attempt to borrow until it reaches <a class="link" href="#vl-qc-htb-params-ceil"><em class="parameter"><code>ceil</code></em></a>, at which
        point it will begin to queue packets for transmission until more
        tokens/ctokens are available.  As there are only two primary types of
        classes which can be created with HTB the following table and
        diagram identify the various possible states and the behaviour of the
        borrowing mechanisms.
      </p><p>
      </p><div class="table"><a name="tb-qc-htb-borrowing"></a><p class="title"><b>Table 2. HTB class states and potential actions taken</b></p><div class="table-contents"><table class="table" summary="HTB class states and potential actions taken" border="1"><colgroup><col><col><col><col></colgroup><thead><tr><th align="left">type of class</th><th align="left">class state</th><th align="left">HTB internal state</th><th align="left">action taken</th></tr></thead><tbody><tr><td align="left">leaf</td><td align="left">&lt; <em class="parameter"><code>rate</code></em></td><td align="left"><em class="parameter"><code>HTB_CAN_SEND</code></em></td><td align="left">
                Leaf class will dequeue queued bytes up
                to available tokens (no more than burst packets)
              </td></tr><tr><td align="left">leaf</td><td align="left">&gt; <em class="parameter"><code>rate</code></em>, &lt; <em class="parameter"><code>ceil</code></em></td><td align="left"><em class="parameter"><code>HTB_MAY_BORROW</code></em></td><td align="left">
                Leaf class will attempt to borrow tokens/ctokens from
                parent class.  If tokens are available, they will be lent in
                <em class="parameter"><code>quantum</code></em> increments and the leaf class will dequeue up
                to <em class="parameter"><code>cburst</code></em> bytes
              </td></tr><tr><td align="left">leaf</td><td align="left">&gt; <em class="parameter"><code>ceil</code></em></td><td align="left"><em class="parameter"><code>HTB_CANT_SEND</code></em></td><td align="left">
                No packets will be dequeued.  This will cause packet
                delay and will increase latency to meet the desired
                rate.
              </td></tr><tr><td align="left">inner, root</td><td align="left">&lt; <em class="parameter"><code>rate</code></em></td><td align="left"><em class="parameter"><code>HTB_CAN_SEND</code></em></td><td align="left">
                Inner class will lend tokens to children.
              </td></tr><tr><td align="left">inner, root</td><td align="left">&gt; <em class="parameter"><code>rate</code></em>, &lt; <em class="parameter"><code>ceil</code></em></td><td align="left"><em class="parameter"><code>HTB_MAY_BORROW</code></em></td><td align="left">
                Inner class will attempt to borrow tokens/ctokens from
                parent class, lending them to competing children in
                <em class="parameter"><code>quantum</code></em> increments per request.
              </td></tr><tr><td align="left">inner, root</td><td align="left">&gt; <em class="parameter"><code>ceil</code></em></td><td align="left"><em class="parameter"><code>HTB_CANT_SEND</code></em></td><td align="left">
                Inner class will not attempt to borrow from its parent
                and will not lend tokens/ctokens to children classes.
              </td></tr></tbody></table></div></div><br class="table-break"><p>
        This diagram identifies the flow of borrowed tokens and the manner in
        which tokens are charged to parent classes.  In order for the
        borrowing model to work, each class must have an accurate count of the
        number of tokens used by itself and all of its children.  For this
        reason, any token used in a child or leaf class is charged to each
        parent class until the root class is reached.
      </p><p>
        Any child class which wishes to borrow a token will request a token
        from its parent class, which if it is also over its <em class="parameter"><code>rate</code></em> will
        request to borrow from its parent class until either a token is
        located or the root class is reached.  So the borrowing of tokens
        flows toward the leaf classes and the charging of the usage of tokens
        flows toward the root class.
      </p><div class="mediaobject"><a name="img-qc-htb-borrow"></a><object type="image/svg+xml" data="images/htb-borrow.svg"></object><div class="caption"><p><span class="command"><strong>Figure 11:</strong></span> <span class="emphasis"><em>Hierarchical Token Bucket (HTB)</em></span></p></div></div><p>
        Note in this diagram that there are several HTB root classes.
        Each of these root classes can simulate a virtual circuit.
      </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-htb-params"></a>7.1.4. HTB class parameters</h4></div></div></div><p>
      </p><div class="variablelist"><a name="vl-qc-htb-params"></a><dl class="variablelist"><dt><a name="vl-qc-htb-params-default"></a><span class="term"><em class="parameter"><code>default</code></em></span></dt><dd><p>
              An optional parameter with every HTB <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a> object,
              the default <em class="parameter"><code>default</code></em> is 0, which cause any unclassified
              traffic to be dequeued at hardware speed, completely bypassing
              any of the classes attached to the <code class="constant">root</code> qdisc.
            </p></dd><dt><a name="vl-qc-htb-params-rate"></a><span class="term"><em class="parameter"><code>rate</code></em></span></dt><dd><p>
              Used to set the minimum desired speed to which to limit
              transmitted traffic.  This can be considered the equivalent of a
              committed information rate (<acronym class="acronym">CIR</acronym>), or the
              guaranteed bandwidth for a given leaf class.
            </p></dd><dt><a name="vl-qc-htb-params-ceil"></a><span class="term"><em class="parameter"><code>ceil</code></em></span></dt><dd><p>
              Used to set the maximum desired speed to which to limit the
              transmitted traffic.  The borrowing model should illustrate how
              this parameter is used.  This can be considered the equivalent
              of <span class="quote">&#8220;<span class="quote">burstable bandwidth</span>&#8221;</span>.
            </p></dd><dt><a name="vl-qc-htb-params-burst"></a><span class="term"><em class="parameter"><code>burst</code></em></span></dt><dd><p>
              This is the size of the <a class="link" href="#vl-qc-htb-params-rate"><em class="parameter"><code>rate</code></em></a> bucket (see
              <a class="xref" href="#o-buckets">Tokens and buckets</a>).  HTB will dequeue
              <em class="parameter"><code>burst</code></em> bytes before awaiting the arrival of more
              tokens.
            </p></dd><dt><a name="vl-qc-htb-params-cburst"></a><span class="term"><em class="parameter"><code>cburst</code></em></span></dt><dd><p>
              This is the size of the <a class="link" href="#vl-qc-htb-params-ceil"><em class="parameter"><code>ceil</code></em></a> bucket (see
              <a class="xref" href="#o-buckets">Tokens and buckets</a>).  HTB will dequeue
              <em class="parameter"><code>cburst</code></em> bytes before awaiting the arrival of more
              ctokens.
            </p></dd><dt><a name="vl-qc-htb-params-quantum"></a><span class="term"><em class="parameter"><code>quantum</code></em></span></dt><dd><p>
              This is a key parameter used by HTB to control borrowing.
              Normally, the correct <em class="parameter"><code>quantum</code></em> is calculated by
              HTB, not specified by the user.  Tweaking this parameter
              can have tremendous effects on borrowing and shaping under
              contention, because it is used both to split traffic between
              children classes over <a class="link" href="#vl-qc-htb-params-rate"><em class="parameter"><code>rate</code></em></a> (but below
              <a class="link" href="#vl-qc-htb-params-ceil"><em class="parameter"><code>ceil</code></em></a>) and to transmit packets from these same
              classes.
            </p></dd><dt><a name="vl-qc-htb-params-r2q"></a><span class="term"><em class="parameter"><code>r2q</code></em></span></dt><dd><p>
              Also, usually calculated for the user, <em class="parameter"><code>r2q</code></em> is a hint to
              HTB to help determine the optimal <a class="link" href="#vl-qc-htb-params-quantum"><em class="parameter"><code>quantum</code></em></a>
              for a particular class.
            </p></dd><dt><a name="vl-qc-htb-params-mtu"></a><span class="term"><em class="parameter"><code>mtu</code></em></span></dt><dd><p>
            </p></dd><dt><a name="vl-qc-htb-params-prio"></a><span class="term"><em class="parameter"><code>prio</code></em></span></dt><dd><p>
                In the round-robin process, classes  with  the  lowest  priority field are tried for packets first. Mandatory field.
            </p></dd><dt><a name="vl-qc-htb-params-parent"></a><span class="term"><em class="parameter"><code>prio</code></em></span></dt><dd><p>
                    Place of this class within the hierarchy. If  attached directly to a qdisc and not to another class, minor can be omitted. Mandatory field.
                </p></dd><dt><a name="vl-qc-htb-params-classid"></a><span class="term"><em class="parameter"><code>prio</code></em></span></dt><dd><p>
                    Like qdiscs, classes can be named.  The major number must be equal to the major number of the qdisc to which it belongs. Optional, but needed if this class is going to have children.
                </p></dd></dl></div><p>
      </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-htb-params-root"></a>7.1.5. HTB root parameters</h4></div></div></div><p>
            The root of a HTB qdisc class tree has the following parameters:
        </p><div class="variablelist"><a name="vl-qc-htb-params-root"></a><dl class="variablelist"><dt><a name="vl-qc-htb-params-root-parent"></a><span class="term"><code class="constant">parent major:minor | root</code></span></dt><dd><p>
                        This mandatory parameter determines the place of the HTB instance, either at the root of an interface or within an existing class.
                    </p></dd><dt><a name="vl-qc-htb-params-root-handle"></a><span class="term"><code class="constant">handle major:</code></span></dt><dd><p>
                       Like all other qdiscs, the HTB can be assigned a handle. Should consist only of a major number, followed by a colon. Optional, but very useful if classes will be generated within this qdisc.
                    </p></dd><dt><a name="vl-qc-htb-params-root-default"></a><span class="term"><code class="constant">default minor-id</code></span></dt><dd><p>
                        Unclassified traffic gets sent to the class with this minor-id.
                    </p></dd></dl></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-htb-rules"></a>7.1.6. Rules</h4></div></div></div><p>
        Below are some general guidelines to using HTB culled from
        <a class="ulink" href="http://www.docum.org/docum.org/" target="_top">http://www.docum.org/docum.org/</a> and the <a class="ulink" href="http://www.spinics.net/lists/lartc/" target="_top">(new) LARTC 
         mailing list</a> (see also the
        <a class="ulink" href="http://mailman.ds9a.nl/mailman/listinfo/lartc/" target="_top">(old) LARTC
         mailing list archive</a>).  These rules are
        simply a recommendation for beginners to maximize the benefit of
        HTB until gaining a better understanding of the practical
        application of HTB.
      </p><p>
      </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
            Shaping with HTB occurs only in leaf classes.  See also
            <a class="xref" href="#qc-htb-shaping" title="7.1.2. Shaping">Section 7.1.2, &#8220;Shaping&#8221;</a>.
          </p></li><li class="listitem"><p>
            Because HTB does not shape in any class except the leaf
            class, the sum of the <em class="parameter"><code>rate</code></em>s of leaf classes should not
            exceed the <em class="parameter"><code>ceil</code></em> of a parent class.  Ideally, the sum of
            the <em class="parameter"><code>rate</code></em>s of the children classes would match the
            <em class="parameter"><code>rate</code></em> of the parent class, allowing the parent class to
            distribute leftover bandwidth (<em class="parameter"><code>ceil</code></em> - <em class="parameter"><code>rate</code></em>) among
            the children classes.
          </p><p>
            This key concept in employing HTB bears repeating.  Only
            leaf classes actually shape packets; packets are only delayed in
            these leaf classes.  The inner classes (all the way up to the root
            class) exist to define how borrowing/lending occurs (see also
            <a class="xref" href="#qc-htb-borrowing" title="7.1.3. Borrowing">Section 7.1.3, &#8220;Borrowing&#8221;</a>).
          </p></li><li class="listitem"><p>
            The <em class="parameter"><code>quantum</code></em> is only  only used when a class is over
            <em class="parameter"><code>rate</code></em> but below <em class="parameter"><code>ceil</code></em>.
          </p></li><li class="listitem"><p>
            The <em class="parameter"><code>quantum</code></em> should be set at MTU or higher.  HTB
            will dequeue a single packet at least per service opportunity even
            if <em class="parameter"><code>quantum</code></em> is too small.  In such a case, it will not be
            able to calculate accurately the real bandwidth consumed
            <a href="#ftn.idm1853" class="footnote" name="idm1853"><sup class="footnote">[9]</sup></a>.
          </p></li><li class="listitem"><p>
            Parent classes lend tokens to children in increments of
            <em class="parameter"><code>quantum</code></em>, so for maximum granularity and most
            instantaneously evenly distributed bandwidth, <em class="parameter"><code>quantum</code></em>
            should be as low as possible while still no less than MTU.
          </p></li><li class="listitem"><p>
            A distinction between tokens and ctokens is only meaningful in a
            leaf class, because non-leaf classes only lend tokens to child
            classes.
          </p></li><li class="listitem"><p>
            HTB borrowing could more accurately be described as
            <span class="quote">&#8220;<span class="quote">using</span>&#8221;</span>.
          </p></li></ul></div><p>
      </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-htb-classification"></a>7.1.7. Classification</h4></div></div></div><p>
            Like see before, within the one HTB instance many classes may exist. Each of these classes contains another qdisc, by default <a class="link" href="#qs-fifo" title="6.1. FIFO, First-In First-Out (pfifo and bfifo)">tc-pfifo</a>.When enqueueing a packet, HTB starts at the root and uses various methods to determine which class should receive the data. In the absence of uncommon configuration options, the process is rather easy.  At each node we look for an instruction,  and  then  go  to  the class  the  instruction  refers  us  to. If the class found is a barren leaf-node (without children), we enqueue the packet there. If it is not yet a leaf node, we do the whole thing over again starting from that node.
        </p><p>
            The following actions are performed, in order at each node we visit, until one sends us to another node, or terminates the process.
        </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                    Consult filters attached to the class. If sent to a leafnode, we are done.  Otherwise, restart.
                </p></li><li class="listitem"><p>
                    If none of the above returned with an  instruction,  enqueue  at this node.
                </p></li></ul></div><p>
            This algorithm makes sure that a packet always ends up somewhere, even while you are busy building your configuration.
        </p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="qc-hfsc"></a>7.2. HFSC, Hierarchical Fair Service Curve</h3></div></div></div><p>
      The HFSC classful qdisc balances delay-sensitive traffic against
      throughput sensitive traffic.  In a congested or backlogged state, the
      HFSC queuing discipline interleaves the delay-sensitive traffic when
      required according service curve definitions.  Read about the Linux
      implementation in German, <a class="ulink" href="http://klaus.geekserver.net/hfsc/hfsc.html" target="_top">HFSC
           Scheduling mit Linux</a> or read a
      translation into English, <a class="ulink" href="http://linux-ip.net/tc/hfsc.en/" target="_top">HFSC Scheduling
          with Linux</a>.  The original
      research article, <a class="ulink" href="http://acm.org/sigcomm/sigcomm97/program.html#ab011" target="_top">A
         Hierarchical Fair Service Curve Algorithm For Link-Sharing, Real-Time
         and Priority Services</a>, also remains available.
    </p><p>
      This section will be completed at a later date.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="qc-prio"></a>7.3. PRIO, priority scheduler</h3></div></div></div><p>
      The PRIO classful qdisc works on a very simple precept.  When it
      is ready to dequeue a packet, the first class is checked for a packet.
      If there's a packet, it gets dequeued.  If there's no packet, then the
      next class is checked, until the queuing mechanism has no more classes
      to check. PRIO is a scheduler and never delays packets - it is a work-conserving  qdisc, though the qdiscs contained in the classes may not be
    </p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-prio-algorithm"></a>7.3.1. Algorithm</h4></div></div></div><p>
        On creation with <span class="command"><strong>tc qdisc add</strong></span>, a fixed number of
        bands is created.  Each band is a class, although is not possible to
        add classes with <span class="command"><strong>tc class add</strong></span>.  The number of bands
        to be created is fixed at the creation of the qdisc itself.
    </p><p>
        When dequeueing packets, band 0 is always checked first.  If it
        has no packet to dequeue, then PRIO will try band 1, and so
        onwards.  Maximum reliability packets should therefore go to band 0,
        minimum delay to band 1 and the rest to band 2.
    </p><p>
        As the PRIO qdisc itself will have minor number 0, band 0 is  actually major:1, band 1 is major:2, etc. For major, substitute the major number assigned to the qdisc on 'tc qdisc add' with the handle parameter.
    </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-prio-synopsis"></a>7.3.2. Synopsis</h4></div></div></div><pre class="programlisting">
$ tc qdisc ... dev dev ( parent classid | root) [ handle major: ] prio [bands bands ] [ priomap band band band...  ] [ estimator interval time&#8208;constant ]
        </pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-prio-classification"></a>7.3.3. Classification</h4></div></div></div><p>
            Three methods are available to determine the target band in which a packet will be enqueued.
        </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                <span class="emphasis"><em>From userspace</em></span>, a process with
                sufficient privileges can encode the destination class
                directly with SO_PRIORITY.
            </p></li><li class="listitem"><p>
                <span class="emphasis"><em>Programmatically</em></span>, a <span class="command"><strong>tc filter</strong></span>
                attached to the root qdisc can point any traffic directly to a
                class.
            </p></li><li class="listitem"><p>
                And, typically, <span class="emphasis"><em>with reference to the priomap</em></span>, a packet's
                priority, is derived from the Type of Service
                (<acronym class="acronym">ToS</acronym>) assigned to the packet.
            </p></li></ul></div><p>
            Only the priomap is specific to this qdisc.
        </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-prio-parameters"></a>7.3.4. Configurable parameters</h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">bands</span></dt><dd><p>
                  total number of distinct bands. If changed from the default of 3, priomap must be updated as well.
                </p></dd><dt><span class="term">priomap</span></dt><dd><p>
                  a tc filter attached to the  root  qdisc  can  point  traffic directly to a class
                </p></dd></dl></div><p>
          The <span class="emphasis"><em>priomap</em></span> specifies how this qdisc determines
          how a packet maps to a specific band.  Mapping occurs based on the
          value of the ToS octet of a packet.
        </p><pre class="screen">
   0     1     2     3     4     5     6     7
+-----+-----+-----+-----+-----+-----+-----+-----+
|   PRECEDENCE    |          ToS          | MBZ |   RFC 791
+-----+-----+-----+-----+-----+-----+-----+-----+

   0     1     2     3     4     5     6     7
+-----+-----+-----+-----+-----+-----+-----+-----+
|    DiffServ Code Point (DSCP)     |  (unused) |   RFC 2474
+-----+-----+-----+-----+-----+-----+-----+-----+
        </pre><p>
          The four ToS bits from the  (the 'ToS field') are defined slightly
          differently in RFC 791 and RFC 2474.  The later RFC supersedes the
          definitions of the former, but not all software, systems and
          terminology have caught up to that change.  So, often packet
          analysis programs will still refer to Type of Service (ToS) instead
          of DiffServ Code Point (DSCP).
        </p><div class="table"><a name="idm1928"></a><p class="title"><b>Table 3. <a class="ulink" href="https://tools.ietf.org/rfc/rfc791.txt" target="_top">RFC 791</a> interpretation of IP ToS header</b></p><div class="table-contents"><table class="table" summary="RFC 791 interpretation of IP ToS header" border="1"><colgroup><col><col><col></colgroup><thead><tr><th>Binary</th><th>Decimal</th><th>Meaning</th></tr></thead><tbody><tr><td>1000</td><td>8</td><td>Minimize delay (md)</td></tr><tr><td>0100</td><td>4</td><td>Maximize throughput (mt)</td></tr><tr><td>0010</td><td>2</td><td>Maximize reliability (mr)</td></tr><tr><td>0001</td><td>1</td><td>Minimize monetary cost (mmc)</td></tr><tr><td>0000</td><td>0</td><td>Normal Service</td></tr></tbody></table></div></div><br class="table-break"><p>
            As there is 1 bit to the right of these four bits, the actual
            value of the ToS field is double the value of the ToS bits.
            Running <span class="command"><strong>tcpdump -v -v</strong></span> shows you the value of
            the entire ToS field, not just the four bits. It is the value you
            see in the  first  column  of this table:
        </p><div class="table"><a name="idm1960"></a><p class="title"><b>Table 4. Mapping ToS value to priomap band</b></p><div class="table-contents"><table class="table" summary="Mapping ToS value to priomap band" border="1"><colgroup><col><col><col><col><col></colgroup><thead><tr><th>ToS Field</th><th>ToS Bits</th><th>Meaning</th><th>Linux Priority</th><th>Band</th></tr></thead><tbody><tr><td>0x0</td><td>0</td><td>Normal Service</td><td>0 Best Effort</td><td>1</td></tr><tr><td>0x2</td><td>1</td><td>Minimize Monetary Cost (mmc)</td><td>1 Filler</td><td>2</td></tr><tr><td>0x4</td><td>2</td><td>Maximize Reliability (mr)</td><td>0 Best Effort</td><td>1</td></tr><tr><td>0x6</td><td>3</td><td>mmc+mr</td><td>0 Best Effort</td><td>1</td></tr><tr><td>0x8</td><td>4</td><td>Maximize Throughput (mt)</td><td>2 Bulk</td><td>2</td></tr><tr><td>0xa</td><td>5</td><td>mmc+mt</td><td>2 Bulk</td><td>2</td></tr><tr><td>0xc</td><td>6</td><td>mr+mt</td><td>2 Bulk</td><td>2</td></tr><tr><td>0xe</td><td>7</td><td>mmc+mr+mt</td><td>2 Bulk</td><td>2</td></tr><tr><td>0x10</td><td>8</td><td>Minimize Delay (md)</td><td>6 Interactive</td><td>0</td></tr><tr><td>0x12</td><td>9</td><td>mmc+md</td><td>6 Interactive</td><td>0</td></tr><tr><td>0x14</td><td>10</td><td>mr+md</td><td>6 Interactive</td><td>0</td></tr><tr><td>0x16</td><td>11</td><td>mmc+mr+md</td><td>6 Interactive</td><td>0</td></tr><tr><td>0x18</td><td>12</td><td>mt+md</td><td>4 Int. Bulk</td><td>1</td></tr><tr><td>0x1a</td><td>13</td><td>mmc+mt+md</td><td>4 Int. Bulk</td><td>1</td></tr><tr><td>0x1c</td><td>14</td><td>mr+mt+md</td><td>4 Int. Bulk</td><td>1</td></tr><tr><td>0x1e</td><td>15</td><td>mmc+mr+mt+md</td><td>4 Int. Bulk</td><td>1</td></tr></tbody></table></div></div><br class="table-break"><p>
            The second column contains the value of the relevant four ToS bits, followed by their  translated  meaning. For example, 15 stands for a packet wanting Minimal Monetary Cost, Maximum Reliability, Maximum Throughput AND Minimum Delay.
        </p><p>
            The fourth column lists the way the Linux kernel interprets the ToS bits, by showing to which Priority they are mapped.
        </p><p>
            The last column shows the result of the default priomap. On the command line, the default priomap looks like this:
        </p><p>
            1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1
        </p><p>
            This means that priority 4, for example,  gets  mapped  to  band number 1.  The priomap also allows you to list higher priorities (&gt; 7) which do not correspond to ToS mappings, but which are set by other means.
        </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-prio-classes"></a>7.3.5. Classes</h4></div></div></div><p>
            PRIO classes cannot be configured further - they are automatically created when the PRIO qdisc is attached. Each class however can contain yet a further qdisc.
        </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-prio-bugs"></a>7.3.6. Bugs</h4></div></div></div><p>
            Large amounts of traffic in the lower bands can cause starvation  of higher  bands. Can be prevented by attaching a shaper to these bands to make sure they cannot dominate the link.
        </p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="qc-cbq"></a>7.4. CBQ, Class Based Queuing (<acronym class="acronym">CBQ</acronym>)</h3></div></div></div><p>
      Class Based Queuing (<acronym class="acronym">CBQ</acronym>) is the classic
      implementation (also called venerable) of a traffic control system.  CBQ
      is a classful qdisc that implements a rich link sharing hierarchy of
      classes.  It contains shaping elements as well as prioritizing
      capabilities.  Shaping is performed by calculating link idle time
      based on the timing of dequeue events and knowledge of the underlying
      link layer bandwidth.
    </p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-cbq-shaping"></a>7.4.1. Shaping Algorithm</h4></div></div></div><p>
            Shaping is done using link idle time calculations, and actions taken if these calculations deviate from set limits.
        </p><p>
            When shaping a 10mbit/s connection to 1mbit/s, the link will be idle 90% of the time. If it isn't, it needs to be throttled so that it is idle 90% of the time.
        </p><p>
            From the kernel's perspective, this is hard to measure, so CBQ instead computes idle time from the number of microseconds that elapse between requests from the device driver for more data.  Combined with the knowledge of packet sizes, this is used to approximate how full or empty the link is.
        </p><p>
            This is rather circumspect and doesn't always arrive at proper results. The physical link bandwidth may be ill defined in case of not-quite-real network devices like PPP over Ethernet or PPTP over TCP/IP. The effective bandwidth in that case is probably determined by the efficiency of pipes to userspace - which not defined.
        </p><p>
            During operations, the effective idletime is measured using an exponential weighted moving average (EWMA).  This calculation of activity against idleness values recent packets exponentially more than predecessors.  The EWMA is an effective calculation to deal with the problem that a system is either active or inactive.  For example, the Unix system load average is calculated in the same way.
        </p><p>
            The calculated idle time is subtracted from the EWMA measured one, the resulting number is called 'avgidle'. A perfectly loaded link has an avgidle of zero: packets arrive exactly at the calculated interval.
        </p><p>
            An overloaded link has a negative avgidle and if it gets too negative, CBQ throttles and is then 'overlimit'. Conversely, an idle link might amass a huge avgidle, which would then allow infinite bandwidths after a few  hours  of silence.  To  prevent this, avgidle is capped at maxidle.
        </p><p>
            If  overlimit, in theory, the CBQ could throttle itself for exactly the amount of time that was calculated to pass between packets, and then pass one packet, and throttle again. Due to timer resolution constraints, this may not be feasible, see the minburst parameter below.

        </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-cbq-classification"></a>7.4.2. Classification</h4></div></div></div><p>
            Under one installed CBQ qdisc many classes may exist.  Each of these classes contains another qdisc, by default <a class="link" href="#qs-fifo" title="6.1. FIFO, First-In First-Out (pfifo and bfifo)">tc-pfifo</a>.
        </p><p>
            When enqueueing a packet, CBQ starts at the root and uses various methods to determine which class should receive the data. If a verdict is reached, this process is repeated for the recipient class which might have further means of classifying traffic to its children, if any. CBQ has the following methods available to classify  a  packet  to  any child classes.
        </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                    skb&gt;priority  class  encoding.  Can be set from userspace by an application with the SO_PRIORITY setsockopt.  The skb-&gt;priority class encoding  only  applies  if  the  skb-&gt;priority  holds  a major:minor handle of an existing class within  this qdisc.
                </p></li><li class="listitem"><p>
                   <span class="emphasis"><em> tc filters attached to the class. </em></span>
                </p></li><li class="listitem"><p>
                    <span class="emphasis"><em>The defmap of a class</em></span>, as set with the split and  defmap parameters.  The defmap  may contain instructions for each possible Linux packet priority.
                </p></li></ul></div><p>
            Each class also has a level.  Leaf nodes, attached to the bottom of theclass hierarchy, have a level of 0.
        </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-cbq-classification-algorithm"></a>7.4.3. Classification Algorithm</h4></div></div></div><p>
            Classification is a loop, which terminates when a leaf class is found. At any point the loop may jump to the fallback algorithm. The loop consists of the following steps:
        </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                    If the packet is generated locally and has a valid  classid encoded within its skb-&gt;priority, choose it and terminate.
                </p></li><li class="listitem"><p>
                    Consult the tc filters, if any, attached to this child. If these return a class which is not a leaf class, restart loop from he class returned.  If it is a leaf, choose it and terminate.
                </p></li><li class="listitem"><p>
                    If the tc filters did not return a class, but did return a classid, try to find a class with that id within this qdisc.   Checkif the found class is of a lower level than the current class. If so, and the returned class is not a leaf  node,  restart  the loop at the found class. If it is a leaf node, terminate.  If we found an upward reference to a higher level, enter the  fallback algorithm.
                </p></li><li class="listitem"><p>
                    If the tc filters did not return a class, nor a valid reference to one, consider the minor number of the  reference  to  be  the priority. Retrieve a class from the defmap of this class for the priority. If this did not contain a class, consult the defmap of this class for the BEST_EFFORT class. If this is an upward reference, or no BEST_EFFORT class was defined, enter the  fallback algorithm.  If a valid  class  was found, and it is not a leaf node, restart the loop at this class. If it is a leaf, choose it and terminate. If neither the priority distilled from the classid, nor the BEST_EFFORT priority yielded  a  class,  enter  the fallback algorithm.
                </p></li></ul></div><p>
            The fallback algorithm resides outside of the loop and is as follows.
        </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                    Consult the  defmap of the class at which the jump to fallback occured. If the defmap contains a class for the priority of the class (which is related to the ToS field), choose this class and terminate.
                </p></li><li class="listitem"><p>
                    Consult the map for a class for the  BEST_EFFORT  priority.  If found, choose it, and terminate.
                </p></li><li class="listitem"><p>
                    Choose the  class at which break out to the fallback algorithm occurred. Terminate.
                </p></li></ul></div><p>
            The packet is enqueued to the class which was chosen when either  algorithm  terminated. It is therefore possible for a packet to be enqueued not at a leaf node, but in the middle of the hierarchy.
        </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-cbq-link"></a>7.4.4. Link Sharing Algorithm</h4></div></div></div><p>
            When dequeuing for sending to the network device, CBQ decides which of its  classes  will be allowed to send. It does so with a Weighted Round Robin process in which each class with packets gets a chance to send in turn.  The WRR  process  starts by asking the highest priority classes (lowest numerically - highest semantically) for packets, and will  continue to do so until they have no more data to offer, in which case the process repeats for lower priorities.
        </p><p>
            Each class is not allowed to send at length  though,  they  can  only dequeue a configurable amount of data during each round.
        </p><p>
            If a class is about to go overlimit, and it is not bounded it will try to borrow avgidle from siblings that are not isolated.  This process is repeated from the bottom upwards. If a class is unable to borrow enough avgidle to send a packet, it is throttled and not asked  for  a  packet for enough time for the avgidle to increase above zero.
        </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-cbq-root-params"></a>7.4.5. Root Parameters</h4></div></div></div><p>
        The root qdisc of a CBQ class tree has the following parameters:
    </p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="constant">parent</code> <em class="replaceable"><code>root</code></em> | <em class="replaceable"><code>major:minor</code></em></span></dt><dd><p>
                this mandatory parameter determines the place of the CBQ
                instance, either at the root of an interface or within an
                existing class.
            </p></dd><dt><span class="term"><code class="constant">handle</code> <em class="replaceable"><code>major:</code></em></span></dt><dd><p>
                like all other qdiscs, the CBQ can be assigned a handle.
                Should consist only of a major number, followed by a colon.
                This parameter is optional.
            </p></dd><dt><span class="term"><code class="constant">avpkt</code> <em class="replaceable"><code>bytes</code></em></span></dt><dd><p>
                for calculations, the average packet size must be known.  It is
                silently capped at a minimum of 2/3 of the interface MTU.  This
                parameter is mandatory.
            </p></dd><dt><span class="term"><code class="constant">bandwidth</code> <em class="replaceable"><code>rate</code></em></span></dt><dd><p>
                underlying available bandwidth; to determine the idle time,
                CBQ must know the bandwidth of your A) desired target
                bandwidth, B) underlying physical interface or C) parent
                qdisc. This is a vital parameter, more about it later. This
                parameter is mandatory.
            </p></dd><dt><span class="term"><code class="constant">cell</code> <em class="replaceable"><code>size</code></em></span></dt><dd><p>
                the cell size determines the granularity of packet
                transmission time calculations. Must be an integral power of
                2, defaults to 8.
            </p></dd><dt><span class="term"><code class="constant">mpu</code> <em class="replaceable"><code>bytes</code></em></span></dt><dd><p>
                a zero sized packet may still take time to transmit. This
                value is the lower cap for packet transmission time
                calculations - packets smaller than this value are still
                deemed to have this size.  Defaults to 0.
            </p></dd><dt><span class="term"><code class="constant">ewma</code> <em class="replaceable"><code>log</code></em></span></dt><dd><p>
                CBQ calculates idleness using an Exponentially Weighted Moving
                Average (<acronym class="acronym">EWMA</acronym>) which smooths out
                measurements easily accommodating short bursts. The
                <em class="replaceable"><code>log</code></em> value determines how much
                smoothing occurs.  Lower values imply greater sensitivity.
                Must be between 0 and 31.  Defaults to 5.
            </p></dd></dl></div><p>
        A CBQ qdisc does not shape out of its own accord. It only needs to know certain parameters about the underlying link. Actual shaping is done in classes.
    </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="qc-cbq-class-params"></a>7.4.6. Class Parameters</h4></div></div></div><p>
        Classes have a lot of parameters to configure their operation.
    </p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="constant">parent</code> <em class="replaceable"><code>major:minor</code></em></span></dt><dd><p>
                place of this class within the hierarchy. If attached directly
                to a qdisc and not to another  class,  minor  can  be
                omitted.  This parameter is mandatory.
            </p></dd><dt><span class="term"><code class="constant">classid</code> <em class="replaceable"><code>major:minor</code></em></span></dt><dd><p>
                like  qdiscs,  classes  can  be  named. The major number must be equal to the major number of the  qdisc  to  which  it  belongs. Optional, but needed if this class is going to have children.
            </p></dd><dt><span class="term"><code class="constant">weight</code> <em class="replaceable"><code>weightvalue</code></em></span></dt><dd><p>
                when  dequeuing to the lower layer, classes are tried for
                traffic in a round-robin fashion. Classes with a higher
                configured qdisc will generally have more traffic to offer
                during each round, so it makes sense to allow it to dequeue
                more traffic. All weights under a class are normalized, so
                only the ratios matter.  Defaults to the configured rate,
                unless the priority of this class is maximal, in which case it
                is set to 1.
            </p></dd><dt><span class="term"><code class="constant">allot</code> <em class="replaceable"><code>bytes</code></em></span></dt><dd><p>
                allot specifies how many bytes a qdisc can dequeue during
                each round of the process.  This parameter is weighted
                using the renormalized class weight described above.
            </p></dd><dt><span class="term"><code class="constant">priority</code> <em class="replaceable"><code>priovalue</code></em></span></dt><dd><p>
                in the round-robin process, classes with the lowest priority
                field are tried for packets first. This parameter is
                mandatory.
            </p></dd><dt><span class="term"><code class="constant">rate</code> <em class="replaceable"><code>bitrate</code></em></span></dt><dd><p>
                maximum aggregated rate at which this class (children
                inclusive) can transmit.  The bitrate is specified using the
                <span class="command"><strong>tc</strong></span> way of specifying rates (e.g.
                '1544kbit').  This parameter is mandatory.
            </p></dd><dt><span class="term"><code class="constant">bandwidth</code> <em class="replaceable"><code>bitrate</code></em></span></dt><dd><p>
                this is different from the bandwidth specified when creating a
                parent CBQ qdisc.  The CBQ class
                <code class="constant">bandwidth</code> parameter is only used to
                determine maxidle and offtime, which, in turn, are only
                calculated when specifying maxburst or minburst.  Thus, this
                parameter is only required if specifying
                <code class="constant">maxburst</code> or
                <code class="constant">minburst</code>.
            </p></dd><dt><span class="term"><code class="constant">maxburst</code> <em class="replaceable"><code>packetcount</code></em>, </span><span class="term"></span></dt><dd><p>
                this number of packets is used to calculate maxidle so that
                when avgidle  is  at  maxidle,  this number of average packets
                can be burst before avgidle drops to 0. Set it higher to be
                more tolerant  of  bursts.  You can't set maxidle directly,
                only via this parameter.
                </p></dd><dt><span class="term"><code class="constant">minburst</code> <em class="replaceable"><code>packetcount</code></em></span></dt><dd><p>
                    as mentioned before, CBQ needs to throttle in case of
                    overlimit. The  ideal  solution is to do so for exactly
                    the calculated idle time, and pass 1 packet. However, Unix
                    kernels generally have  a hard  time  scheduling events
                    shorter than 10ms, so it is better to throttle for a
                    longer period, and then pass minburst  packets in one go,
                    and then sleep minburst times longer. The  time  to  wait
                    is called the offtime. Higher values of minburst lead to
                    more accurate shaping in the  long  term,  but  to bigger
                    bursts at millisecond timescales.
                </p></dd><dt><span class="term"><code class="constant">minidle</code> <em class="replaceable"><code>microseconds</code></em>, </span><span class="term"></span></dt><dd><p>
                    <span class="emphasis"><em>minidle:</em></span> if  avgidle is below 0, we are overlimits and need to wait until avgidle will be big enough to send one packet. To prevent a sudden burst from shutting down the link for a prolonged period of time, avgidle is reset to minidle if it gets too low.  Minidle is specified in negative microseconds, so 10 means  that avgidle is capped at -10us.
                </p></dd><dt><span class="term"><code class="constant">bounded</code> | <code class="constant">borrow</code>, </span><span class="term"></span></dt><dd><p>
                    identifies a borrowing policy.  Either the class will try
                    to <code class="constant">borrow</code> bandwidth from its siblings
                    or it will consider itself <code class="constant">bounded</code>.
                    Mutually exclusive.
                </p></dd><dt><span class="term"><code class="constant">isolated</code> | <code class="constant">sharing</code></span></dt><dd><p>
                    identifies a sharing policy.  Either the class will engage
                    in a <code class="constant">sharing</code> policy toward its
                    siblings or it will consider itself
                    <code class="constant">isolated</code>.  Mutually exclusive.
                </p></dd><dt><span class="term"></span></dt><dd><p>
                    <span class="emphasis"><em>split major:minor and defmap bitmap[/bitmap]:</em></span> if consulting filters attached to a class did not  give  a  verdict,  CBQ  can  also  classify  based on the packet's priority. There are 16 priorities available, numbered from 0 to 15. The defmap  specifies  which  priorities  this  class  wants  to receive, specified as a bitmap. The Least Significant Bit corresponds to priority zero. The split parameter tells CBQ at  which class the decision must be made, which should be a (grand)parent of the class you are adding.
                </p><p>
                    As an example, 'tc class add ... classid 10:1 cbq .. split  10:0 defmap c0' configures class 10:0 to send packets with priorities  6 and 7 to 10:1.
                </p><p>
                    The complimentary configuration would then be: 'tc class add ... classid  10:2 cbq ... split 10:0 defmap 3f' Which would send all packets 0, 1, 2, 3, 4 and 5 to 10:1.
                </p></dd><dt><span class="term"></span></dt><dd><p>
                    <span class="emphasis"><em>estimator interval timeconstant:</em></span> CBQ can measure how much bandwidth each class is using, which tc filters  can use to classify packets with. In order to determine the bandwidth it uses a very simple estimator that measures once every  interval  microseconds  how much traffic has passed. This again is a EWMA, for which the time constant can  be  specified, also in microseconds. The time constant corresponds to the sluggishness of the measurement or, conversely, to  the  sensitivity of  the  average to short bursts. Higher values mean less sensitivity.
                </p></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="qc-wrr"></a>7.5. WRR, Weighted Round Robin</h3></div></div></div><p>
          This qdisc is not included in the standard kernels.
      </p><p>
          The WRR qdisc distributes bandwidth between its classes using the weighted round robin scheme. That is, like the CBQ qdisc it contains classes into which arbitrary qdiscs can be plugged. All classes which have sufficient demand will get bandwidth proportional to the weights associated with the classes. The weights can be set manually using the tc program. But they can also be made automatically decreasing for classes transferring much data.
      </p><p>
          The qdisc has a built-in classifier which assigns packets coming from or sent to different machines to different classes. Either the MAC or IP and either source or destination addresses can be used. The MAC address can only be used when the Linux box is acting as an ethernet bridge, however. The classes are automatically assigned to machines based on the packets seen.
      </p><p>
          The qdisc can be very useful at sites where a lot of unrelated individuals share an Internet connection. A set of scripts setting up a relevant behavior for such a site is a central part of the WRR distribution.
      </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="rules"></a>8. Rules, Guidelines and Approaches</h2></div></div></div><p>
  </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="r-general"></a>8.1. General Rules of Linux Traffic Control</h3></div></div></div><p>
      There are a few general rules which ease the study of Linux traffic
      control.
      Traffic control structures under Linux are the same whether the initial
      configuration has been done with <a class="link" href="#s-tcng" title="5.3. tcng, Traffic Control Next Generation"><span class="command"><strong>tcng</strong></span></a> or with <a class="link" href="#s-iproute2-tc"><span class="command"><strong>tc</strong></span></a>.
    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
          Any router performing a shaping function should be the bottleneck on
          the link, and should be shaping slightly below the maximum available
          link bandwidth.  This prevents queues from forming in other routers,
          affording maximum control of packet latency/deferral to the shaping
          device.
        </p></li><li class="listitem"><p>
          A device can only shape traffic it transmits
          <a href="#ftn.idm2305" class="footnote" name="idm2305"><sup class="footnote">[10]</sup></a>.  Because the traffic has already been received on an
          input interface, the traffic cannot be shaped.  A traditional
          solution to this problem is an ingress policer.
        </p></li><li class="listitem"><p>
          Every interface must have a <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a>.  The default qdisc
          (the <a class="link" href="#qs-pfifo_fast" title="6.2. pfifo_fast, the default Linux qdisc"><code class="constant">pfifo_fast</code></a> qdisc) is used when another qdisc is not
          explicitly attached to the interface.
        </p></li><li class="listitem"><p>
          One of the <a class="link" href="#classful-qdiscs" title="7. Classful Queuing Disciplines (qdiscs)">classful qdiscs</a> added to an interface with no children
          classes typically only consumes CPU for no benefit.
        </p></li><li class="listitem"><p>
          Any newly created class contains a <a class="link" href="#qs-fifo" title="6.1. FIFO, First-In First-Out (pfifo and bfifo)">FIFO</a>.
          This qdisc can be replaced explicitly with any other qdisc.  The
          FIFO qdisc will be removed implicitly if a child class is
          attached to this class.
        </p></li><li class="listitem"><p>
          Classes directly attached to the <code class="constant">root</code> qdisc can be used to
          simulate virtual circuits.
        </p></li><li class="listitem"><p>
          A <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a> can be attached to classes or one of the
          <a class="link" href="#classful-qdiscs" title="7. Classful Queuing Disciplines (qdiscs)">classful qdiscs</a>.
        </p></li></ul></div><p>
    </p><p>
    </p><p>
    </p><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="r-known-bandwidth"></a>8.2. Handling a link with a known bandwidth</h3></div></div></div><p>
      HTB is an ideal <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a> to use on a link with a known
      bandwidth, because the innermost (root-most) class can be set to the
      maximum bandwidth available on a given link.  Flows can be further
      subdivided into children classes, allowing either guaranteed bandwidth
      to particular classes of traffic or allowing preference to specific
      kinds of traffic.
    </p><p>
    </p><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="r-unknown-bandwidth"></a>8.3. Handling a link with a variable (or unknown) bandwidth</h3></div></div></div><p>
      In theory, the PRIO scheduler is an ideal match for links with
      variable bandwidth, because it is a work-conserving <a class="link" href="#c-qdisc" title="4.1. qdisc"><code class="constant">qdisc</code></a> (which
      means that it provides no <a class="link" href="#e-shaping" title="3.1. Shaping">shaping</a>).  In the case of a link
      with an unknown or fluctuating bandwidth, the PRIO scheduler
      simply prefers to dequeue any available packet in the highest priority
      band first, then falling to the lower priority queues.
    </p><p>
    </p><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="r-sharing-flows"></a>8.4. Sharing/splitting bandwidth based on flows</h3></div></div></div><p>
      Of the many types of contention for network bandwidth, this is one of
      the easier types of contention to address in general.  By using the
      SFQ qdisc, traffic in a particular queue can be separated into
      flows, each of which will be serviced fairly (inside that queue).
      Well-behaved applications (and users) will find that using SFQ and
      ESFQ are sufficient for most sharing needs.
    </p><p>
      The Achilles heel of these fair queuing algorithms is a misbehaving user
      or application which opens many connections simultaneously (e.g., eMule,
      eDonkey, Kazaa).  By creating a large number of individual flows, the
      application can dominate slots in the fair queuing algorithm.  Restated,
      the fair queuing algorithm has no idea that a single application is
      generating the majority of the flows, and cannot penalize the user.
      Other methods  are called for.
    </p><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="r-sharing-ips"></a>8.5. Sharing/splitting bandwidth based on IP</h3></div></div></div><p>
      For many administrators this is the ideal method of dividing bandwidth
      amongst their users.  Unfortunately, there is no easy solution, and it
      becomes increasingly complex with the number of machine sharing a
      network link.
    </p><p>
      To divide bandwidth equitably between <em class="parameter"><code>N</code></em> IP
      addresses, there must be <em class="parameter"><code>N</code></em> classes.
    </p><p>
    </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="scripts"></a>9. Scripts for use with QoS/Traffic Control</h2></div></div></div><p>
  </p><p>
  </p><p>
  </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="sc-wondershaper"></a>9.1. wondershaper</h3></div></div></div><p>
      More to come, see <a class="ulink" href="http://lartc.org/wondershaper/" target="_top">wondershaper</a>.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="sc-myshaper"></a>9.2. ADSL Bandwidth HOWTO script (<code class="filename">myshaper</code>)</h3></div></div></div><p>
      More to come, see <a class="ulink" href="http://www.tldp.org/HOWTO/ADSL-Bandwidth-Management-HOWTO/implementation.html" target="_top">myshaper</a>.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="sc-htb.init"></a>9.3. <code class="filename">htb.init</code></h3></div></div></div><p>
      More to come, see <a class="ulink" href="http://sourceforge.net/projects/htbinit/" target="_top"><code class="filename">htb.init</code></a>.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="sc-tcng.init"></a>9.4. <code class="filename">tcng.init</code></h3></div></div></div><p>
      More to come, see <a class="ulink" href="http://linux-ip.net/code/tcng/tcng.init" target="_top"><code class="filename">tcng.init</code></a>.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="sc-cbq.init"></a>9.5. <code class="filename">cbq.init</code></h3></div></div></div><p>
      More to come, see <a class="ulink" href="http://sourceforge.net/projects/cbqinit/" target="_top"><code class="filename">cbq.init</code></a>.
    </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="diagram"></a>10. Diagram</h2></div></div></div><p>
  </p><p>
  </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="d-general"></a>10.1. General diagram</h3></div></div></div><p>
      Below is a general diagram of the relationships of the components of a
      classful queuing discipline (HTB pictured).  A larger version of
      the diagram is
      <a class="ulink" href="http://linux-ip.net/traffic-control/htb-class.png" target="_top">available</a>.
    </p><p>
    </p><div class="example"><a name="d-tcng-config"></a><p class="title"><b>Example 12. An example HTB <span class="command">tcng</span> configuration</b></p><div class="example-contents"><pre class="programlisting">
/*
 *
 *  possible mock up of diagram shown at
 *  http://linux-ip.net/traffic-control/htb-class.png
 *
 */

$m_web = trTCM (
                 cir 512  kbps,  /* commited information rate */
                 cbs 10   kB,    /* burst for CIR */
                 pir 1024 kbps,  /* peak information rate */
                 pbs 10   kB     /* burst for PIR */
               ) ;

dev eth0 {
    egress {

        class ( &lt;$web&gt; )  if tcp_dport == PORT_HTTP &amp;&amp;  __trTCM_green( $m_web );
        class ( &lt;$bulk&gt; ) if tcp_dport == PORT_HTTP &amp;&amp; __trTCM_yellow( $m_web );
        drop              if                              __trTCM_red( $m_web );
        class ( &lt;$bulk&gt; ) if tcp_dport == PORT_SSH ;

        htb () {  /* root qdisc */

            class ( rate 1544kbps, ceil 1544kbps ) {  /* root class */

                $web  = class ( rate 512kbps, ceil  512kbps ) { sfq ; } ;
                $bulk = class ( rate 512kbps, ceil 1544kbps ) { sfq ; } ;

            }
        }
    }
}
      </pre></div></div><br class="example-break"><div class="mediaobject"><a name="img-d-general"></a><object type="image/svg+xml" data="images/htb-class.svg"></object></div><p>
    </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="links"></a>11. Annotated Traffic Control Links</h2></div></div></div><p>
    This section identifies a number of links to documentation
    about traffic control and Linux traffic control software.  Each link will
    be listed with a brief description of the content at that site.
  </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
        <a class="ulink" href="http://luxik.cdi.cz/~devik/qos/htb/" target="_top">HTB
          site</a>,
        <a class="ulink" href="http://luxik.cdi.cz/~devik/qos/htb/manual/userg.htm" target="_top">HTB
         user guide</a> and
        <a class="ulink" href="http://luxik.cdi.cz/~devik/qos/htb/manual/theory.htm" target="_top">HTB
         theory</a>
        (<span class="emphasis"><em>Martin <span class="quote">&#8220;<span class="quote">devik</span>&#8221;</span> Devera</em></span>)
      </p><p>
        Hierarchical Token Bucket, <a class="link" href="#qc-htb" title="7.1. HTB, Hierarchical Token Bucket">HTB</a>, is a classful queuing
        discipline.  Widely used and supported it is also fairly well
        documented in the user guide and at
        <a class="ulink" href="http://www.docum.org/docum.org/" target="_top">Stef Coene's site</a>
        (see below).
      </p></li><li class="listitem"><p>
        <a class="ulink" href="http://opalsoft.net/qos/" target="_top">General Quality of
        Service docs</a> (<span class="emphasis"><em>Leonardo Balliache</em></span>)
      </p><p>
        There is a good deal of understandable and introductory documentation
        on his site, and in particular has some excellent overview material.
        See in particular, the detailed
        <a class="ulink" href="http://opalsoft.net/qos/DS.htm" target="_top">Linux QoS</a> document
        among others.
      </p></li><li class="listitem"><p>
         <a class="ulink" href="http://tcng.sourceforge.net/" target="_top"><span class="command"><strong>tcng</strong></span> (Traffic Control
           Next Generation)</a> and
        <a class="ulink" href="http://linux-ip.net/gl/tcng/" target="_top"><span class="command"><strong>tcng</strong></span> manual</a>
          (<span class="emphasis"><em>Werner Almesberger</em></span>)
      </p><p>
        The <span class="command"><strong>tcng</strong></span> software includes a language and a set of tools for
        creating and testing traffic control structures.  In addition to
        generating <span class="command"><strong>tc</strong></span> commands as output, it is also capable of providing
        output for non-Linux applications.  A key piece of the <span class="command"><strong>tcng</strong></span> suite
        which is ignored in this documentation is the <span class="command"><strong>tcsim</strong></span>
        traffic control simulator.
      </p><p>
        The user manual provided with the <span class="command"><strong>tcng</strong></span> software has been converted
        to HTML with <span class="command"><strong>latex2html</strong></span>.  The distribution comes
        with the TeX documentation.
      </p></li><li class="listitem"><p>
        <a class="ulink" href="ftp://ftp.inr.ac.ru/ip-routing/" target="_top"><span class="command"><strong>iproute2</strong></span></a> and
        <a class="ulink" href="http://linux-ip.net/gl/ip-cref/" target="_top"><span class="command"><strong>iproute2</strong></span> manual</a>
        (<span class="emphasis"><em>Alexey Kuznetsov</em></span>)
      </p><p>
        This is a the source code for the <span class="command"><strong>iproute2</strong></span> suite, which includes the
        essential <span class="command"><strong>tc</strong></span> binary.  Note, that as of
        iproute2-2.4.7-now-ss020116-try.tar.gz, the package did not support
        HTB, so a patch available from the <a class="ulink" href="http://luxik.cdi.cz/~devik/qos/htb/" target="_top">HTB</a> site will be
        required.
      </p><p>
        The manual documents the entire suite of tools, although the <span class="command"><strong>tc</strong></span>
        utility is not adequately documented here.  The ambitious reader is
        recommended to the LARTC HOWTO after consuming this introduction.
      </p></li><li class="listitem"><p>
        <a class="ulink" href="http://www.docum.org/" target="_top">Documentation, graphs, scripts and
        guidelines to traffic control under Linux</a>
        (<span class="emphasis"><em>Stef Coene</em></span>)
      </p><p>
        Stef Coene has been gathering statistics and test results, scripts and
        tips for the use of QoS under Linux.  There are some particularly
        useful graphs and guidelines available for implementing traffic
        control at Stef's site.
      </p></li><li class="listitem"><p>
        <a class="ulink" href="http://lartc.org/howto/" target="_top">LARTC HOWTO</a>
        (<span class="emphasis"><em>bert hubert, et. al.</em></span>)
      </p><p>
        The Linux Advanced Routing and Traffic Control HOWTO is one of the key
        sources of data about the sophisticated techniques which are available
        for use under Linux.  The Traffic Control Introduction HOWTO should
        provide the reader with enough background in the language and concepts
        of traffic control.  The LARTC HOWTO is the next place the reader
        should look for general traffic control information.
      </p></li><li class="listitem"><p>
        <a class="ulink" href="http://linux-ip.net/" target="_top">Guide to IP Networking with
        Linux</a> (<span class="emphasis"><em>Martin A. Brown</em></span>)
      </p><p>
        Not directly related to traffic control, this site includes articles
        and general documentation on the behaviour of the Linux IP layer.
      </p></li><li class="listitem"><p>
        <a class="ulink" href="http://www.almesberger.net/cv/papers.html" target="_top">Werner
        Almesberger's Papers</a>
      </p><p>
        Werner Almesberger is one of the main developers and champions of
        traffic control under Linux (he's also the author of <span class="command"><strong>tcng</strong></span>, above).
        One of the key documents describing the entire traffic control
        architecture of the Linux kernel is his Linux Traffic Control -
        Implementation Overview which is available in
        <a class="ulink" href="http://www.almesberger.net/cv/papers/tcio8.pdf" target="_top">PDF</a>
        or
        <a class="ulink" href="http://www.almesberger.net/cv/papers/tcio8.ps.gz" target="_top">PS</a>
        format.
      </p></li><li class="listitem"><p>
        <a class="ulink" href="http://diffserv.sourceforge.net/" target="_top">Linux DiffServ
        project</a>
      </p><p>
        Mercilessly snipped from the main page of the DiffServ site...
      </p><div class="blockquote"><blockquote class="blockquote"><p>
         Differentiated Services (short: Diffserv) is an architecture for
         providing different types or levels of service for network traffic.
         One key characteristic of Diffserv is that flows are aggregated in
         the network, so that core routers only need to distinguish a
         comparably small number of aggregated flows, even if those flows
         contain thousands or millions of individual flows.
         </p></blockquote></div></li></ul></div></div><div class="footnotes"><br><hr style="width:100; text-align:left;margin-left: 0"><div id="ftn.idm113" class="footnote"><p><a href="#idm113" class="para"><sup class="para">[1] </sup></a>
          See <a class="xref" href="#software" title="5. Software and Tools">Section 5, &#8220;Software and Tools&#8221;</a> for more details on the use or
          installation of a particular traffic control mechanism, kernel or
          command line utility.
        </p></div><div id="ftn.idm249" class="footnote"><p><a href="#idm249" class="para"><sup class="para">[2] </sup></a>
          This queueing model has long been used in civilized countries to
          distribute scant food or provisions equitably.  William Faulkner is
          reputed to have walked to the front of the line for to fetch his
          share of ice, proving that not everybody likes the FIFO model, and
          providing us a model for considering priority queuing.
        </p></div><div id="ftn.idm254" class="footnote"><p><a href="#idm254" class="para"><sup class="para">[3] </sup></a>
          Similarly, the entire traffic control system appears as a queue or
          scheduler to the higher layer which is enqueuing packets into this
          layer.
        </p></div><div id="ftn.idm412" class="footnote"><p><a href="#idm412" class="para"><sup class="para">[4] </sup></a>
          This smoothing effect is not always desirable, hence the HTB
          parameters burst and cburst.
        </p></div><div id="ftn.idm577" class="footnote"><p><a href="#idm577" class="para"><sup class="para">[5] </sup></a>
          A classful qdisc can only have children classes of its type.  For
          example, an HTB qdisc can only have HTB classes as children.  A CBQ
          qdisc cannot have HTB classes as children.
        </p></div><div id="ftn.idm639" class="footnote"><p><a href="#idm639" class="para"><sup class="para">[6] </sup></a>
          In this case, you'll have a <a class="link" href="#c-filter" title="4.3. filter"><code class="constant">filter</code></a> which uses a
          <a class="link" href="#c-classifier" title="4.4. classifier"><code class="constant">classifier</code></a> to select the packets you wish to drop.  Then
          you'll use a <a class="link" href="#c-police" title="4.5. policer"><code class="constant">policer</code></a> with a with a drop action like this
          <span class="command"><strong>police rate 1bps burst 1 action drop/drop</strong></span>.
        </p></div><div id="ftn.idm661" class="footnote"><p><a href="#idm661" class="para"><sup class="para">[7] </sup></a>
          I do not know the range nor base of these numbers.  I believe they
          are u32 hexadecimal, but need to confirm this.
        </p></div><div id="ftn.idm803" class="footnote"><p><a href="#idm803" class="para"><sup class="para">[8] </sup></a>
            The options listed in this example are taken from a 2.4.20 kernel
            source tree.  The exact options may differ slightly from kernel
            release to kernel release depending on patches and new schedulers
            and classifiers.
          </p></div><div id="ftn.idm1853" class="footnote"><p><a href="#idm1853" class="para"><sup class="para">[9] </sup></a>
                HTB will report bandwidth usage in this scenario
                incorrectly.  It will calculate the bandwidth used by
                <em class="parameter"><code>quantum</code></em> instead of the real dequeued packet size.
                This can skew results quickly.
              </p></div><div id="ftn.idm2305" class="footnote"><p><a href="#idm2305" class="para"><sup class="para">[10] </sup></a>
              In fact, the
              <a class="link" href="#s-imq" title="5.5. IMQ, Intermediate Queuing device">Intermediate Queuing Device
              (IMQ)</a> simulates an output device onto which traffic
              control structures can be attached.  This clever solution allows
              a networking device to shape ingress traffic in the same fashion
              as egress traffic.  Despite the apparent contradiction of the
              rule, IMQ appears as a device to the kernel.  Thus, there has
              been no violation of the rule, but rather a sneaky
              reinterpretation of that rule.
            </p></div></div></div></body></html>
