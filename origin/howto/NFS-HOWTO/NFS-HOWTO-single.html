<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN""http://www.w3.org/TR/html4/loose.dtd">
<HTML
><HEAD
><TITLE
>Linux NFS-HOWTO</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.79"></HEAD
><BODY
CLASS="ARTICLE"
BGCOLOR="#FFFFFF"
TEXT="#000000"
LINK="#0000FF"
VLINK="#840084"
ALINK="#0000FF"
><DIV
CLASS="ARTICLE"
><DIV
CLASS="TITLEPAGE"
><H1
CLASS="TITLE"
><A
NAME="AEN2"
>Linux NFS-HOWTO</A
></H1
><H3
CLASS="AUTHOR"
><A
NAME="AEN4"
>Tavis Barr</A
></H3
><DIV
CLASS="AFFILIATION"
><DIV
CLASS="ADDRESS"
><P
CLASS="ADDRESS"
>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<CODE
CLASS="EMAIL"
>&#60;<A
HREF="mailto:tavis dot barr at liu dot edu"
>tavis dot barr at liu dot edu</A
>&#62;</CODE
><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</P
></DIV
></DIV
><H3
CLASS="AUTHOR"
><A
NAME="AEN10"
>Nicolai Langfeldt</A
></H3
><DIV
CLASS="AFFILIATION"
><DIV
CLASS="ADDRESS"
><P
CLASS="ADDRESS"
>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<CODE
CLASS="EMAIL"
>&#60;<A
HREF="mailto:janl at linpro dot no"
>janl at linpro dot no</A
>&#62;</CODE
><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</P
></DIV
></DIV
><H3
CLASS="AUTHOR"
><A
NAME="AEN16"
>Seth Vidal</A
></H3
><DIV
CLASS="AFFILIATION"
><DIV
CLASS="ADDRESS"
><P
CLASS="ADDRESS"
>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<CODE
CLASS="EMAIL"
>&#60;<A
HREF="mailto:skvidal at phy dot duke dot edu"
>skvidal at phy dot duke dot edu</A
>&#62;</CODE
><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</P
></DIV
></DIV
><H3
CLASS="AUTHOR"
><A
NAME="AEN22"
>Tom McNeal</A
></H3
><DIV
CLASS="AFFILIATION"
><DIV
CLASS="ADDRESS"
><P
CLASS="ADDRESS"
>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<CODE
CLASS="EMAIL"
>&#60;<A
HREF="mailto:trmcneal at attbi dot com"
>trmcneal at attbi dot com</A
>&#62;</CODE
><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</P
></DIV
></DIV
><P
CLASS="PUBDATE"
>2002-08-25<BR></P
><DIV
CLASS="REVHISTORY"
><TABLE
WIDTH="100%"
BORDER="0"
><TR
><TH
ALIGN="LEFT"
VALIGN="TOP"
COLSPAN="3"
><B
>Revision History</B
></TH
></TR
><TR
><TD
ALIGN="LEFT"
>Revision v3.1</TD
><TD
ALIGN="LEFT"
>2002-08-25</TD
><TD
ALIGN="LEFT"
>Revised by: tavis</TD
></TR
><TR
><TD
ALIGN="LEFT"
COLSPAN="3"
>Typo in firewalling section in 3.0</TD
></TR
><TR
><TD
ALIGN="LEFT"
>Revision v3.0</TD
><TD
ALIGN="LEFT"
>2002-07-16</TD
><TD
ALIGN="LEFT"
>Revised by: tavis</TD
></TR
><TR
><TD
ALIGN="LEFT"
COLSPAN="3"
>Updates plus additions to performance, security</TD
></TR
></TABLE
></DIV
><HR></DIV
><DIV
CLASS="TOC"
><DL
><DT
><B
>Table of Contents</B
></DT
><DT
>1. <A
HREF="#PREAMBLE"
>Preamble</A
></DT
><DD
><DL
><DT
>1.1. <A
HREF="#LEGAL"
>Legal stuff</A
></DT
><DT
>1.2. <A
HREF="#DISCLAIMER"
>Disclaimer</A
></DT
><DT
>1.3. <A
HREF="#FEEDBACK"
>Feedback</A
></DT
><DT
>1.4. <A
HREF="#TRANSLATION"
>Translation</A
></DT
><DT
>1.5. <A
HREF="#DEDICATION"
>Dedication</A
></DT
></DL
></DD
><DT
>2. <A
HREF="#INTRO"
>Introduction</A
></DT
><DD
><DL
><DT
>2.1. <A
HREF="#WHAT"
>What is NFS?</A
></DT
><DT
>2.2. <A
HREF="#SCOPE"
>What is this HOWTO and what is it not?</A
></DT
><DT
>2.3. <A
HREF="#KNOWPREREQ"
>Knowledge Pre-Requisites</A
></DT
><DT
>2.4. <A
HREF="#SWPREREQ"
>Software Pre-Requisites: Kernel Version and nfs-utils</A
></DT
><DT
>2.5. <A
HREF="#FURTHERHELP"
>Where to get help and further information</A
></DT
></DL
></DD
><DT
>3. <A
HREF="#SERVER"
>Setting Up an NFS Server</A
></DT
><DD
><DL
><DT
>3.1. <A
HREF="#SERVERINTRO"
>Introduction to the server setup</A
></DT
><DT
>3.2. <A
HREF="#CONFIG"
>Setting up the Configuration Files</A
></DT
><DT
>3.3. <A
HREF="#SERVICESTART"
>Getting the services started</A
></DT
><DT
>3.4. <A
HREF="#VERIFY"
>Verifying that NFS is running</A
></DT
><DT
>3.5. <A
HREF="#LATER"
>Making changes to /etc/exports later on</A
></DT
></DL
></DD
><DT
>4. <A
HREF="#CLIENT"
>Setting up an NFS Client</A
></DT
><DD
><DL
><DT
>4.1. <A
HREF="#REMOTEMOUNT"
>Mounting remote directories</A
></DT
><DT
>4.2. <A
HREF="#BOOT-TIME-NFS"
>Getting NFS File Systems to Be Mounted at Boot Time</A
></DT
><DT
>4.3. <A
HREF="#MOUNTOPTIONS"
>Mount options</A
></DT
></DL
></DD
><DT
>5. <A
HREF="#PERFORMANCE"
>Optimizing NFS Performance</A
></DT
><DD
><DL
><DT
>5.1. <A
HREF="#BLOCKSIZES"
>Setting Block Size to Optimize Transfer Speeds</A
></DT
><DT
>5.2. <A
HREF="#PACKET-AND-NETWORK"
>Packet Size and Network Drivers</A
></DT
><DT
>5.3. <A
HREF="#FRAG-OVERFLOW"
>Overflow of Fragmented Packets</A
></DT
><DT
>5.4. <A
HREF="#NFS-TCP"
>NFS over TCP</A
></DT
><DT
>5.5. <A
HREF="#TIMEOUT"
>Timeout and Retransmission Values</A
></DT
><DT
>5.6. <A
HREF="#NFSD-INSTANCE"
>Number of Instances of the NFSD Server Daemon</A
></DT
><DT
>5.7. <A
HREF="#MEMLIMITS"
>Memory Limits on the Input Queue</A
></DT
><DT
>5.8. <A
HREF="#AUTONEGOTIATION"
>Turning Off Autonegotiation of NICs and Hubs</A
></DT
><DT
>5.9. <A
HREF="#SYNC-ASYNC"
>Synchronous vs. Asynchronous Behavior in NFS</A
></DT
><DT
>5.10. <A
HREF="#NON-NFS-PERFORMANCE"
>Non-NFS-Related Means of Enhancing Server Performance</A
></DT
></DL
></DD
><DT
>6. <A
HREF="#SECURITY"
>Security and NFS</A
></DT
><DD
><DL
><DT
>6.1. <A
HREF="#PORTMAPPER-SECURITY"
>The portmapper</A
></DT
><DT
>6.2. <A
HREF="#SERVER.SECURITY"
>Server security: nfsd and mountd</A
></DT
><DT
>6.3. <A
HREF="#CLIENT.SECURITY"
>Client Security</A
></DT
><DT
>6.4. <A
HREF="#FIREWALLS"
>NFS and firewalls (ipchains and netfilter)</A
></DT
><DT
>6.5. <A
HREF="#NFS-SSH"
>Tunneling NFS through SSH</A
></DT
><DT
>6.6. <A
HREF="#SUMMARY"
>Summary</A
></DT
></DL
></DD
><DT
>7. <A
HREF="#TROUBLESHOOTING"
>Troubleshooting</A
></DT
><DD
><DL
><DT
>7.1. <A
HREF="#SYMPTOM1"
>Unable to See Files on a Mounted File System</A
></DT
><DT
>7.2. <A
HREF="#SYMPTOM2"
>File requests hang or timeout waiting for access to the file.</A
></DT
><DT
>7.3. <A
HREF="#SYMPTOM3"
>Unable to mount a file system</A
></DT
><DT
>7.4. <A
HREF="#SYMPTOM4"
>I do not have permission to access files on the mounted volume.</A
></DT
><DT
>7.5. <A
HREF="#SYMPTOM5"
>When I transfer really big files, NFS takes over
   all the CPU cycles on the server and it screeches to a halt.</A
></DT
><DT
>7.6. <A
HREF="#SYMPTOM6"
>Strange error or log messages</A
></DT
><DT
>7.7. <A
HREF="#SYMPTOM7"
>Real permissions don't match what's in <TT
CLASS="FILENAME"
>/etc/exports</TT
>.</A
></DT
><DT
>7.8. <A
HREF="#SYMPTOM8"
>Flaky and unreliable behavior</A
></DT
><DT
>7.9. <A
HREF="#SYMPTOM9"
>nfsd won't start</A
></DT
><DT
>7.10. <A
HREF="#SYMPTOM10"
>File Corruption When Using Multiple Clients</A
></DT
></DL
></DD
><DT
>8. <A
HREF="#INTEROP"
>Using Linux NFS with Other OSes</A
></DT
><DD
><DL
><DT
>8.1. <A
HREF="#AIX"
>AIX</A
></DT
><DT
>8.2. <A
HREF="#BSD"
>BSD</A
></DT
><DT
>8.3. <A
HREF="#TRU64"
>Tru64 Unix</A
></DT
><DT
>8.4. <A
HREF="#HPUX"
>HP-UX</A
></DT
><DT
>8.5. <A
HREF="#IRIX"
>IRIX</A
></DT
><DT
>8.6. <A
HREF="#SOLARIS"
>Solaris</A
></DT
><DT
>8.7. <A
HREF="#SUNOS"
>SunOS</A
></DT
></DL
></DD
></DL
></DIV
><DIV
CLASS="SECT1"
><H2
CLASS="SECT1"
><A
NAME="PREAMBLE"
>1. Preamble</A
></H2
><DIV
CLASS="SECT2"
><H3
CLASS="SECT2"
><A
NAME="LEGAL"
>1.1. Legal stuff</A
></H3
><P
>Copyright (c) &#60;2002&#62; by Tavis Barr, Nicolai Langfeldt, 
Seth Vidal, and Tom McNeal.   
This material may be distributed only subject to the terms and conditions set 
forth in the Open Publication License, v1.0 or later (the latest version 
is presently available at <A
HREF="http://www.opencontent.org/openpub/"
TARGET="_top"
>http://www.opencontent.org/openpub/</A
>).</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="DISCLAIMER"
>1.2. Disclaimer</A
></H3
><P
>This document is provided without any guarantees, including 
merchantability or fitness for a particular use.  The maintainers
cannot be responsible if following instructions in this document
leads to damaged equipment or data, angry neighbors, strange habits, 
divorce, or any other calamity.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="FEEDBACK"
>1.3. Feedback</A
></H3
><P
>This will never be a finished document; we welcome feedback about 
  how it can be improved.  As of February 2002, the Linux NFS home 
  page is  being hosted at <A
HREF="http://nfs.sourceforge.net"
TARGET="_top"
>http://nfs.sourceforge.net</A
>.  Check there 
  for mailing lists, bug fixes, and updates, and also to verify
  who currently maintains this document.
 </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="TRANSLATION"
>1.4. Translation</A
></H3
><P
>If you are able to translate this document into another language, 
we would be grateful and we will also do our best to assist you.
Please notify the maintainers.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="DEDICATION"
>1.5. Dedication</A
></H3
><P
>NFS on Linux was made possible by a collaborative effort of many 
people, but a few stand out for special recognition.  The original
version was developed by Olaf Kirch and Alan Cox.  The version 3 
server code was solidified by Neil Brown, based on work from
Saadia Khan, James Yarbrough, Allen Morris, H.J. Lu, and others
(including himself).  The client code was written by Olaf Kirch and 
updated by Trond Myklebust.  The version 4 lock manager was developed 
by Saadia Khan.  Dave Higgen and H.J. Lu both have undertaken the
thankless job of extensive maintenance and bug fixes to get the 
code to actually work the way it was supposed to.  H.J. has also
done extensive development of the nfs-utils package.  Of course this 
dedication is leaving many people out.</P
><P
>The original version of this document was developed by Nicolai 
Langfeldt.  It was heavily rewritten in 2000 by Tavis Barr
and Seth Vidal to reflect substantial changes in the workings
of NFS for Linux developed between the 2.0 and 2.4 kernels.
It was edited again in February 2002, when Tom McNeal made substantial
additions to the performance section.
Thomas Emmel, Neil Brown, Trond Myklebust, Erez Zadok, and Ion Badulescu
also provided valuable comments and contributions.</P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="INTRO"
>2. Introduction</A
></H2
><DIV
CLASS="SECT2"
><H3
CLASS="SECT2"
><A
NAME="WHAT"
>2.1. What is NFS?</A
></H3
><P
>      The Network File System (NFS) was developed to allow machines 
      to mount a disk partition on a remote machine as if it were on
      a local hard drive.   This allows for fast, seamless sharing of 
      files across a network.
    </P
><P
>      It also gives the potential for unwanted people to access your 
      hard drive over the network (and thereby possibly read your email 
      and delete all  your files as well as break into your system) if 
      you set it up incorrectly.  So please read the Security section of 
      this document carefully if you intend to implement an NFS setup.
    </P
><P
>      There are other systems that provide similar functionality to NFS.
      Samba (<A
HREF="http://www.samba.org"
TARGET="_top"
>http://www.samba.org</A
>)
      provides file services to Windows clients.  The Andrew File
      System from IBM (<A
HREF="http://www.transarc.com/Product/EFS/AFS/index.html"
TARGET="_top"
>http://www.transarc.com/Product/EFS/AFS/index.html</A
>), 
      recently open-sourced, provides a file sharing mechanism with some 
      additional security and performance features.  The Coda File System 
      (<A
HREF="http://www.coda.cs.cmu.edu/"
TARGET="_top"
>http://www.coda.cs.cmu.edu/</A
>) is still in development as of this writing
      but is designed to work well with disconnected clients.  Many of the
      features of the Andrew and Coda file systems are slated for inclusion
      in the next version of NFS (Version 4) (<A
HREF="http://www.nfsv4.org"
TARGET="_top"
>http://www.nfsv4.org</A
>).  The 
      advantage of NFS today is that it is mature, standard, well understood, 
      and supported robustly across a variety of platforms.
     </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SCOPE"
>2.2. What is this HOWTO and what is it not?</A
></H3
><P
>     This HOWTO is intended as a complete, step-by-step guide to setting
     up NFS correctly and effectively.  Setting up NFS involves two steps,
     namely configuring the server and then configuring the client.  Each 
     of these steps is dealt with in order.  The document then offers     
     some tips for people with particular needs and hardware setups, as   
     well as security and troubleshooting advice.  
   </P
><P
>     This HOWTO is not a description of the guts and
     underlying structure of NFS.  For that you may wish to read
     <EM
>Linux NFS and Automounter Administration</EM
> by Erez Zadok (Sybex, 2001).  The classic NFS book, updated and still quite useful, is <EM
>Managing NFS and NIS</EM
> by Hal Stern, published by O'Reilly &#38;
     Associates, Inc.  A much more advanced technical 
     description of NFS is available in <EM
>NFS Illustrated</EM
> by Brent Callaghan.
   </P
><P
>     This document is also not intended as a complete reference manual,
     and does not contain an exhaustive list of the features of Linux
     NFS.  For that, you can look at the man pages for <EM
>nfs(5)</EM
>, 
     <EM
>exports(5)</EM
>, <EM
>mount(8)</EM
>, <EM
>fstab(5)</EM
>, 
     <EM
>nfsd(8)</EM
>, <EM
>lockd(8)</EM
>, <EM
>statd(8)</EM
>, 
     <EM
>rquotad(8)</EM
>, and <EM
>mountd(8)</EM
>.
   </P
><P
>     It will also not cover PC-NFS, which is considered obsolete (users
     are encouraged to use Samba to share files with Windows machines) or NFS 
     Version 4, which is still in development.
   </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="KNOWPREREQ"
>2.3. Knowledge Pre-Requisites</A
></H3
><P
>     You should know some basic things about TCP/IP networking before 
     reading this HOWTO; if you are in doubt, read the 
     <A
HREF="http://www.linuxdoc.org/HOWTO/Networking-Overview-HOWTO.html"
TARGET="_top"
>Networking-
     Overview-HOWTO</A
>.
   </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SWPREREQ"
>2.4. Software Pre-Requisites: Kernel Version and nfs-utils</A
></H3
><P
>     The difference between Version 2 NFS and version 3 NFS will be 
     explained later on; for now, you might simply take the suggestion 
     that you will need NFS Version 3 if you are installing a dedicated
     or high-volume file server.  NFS Version 2 should be fine for 
     casual use.
   </P
><P
>     NFS Version 2 has been around for quite some time now (at least 
     since the 1.2 kernel series) however you will need a kernel version
     of at least 2.2.18 if you wish to do any of the following:
    <P
></P
><UL
><LI
><P
>Mix Linux NFS with other operating systems' NFS</P
></LI
><LI
><P
>Use file locking reliably over NFS</P
></LI
><LI
><P
>Use NFS Version 3.</P
></LI
></UL
>
   </P
><P
>     There are also patches available for kernel versions above 2.2.14 
     that provide the above functionality.  Some of them can be downloaded
     from the Linux NFS homepage.  If your kernel version is 2.2.14-
     2.2.17 and you have the source code on hand, you can tell if these 
     patches have been added because NFS Version 3 server support will be 
     a configuration option.  However, unless you have some particular
     reason to use an older kernel, you should upgrade because many bugs
     have been fixed along the way.  Kernel 2.2.19 contains some additional
     locking improvements over 2.2.18.
   </P
><P
>     Version 3 functionality will also require the nfs-utils package of 
     at least version 0.1.6, and mount version 2.10m or newer.  However 
     because nfs-utils and mount are fully backwards compatible, and because 
     newer versions have lots of security and bug fixes, there is no good 
     reason not to install the newest nfs-utils and mount packages if you 
     are beginning an NFS setup.
   </P
><P
>     All 2.4 and higher kernels have full NFS Version 3 functionality.
   </P
><P
>     In all cases, if you are building your own kernel, you will need
     to select NFS and NFS Version 3 support at compile time.  Most
     (but not all) standard distributions come with kernels that support
     NFS version 3.
   </P
><P
>     Handling files larger than 2 GB will require a 2.4x kernel and a 
     2.2.x version of <SPAN
CLASS="APPLICATION"
>glibc</SPAN
>.
   </P
><P
>     All kernels after 2.2.18 support NFS over TCP on the client side.
     As of this writing, server-side NFS over TCP only exists in a
     buggy form as an experimental option in the post-2.2.18 series;
     patches for 2.4 and 2.5 kernels have been introduced starting with
     2.4.17 and 2.5.6.  The patches are believed to be stable, though
     as of this writing they are relatively new and have not seen
     widespread use or integration into the mainstream 2.4 kernel.
   </P
><P
>     Because so many of the above functionalities were introduced in
     kernel version 2.2.18, this document was written to be consistent
     with kernels above this version (including 2.4.x).  If you have an
     older kernel, this document may not describe your NFS system
     correctly.
   </P
><P
>     As we write this document, NFS version 4 has only recently been
     finalized as a protocol, and no implementations are considered
     production-ready.  It will not be dealt with here.
   </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="FURTHERHELP"
>2.5. Where to get help and further information</A
></H3
><P
>    As of November 2000, the Linux NFS homepage is at 
    <A
HREF="http://nfs.sourceforge.net"
TARGET="_top"
>http://nfs.sourceforge.net</A
>.  Please check there for NFS related 
    mailing lists as well as the latest version of nfs-utils, NFS 
    kernel patches, and other NFS related packages.
   </P
><P
>When you encounter a problem or have a question not covered in this
manual, the faq or the man pages, you should send a message to the nfs
mailing list (<CODE
CLASS="EMAIL"
>&#60;<A
HREF="mailto:nfs@lists.sourceforge.net"
>nfs@lists.sourceforge.net</A
>&#62;</CODE
>). To best help the developers
and other users help you assess your problem you should include:</P
><P
></P
><UL
><LI
><P
> the version of <SPAN
CLASS="APPLICATION"
>nfs-utils</SPAN
> you are using</P
></LI
><LI
><P
> the version of the kernel and any non-stock applied kernels.</P
></LI
><LI
><P
> the distribution of linux you are using</P
></LI
><LI
><P
> the version(s) of other operating systems involved.</P
></LI
></UL
><P
>It is also useful to know the networking configuration connecting the
hosts.</P
><P
> 
If your problem involves the inability mount or export shares please
also include:</P
><P
></P
><UL
><LI
><P
> a copy of your <TT
CLASS="FILENAME"
>/etc/exports</TT
> file</P
></LI
><LI
><P
> the output of <B
CLASS="COMMAND"
>rpcinfo -p</B
> <EM
>localhost</EM
> run on the server</P
></LI
><LI
><P
> the output of <B
CLASS="COMMAND"
>rpcinfo -p</B
> <EM
>servername</EM
> run on the client</P
></LI
></UL
><P
>Sending all of this information with a specific question, after reading
all the documentation, is the best way to ensure a helpful response from
the list.</P
><P
>    You may also wish to look at the man pages for  <EM
>nfs(5)</EM
>, 
     <EM
>exports(5)</EM
>, <EM
>mount(8)</EM
>, <EM
>fstab(5)</EM
>, 
     <EM
>nfsd(8)</EM
>, <EM
>lockd(8)</EM
>, <EM
>statd(8)</EM
>, 
     <EM
>rquotad(8)</EM
>, and <EM
>mountd(8)</EM
>.
  </P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="SERVER"
>3. Setting Up an NFS Server</A
></H2
><DIV
CLASS="SECT2"
><H3
CLASS="SECT2"
><A
NAME="SERVERINTRO"
>3.1. Introduction to the server setup</A
></H3
><P
>    It is assumed that you will be setting up both a server and a
    client.  If you are just setting up a client to work off of
    somebody else's server (say in your department), you can skip
    to <A
HREF="#CLIENT"
>Section 4</A
>.  However, every client that is set up requires
    modifications on the server to authorize that client (unless 
    the server setup is done in a very insecure way), so even if you
    are not setting up a server you may wish to read this section to
    get an idea what kinds of authorization problems to look out for.
 </P
><P
>    Setting up the server will be done in two steps: Setting up the 
    configuration files for NFS, and then starting the NFS services.
 </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="CONFIG"
>3.2. Setting up the Configuration Files</A
></H3
><P
>   There are three main configuration files you will need to edit to 
   set up an NFS server: <TT
CLASS="FILENAME"
>/etc/exports</TT
>,
   <TT
CLASS="FILENAME"
>/etc/hosts.allow</TT
>, and  <TT
CLASS="FILENAME"
>/etc/hosts.deny</TT
>. 
   Strictly speaking, you only need to edit <TT
CLASS="FILENAME"
>/etc/exports</TT
> to get 
   NFS to work, but you would be left with an extremely insecure setup. You may also need 
   to edit your startup scripts; see <A
HREF="#DAEMONS"
>Section 3.3.3</A
> for more on that.
 </P
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="EXPORTS"
>3.2.1. /etc/exports</A
></H4
><P
>   This file contains a list of entries; each entry indicates a volume
   that is shared and how it is shared.  Check the man pages (<B
CLASS="COMMAND"
>man 
   exports</B
>) for a complete description of all the setup options for 
   the file, although the description here will probably satistfy 
   most people's needs.
 </P
><P
>   An entry in <TT
CLASS="FILENAME"
>/etc/exports</TT
> will typically look like this:
 <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> directory machine1(option11,option12) machine2(option21,option22)</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>  where 
  <DIV
CLASS="GLOSSLIST"
><DL
><DT
><B
>directory</B
></DT
><DD
><P
>    the directory that you want to share.  It may be an 
    entire volume though it need not be.  If you share a directory, 
    then all directories under it within the same file system will 
    be shared as well.
   </P
></DD
><DT
><B
>machine1 and machine2</B
></DT
><DD
><P
>    client machines that will have access to the directory. The machines
    may be listed by their DNS address or their IP address 
    (e.g., <EM
>machine.company.com</EM
> or <EM
>192.168.0.8</EM
>).
    Using IP addresses is more reliable and more secure.  If you need to
    use DNS addresses, and they do not seem to be resolving to the right
    machine, see <A
HREF="#SYMPTOM3"
>Section 7.3</A
>.
   </P
></DD
><DT
><B
>optionxx</B
></DT
><DD
><P
>    the option listing for each machine will describe what kind of 
    access that machine will have.  Important options are:
    <P
></P
><UL
><LI
><P
>       <KBD
CLASS="USERINPUT"
>ro</KBD
>:  The directory is shared read only; the client machine 
      will not be able to write to it.  This is the default.
    </P
></LI
><LI
><P
>       <KBD
CLASS="USERINPUT"
>rw</KBD
>:  The client machine will have read and write access to the 
      directory.
     </P
></LI
><LI
><P
>       <KBD
CLASS="USERINPUT"
>no_root_squash</KBD
>: By default,
       any file request made by user <SAMP
CLASS="COMPUTEROUTPUT"
>root</SAMP
> 
       on the client machine is treated as if it is made by user 
       <SAMP
CLASS="COMPUTEROUTPUT"
>nobody</SAMP
> on the 
       server.  (Excatly which UID the request is 
       mapped to depends on the UID of user "nobody" on the server,
       not the client.)  If <KBD
CLASS="USERINPUT"
>no_root_squash</KBD
> 
       is selected, then 
       root on the client machine will have the same level of access 
       to the files on the system as root on the server.  This 
       can have serious security implications, although it may be 
       necessary if you want to perform any administrative work on 
       the client machine that involves the exported directories.
       You should not specify this option without a good reason.
     </P
></LI
><LI
><P
>       <KBD
CLASS="USERINPUT"
>no_subtree_check</KBD
>: If only part of a volume is exported, a
       routine called subtree checking verifies that a file that is 
       requested from the client is in the appropriate part of the 
       volume.  If the entire volume is exported, disabling this check
       will speed up transfers.
     </P
></LI
><LI
><P
>       <KBD
CLASS="USERINPUT"
>sync</KBD
>:
       By default, all but the most recent version (version 1.11)
       of the <B
CLASS="COMMAND"
>exportfs</B
> command will use 
       <KBD
CLASS="USERINPUT"
>async</KBD
> behavior, telling a client 
       machine that a file write is complete - that is, has been written 
       to stable storage - when NFS has finished handing the write over to 
       the filesysytem.  This behavior may cause data corruption if the 
       server reboots, and the <KBD
CLASS="USERINPUT"
>sync</KBD
> option prevents 
       this.  See <A
HREF="#SYNC-ASYNC"
>Section 5.9</A
> for a complete discussion of 
       <KBD
CLASS="USERINPUT"
>sync</KBD
> and <KBD
CLASS="USERINPUT"
>async</KBD
> behavior.
    </P
></LI
></UL
>
  </P
></DD
></DL
></DIV
></P
><P
>  Suppose we have two client machines, <EM
>slave1</EM
> and <EM
>slave2</EM
>, that have IP
  addresses <EM
>192.168.0.1</EM
> and <EM
>192.168.0.2</EM
>, respectively.  We wish to share
  our software binaries and home directories with these machines.
  A typical setup for <TT
CLASS="FILENAME"
>/etc/exports</TT
> might look like this:
 <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>  /usr/local   192.168.0.1(ro) 192.168.0.2(ro)
  /home        192.168.0.1(rw) 192.168.0.2(rw)
 </PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>  Here we are sharing <TT
CLASS="FILENAME"
>/usr/local</TT
> read-only to slave1 and slave2, 
  because it probably contains our software and there may not be 
  benefits to allowing slave1 and slave2 to write to it that outweigh 
  security concerns.  On the other hand, home directories need to be 
  exported read-write if users are to save work on them.</P
><P
>  If you have a large installation, you may find that you have a bunch 
  of computers all on the same local network that require access to 
  your server.  There are a few ways of simplifying references
  to large numbers of machines.  First, you can give access to a range 
  of machines at once by specifying a network and a netmask.  For 
  example, if you wanted to allow access to all the machines with IP 
  addresses between <EM
>192.168.0.0</EM
> and
<EM
>192.168.0.255</EM
> then you could have the entries:
 <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>  /usr/local 192.168.0.0/255.255.255.0(ro)
  /home      192.168.0.0/255.255.255.0(rw)
 </PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>  See the <A
HREF="http://www.linuxdoc.org/HOWTO/Networking-Overview-HOWTO.html"
TARGET="_top"
>Networking-Overview HOWTO</A
>
  for further information about how netmasks work, and you may also wish to 
  look at the man pages for <TT
CLASS="FILENAME"
>init</TT
> and <TT
CLASS="FILENAME"
>hosts.allow</TT
>.</P
><P
>  Second, you can use NIS netgroups in your entry. To specify a
  netgroup in your exports file, simply prepend the name of the
  netgroup with an "@".  See the <A
HREF="http://www.linuxdoc.org/HOWTO/NIS-HOWTO.html"
TARGET="_top"
>NIS HOWTO</A
>
  for details on how netgroups work. </P
><P
>  Third, you can use wildcards such as <EM
>*.foo.com</EM
> or
 <EM
>192.168.</EM
> instead of hostnames.  There were problems
  with wildcard implementation in the 2.2 kernel series that were fixed
  in kernel 2.2.19.</P
><P
>  However, you should keep in mind that any of these simplifications
  could cause a security risk if there are machines in your netgroup
  or local network that you do not trust completely.</P
><P
>  A few cautions are in order about what cannot (or should not) be
  exported.  First, if a directory is exported, its parent and child
  directories cannot be exported if they are in the same filesystem.
  However, exporting both should not be necessary because listing the
  parent directory in the <TT
CLASS="FILENAME"
>/etc/exports</TT
> file will cause all underlying
  directories within that file system to be exported.  </P
><P
>  Second, it is a poor idea to export a FAT or VFAT (i.e., MS-DOS or 
  Windows 95/98) filesystem with NFS.  FAT is not designed for use on a 
  multi-user machine, and as a result, operations that depend on 
  permissions will not work well.  Moreover, some of the underlying 
  filesystem design is reported to work poorly with NFS's expectations.  </P
><P
>  Third, device or other special files may not export correctly to non-Linux 
  clients.  See <A
HREF="#INTEROP"
>Section 8</A
> for details on particular operating systems.</P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="HOSTS"
>3.2.2. /etc/hosts.allow and /etc/hosts.deny</A
></H4
><P
>   These two files specify which computers on the network can use 
   services on your machine.  Each line of the file 
   contains a single entry listing 
   a service and a set of machines.  When the server gets a request 
   from a machine, it does the following:
  <P
></P
><UL
><LI
><P
>     It first checks <TT
CLASS="FILENAME"
>hosts.allow</TT
> to see if the machine 
     matches a description listed in there. If it does, then the machine
     is allowed access.  
   </P
></LI
><LI
><P
>     If the machine does not match an entry in <TT
CLASS="FILENAME"
>hosts.allow</TT
>, the
     server then checks <TT
CLASS="FILENAME"
>hosts.deny</TT
> to see if the client matches a 
     listing in there.  If it does then the machine is denied  access.  
   </P
></LI
><LI
><P
>     If the client matches no listings in either file, then it
     is allowed access.
   </P
></LI
></UL
>
 </P
><P
>   In addition to controlling access to services 
   handled by <B
CLASS="COMMAND"
>inetd</B
> (such 
   as telnet and FTP), this file can also control access to NFS
   by restricting connections to the daemons that provide NFS services.
   Restrictions are done on a per-service basis.
 </P
><P
>   The first daemon to restrict access to is the portmapper.  This daemon
   essentially just tells requesting clients how to find all the NFS
   services on the system.  Restricting access to the portmapper is the
   best defense against someone breaking into your system through NFS
   because completely unauthorized clients won't know where to find the
   NFS daemons.  However, there are two things to watch out for.  First,
   restricting portmapper isn't enough if the intruder already knows
   for some reason how to find those daemons.  And second, if you are
   running NIS, restricting portmapper will also restrict requests to NIS.
   That should usually be harmless since you usually want
   to restrict NFS and NIS in a similar way, but just be cautioned.
   (Running NIS is generally a good idea if you are running NFS, because
   the client machines need a way of knowing who owns what files on the
   exported volumes.  Of course there are other ways of doing this such
   as syncing password files.  See the <A
HREF="http://www.linuxdoc.org/HOWTO/NIS-HOWTO.html"
TARGET="_top"
>NIS HOWTO</A
> for information on
   setting up NIS.)
 </P
><P
>   In general it is a good idea with NFS (as with most internet services)
   to explicitly deny access to IP addresses that you don't need
   to allow access to.
 </P
><P
>   The first step in doing this is to add the followng entry to 
   <TT
CLASS="FILENAME"
>/etc/hosts.deny</TT
>:
 </P
><P
>  <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>   portmap:ALL
  </PRE
></FONT
></TD
></TR
></TABLE
>
 </P
><P
>   Starting with <SPAN
CLASS="APPLICATION"
>nfs-utils</SPAN
> 0.2.0, you can be a bit more careful by
   controlling access to individual daemons.  It's a good precaution
   since an intruder will often be able to weasel around the portmapper.
   If you have a newer version of <SPAN
CLASS="APPLICATION"
>nfs-utils</SPAN
>, add entries for each of the
   NFS daemons (see the next section to find out what these daemons are;
   for now just put entries for them in hosts.deny):
 </P
><P
>  <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>    lockd:ALL
    mountd:ALL
    rquotad:ALL
    statd:ALL
  </PRE
></FONT
></TD
></TR
></TABLE
>
 </P
><P
> 
   Even if you have an older version of <SPAN
CLASS="APPLICATION"
>nfs-utils</SPAN
>, adding these entries
   is at worst harmless (since they will just be ignored) and at best 
   will save you some trouble when you upgrade.  Some sys admins choose
   to put the entry <KBD
CLASS="USERINPUT"
>ALL:ALL</KBD
> in the file <TT
CLASS="FILENAME"
>/etc/hosts.deny</TT
>, 
   which causes  any service that looks at these files to deny access to all 
   hosts unless it is explicitly allowed.  While this is more secure 
   behavior, it may also get you in trouble when you are installing new 
   services,  you forget you put it there, and you can't figure out for 
   the life of you why they won't work.
 </P
><P
>   Next, we need to add an entry to <TT
CLASS="FILENAME"
>hosts.allow</TT
> to give any hosts 
   access that we want to have access.  (If we just leave the above 
   lines in <TT
CLASS="FILENAME"
>hosts.deny</TT
> then nobody will have access to NFS.)  Entries 
   in <TT
CLASS="FILENAME"
>hosts.allow</TT
> follow the format
  <DIV
CLASS="INFORMALEXAMPLE"
><P
></P
><A
NAME="AEN282"
></A
><TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>    service: host [or network/netmask] , host [or network/netmask]
   </PRE
></FONT
></TD
></TR
></TABLE
><P
></P
></DIV
>
 </P
><P
>   Here, host is IP address of a potential client; it may be possible 
   in some versions to use the DNS name of the host, but it is strongly 
   discouraged.
 </P
><P
>   Suppose we have the setup above and we just want to allow access
   to <EM
>slave1.foo.com</EM
> and <EM
>slave2.foo.com</EM
>, 
   and suppose that the IP addresses of these machines are <EM
>192.168.0.1</EM
>
   and <EM
>192.168.0.2</EM
>, respectively.  We could add the following entry to 
   <TT
CLASS="FILENAME"
>/etc/hosts.allow</TT
>:
  <DIV
CLASS="INFORMALEXAMPLE"
><P
></P
><A
NAME="AEN291"
></A
><TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>   portmap: 192.168.0.1 , 192.168.0.2
   </PRE
></FONT
></TD
></TR
></TABLE
><P
></P
></DIV
>
 </P
><P
>   For recent nfs-utils versions, we would also add the following
   (again, these entries are harmless even if they are not supported):
  <DIV
CLASS="INFORMALEXAMPLE"
><P
></P
><A
NAME="AEN294"
></A
><TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>    lockd: 192.168.0.1 , 192.168.0.2
    rquotad: 192.168.0.1 , 192.168.0.2
    mountd: 192.168.0.1 , 192.168.0.2
    statd: 192.168.0.1 , 192.168.0.2
   </PRE
></FONT
></TD
></TR
></TABLE
><P
></P
></DIV
>
 </P
><P
>   If you intend to run NFS on a large number of machines in a local
   network, <TT
CLASS="FILENAME"
>/etc/hosts.allow</TT
> also allows for network/netmask style 
   entries in the same manner as <TT
CLASS="FILENAME"
>/etc/exports</TT
> above.
 </P
></DIV
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SERVICESTART"
>3.3. Getting the services started</A
></H3
><DIV
CLASS="SECT3"
><H4
CLASS="SECT3"
><A
NAME="PREREQ"
>3.3.1. Pre-requisites</A
></H4
><P
>    The NFS server should now be configured and we can start it running.
    First, you will need to have the appropriate packages installed.  
    This consists mainly of a new enough kernel and a new enough version
    of the <SPAN
CLASS="APPLICATION"
>nfs-utils</SPAN
> package.  
    See <A
HREF="#SWPREREQ"
>Section 2.4</A
> if you are in doubt.
  </P
><P
>    Next, before you can start NFS, you will need to have TCP/IP 
    networking functioning correctly on your machine.  If you can use 
    telnet, FTP, and so on, then chances are your TCP networking is fine.
  </P
><P
>    That said, with most recent Linux distributions you may be able to 
    get NFS up and running simply by rebooting your machine, and the 
    startup scripts should detect that you have set up your <TT
CLASS="FILENAME"
>/etc/exports</TT
>
    file and will start up NFS correctly.  If you try this, see <A
HREF="#VERIFY"
>Section 3.4</A
>
    Verifying that NFS is running.  If this does not work, or if 
    you are not in a position to reboot your machine, then the following
    section will tell you which daemons need to be started in order to 
    run NFS services.  If for some reason <B
CLASS="COMMAND"
>nfsd</B
> 
    was already running when
    you edited your configuration files above, you will have to flush 
    your configuration; see <A
HREF="#LATER"
>Section 3.5</A
> for details.
  </P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="PORTMAPPER"
>3.3.2. Starting the Portmapper</A
></H4
><P
>     NFS depends on the portmapper daemon, either called <B
CLASS="COMMAND"
>portmap</B
> or 
     <B
CLASS="COMMAND"
>rpc.portmap</B
>.  It will need to be started first.  It should be 
     located in <TT
CLASS="FILENAME"
>/sbin</TT
> but is sometimes in <TT
CLASS="FILENAME"
>/usr/sbin</TT
>.  
     Most recent Linux distributions start this daemon in the boot scripts, but it is 
     worth making sure that it is running before you begin working with 
     NFS (just type <B
CLASS="COMMAND"
>ps aux | grep portmap</B
>).
   </P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="DAEMONS"
>3.3.3. The Daemons</A
></H4
><P
>     NFS serving is taken care of by five daemons: <B
CLASS="COMMAND"
>rpc.nfsd</B
>, 
     which does most of the work; <B
CLASS="COMMAND"
>rpc.lockd</B
> and 
     <B
CLASS="COMMAND"
>rpc.statd</B
>, which handle file locking;
     <B
CLASS="COMMAND"
>rpc.mountd</B
>, which handles the initial mount requests, 
     and <B
CLASS="COMMAND"
>rpc.rquotad</B
>, which handles user file quotas on 
     exported volumes. Starting with 2.2.18, <B
CLASS="COMMAND"
>lockd</B
> 
     is called by <B
CLASS="COMMAND"
>nfsd</B
> upon demand, so you do
     not need to worry about starting it yourself. <B
CLASS="COMMAND"
>statd</B
> 
     will need to be started separately. Most recent Linux distributions will
     have startup scripts for these daemons.
   </P
><P
>     The daemons are all part of the nfs-utils package, and may be either
     in the <TT
CLASS="FILENAME"
>/sbin</TT
> directory or the <TT
CLASS="FILENAME"
>/usr/sbin</TT
> directory.
   </P
><P
>    If your distribution does not include them in the startup scripts, 
    then you should add them, configured to start in the following 
    order:
  <P
></P
><TABLE
BORDER="0"
><TBODY
><TR
><TD
><B
CLASS="COMMAND"
>rpc.portmap</B
></TD
></TR
><TR
><TD
><B
CLASS="COMMAND"
>rpc.mountd</B
>, <B
CLASS="COMMAND"
>rpc.nfsd</B
></TD
></TR
><TR
><TD
><B
CLASS="COMMAND"
>rpc.statd</B
>, <B
CLASS="COMMAND"
>rpc.lockd</B
> (if necessary), and <B
CLASS="COMMAND"
>rpc.rquotad</B
></TD
></TR
></TBODY
></TABLE
><P
></P
>
 </P
><P
>   The nfs-utils package has sample startup scripts for RedHat and
   Debian.  If you are using a different distribution, in general you
   can just copy the RedHat script, but you will probably have to take
   out the line that says:
   <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>    . ../init.d/functions
   </PRE
></FONT
></TD
></TR
></TABLE
>
   to avoid getting error messages.
 </P
></DIV
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="VERIFY"
>3.4. Verifying that NFS is running</A
></H3
><P
>    To do this, query the portmapper with the command <B
CLASS="COMMAND"
>rpcinfo -p</B
> to 
    find out what services it is providing.  You should get something 
    like this:
   <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>    program vers proto   port
    100000    2   tcp    111  portmapper
    100000    2   udp    111  portmapper
    100011    1   udp    749  rquotad
    100011    2   udp    749  rquotad
    100005    1   udp    759  mountd
    100005    1   tcp    761  mountd
    100005    2   udp    764  mountd
    100005    2   tcp    766  mountd
    100005    3   udp    769  mountd
    100005    3   tcp    771  mountd
    100003    2   udp   2049  nfs
    100003    3   udp   2049  nfs
    300019    1   tcp    830  amd
    300019    1   udp    831  amd
    100024    1   udp    944  status
    100024    1   tcp    946  status
    100021    1   udp   1042  nlockmgr
    100021    3   udp   1042  nlockmgr
    100021    4   udp   1042  nlockmgr
    100021    1   tcp   1629  nlockmgr
    100021    3   tcp   1629  nlockmgr
    100021    4   tcp   1629  nlockmgr
  </PRE
></FONT
></TD
></TR
></TABLE
>
 </P
><P
>   This says that we have NFS versions 2 and 3, rpc.statd version 1, 
   network lock manager (the service name for rpc.lockd) versions 1, 3, 
   and 4.  There are also different service listings depending on 
   whether NFS is travelling over TCP or UDP.  Linux systems use UDP
   by default unless TCP is explicitly requested; however other OSes 
   such as Solaris default to TCP.
 </P
><P
>   If you do not at least see a line that says 
   <SAMP
CLASS="COMPUTEROUTPUT"
>portmapper</SAMP
>, a line that says 
   <SAMP
CLASS="COMPUTEROUTPUT"
>nfs</SAMP
>, and a line that says 
   <SAMP
CLASS="COMPUTEROUTPUT"
>mountd</SAMP
> then you will need 
   to backtrack and try again to start up the daemons 
   (see <A
HREF="#TROUBLESHOOTING"
>Section 7</A
>, 
   Troubleshooting, if this still doesn't work).
 </P
><P
>   If you do see these services listed, then you should be ready to 
   set up NFS clients to access files from your server.
 </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="LATER"
>3.5. Making changes to /etc/exports later on</A
></H3
><P
>   If you come back and change your <TT
CLASS="FILENAME"
>/etc/exports</TT
> file, the changes you 
   make may not take effect immediately.  You should run the command 
   <B
CLASS="COMMAND"
>exportfs -ra</B
> to force <B
CLASS="COMMAND"
>nfsd</B
> to re-read the <TT
CLASS="FILENAME"
>/etc/exports</TT
>
   file.  If you can't find the <B
CLASS="COMMAND"
>exportfs</B
> command, then you can kill <B
CLASS="COMMAND"
>nfsd</B
> with the 
  <KBD
CLASS="USERINPUT"
> -HUP</KBD
> flag (see the man pages for kill for details).
 </P
><P
>   If that still doesn't work, don't forget to check <TT
CLASS="FILENAME"
>hosts.allow</TT
> to 
   make sure you haven't forgotten to list any new client machines 
   there.  Also check the host listings on any firewalls you may have 
   set up (see <A
HREF="#TROUBLESHOOTING"
>Section 7</A
> and 
   <A
HREF="#SECURITY"
>Section 6</A
> for more details on firewalls and NFS).
 </P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="CLIENT"
>4. Setting up an NFS Client</A
></H2
><DIV
CLASS="SECT2"
><H3
CLASS="SECT2"
><A
NAME="REMOTEMOUNT"
>4.1. Mounting remote directories</A
></H3
><P
>    Before beginning, you should double-check to make sure your mount 
    program is new enough (version 2.10m if you want to use Version 3 
    NFS), and that the client machine supports NFS mounting, though most
    standard distributions do.  If you are using a 2.2 or later kernel 
    with the <TT
CLASS="FILENAME"
>/proc</TT
> filesystem you can check the latter by reading the 
    file <TT
CLASS="FILENAME"
>/proc/filesystems</TT
> and making sure there is a line containing 
    nfs.  If not, typing <KBD
CLASS="USERINPUT"
>insmod nfs</KBD
> may make it
    magically appear if NFS has been compiled as a module; otherwise,
    you will need to build (or download) a kernel that has 
    NFS support built in.  In general, kernels that do not have NFS
    compiled in will give a very specific error when the 
    <B
CLASS="COMMAND"
>mount</B
> command below is run.
  </P
><P
>    To begin using machine as an NFS client, you will need the portmapper 
    running on that machine, and to use NFS file locking, you will
    also need <B
CLASS="COMMAND"
>rpc.statd</B
> and <B
CLASS="COMMAND"
>rpc.lockd</B
>
    running on both the client and the server.  Most recent distributions 
    start those services by default at boot time; if yours doesn't, see 
    <A
HREF="#CONFIG"
>Section 3.2</A
> for information on how to start them up.
  </P
><P
>    With <B
CLASS="COMMAND"
>portmap</B
>, <B
CLASS="COMMAND"
>lockd</B
>, 
    and <B
CLASS="COMMAND"
>statd</B
> running, you should now be able to 
    mount the remote directory from your server just the way you mount 
    a local hard drive, with the mount command.  Continuing our example 
    from the previous section, suppose our server above is called 
    <EM
>master.foo.com</EM
>,and we want to mount the <TT
CLASS="FILENAME"
>/home</TT
> directory on 
    <EM
>slave1.foo.com</EM
>.  Then, all we have to do, from the root prompt on 
    <EM
>slave1.foo.com</EM
>, is type:
  <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>   # mount master.foo.com:/home /mnt/home
  </PRE
></FONT
></TD
></TR
></TABLE
>
  and the directory <TT
CLASS="FILENAME"
>/home</TT
> on master will appear as the directory 
  <TT
CLASS="FILENAME"
>/mnt/home</TT
> on <EM
>slave1</EM
>.  (Note that
  this assumes we have created the directory <TT
CLASS="FILENAME"
>/mnt/home</TT
> 
  as an empty mount point beforehand.)
 </P
><P
>   If this does not work, see the Troubleshooting section (<A
HREF="#TROUBLESHOOTING"
>Section 7</A
>).
 </P
><P
>   You can get rid of the file system by typing 
  <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>   # umount /mnt/home 
  </PRE
></FONT
></TD
></TR
></TABLE
>
 just like you would for a local file system.
 </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="BOOT-TIME-NFS"
>4.2. Getting NFS File Systems to Be Mounted at Boot Time</A
></H3
><P
>   NFS file systems can be added to your <TT
CLASS="FILENAME"
>/etc/fstab</TT
> file the same way
   local file systems can, so that they mount when your system starts
   up.  The only difference is that the file system type will be
   set to <KBD
CLASS="USERINPUT"
>nfs</KBD
> and the dump and fsck order (the last two entries) will
   have to be set to zero.  So for our example above, the entry in
   <TT
CLASS="FILENAME"
>/etc/fstab</TT
> would look like:
  <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>   # device       mountpoint     fs-type     options      dump fsckorder
   ...
   master.foo.com:/home  /mnt    nfs          rw            0    0
   ...
  </PRE
></FONT
></TD
></TR
></TABLE
>
 </P
><P
>   See the man pages for <TT
CLASS="FILENAME"
>fstab</TT
> if you are unfamiliar 
   with the syntax of this file.  If you are using an automounter such as 
   amd or autofs, the options in the corresponding fields of your mount 
   listings should look very similar if not identical.
 </P
><P
>   At this point you should have NFS working, though a few tweaks
   may still be necessary to get it to work well.  You should also
   read <A
HREF="#SECURITY"
>Section 6</A
> to be sure your setup is reasonably secure.
 </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="MOUNTOPTIONS"
>4.3. Mount options</A
></H3
><DIV
CLASS="SECT3"
><H4
CLASS="SECT3"
><A
NAME="SOFT-VS-HARD"
>4.3.1. Soft vs. Hard Mounting</A
></H4
><P
>     There are some options you should consider adding at once.  They
     govern the way the NFS client handles a server crash or network
     outage.  One of the cool things about NFS is that it can handle this
     gracefully.  If you set up the clients right.  There are two distinct
     failure modes:
   </P
><P
>  <DIV
CLASS="GLOSSLIST"
><DL
><DT
><B
>soft</B
></DT
><DD
><P
>   If a file request fails, the NFS client will report an 
   error to the process on the client machine requesting the file 
   access.  Some programs can handle this with composure, most 
   won't.  We do not recommend using this setting; it is a recipe 
   for corrupted files and lost data.  You should especially not 
   use this for mail disks --- if you value your mail, that is.
   </P
></DD
><DT
><B
>hard</B
></DT
><DD
><P
>     The program accessing a file on a NFS mounted file system
     will hang when the server crashes.  The process cannot be 
     interrupted or killed (except by a "sure kill") unless you also 
     specify <KBD
CLASS="USERINPUT"
>intr</KBD
>.  When the 
     NFS server is back online the program will
     continue undisturbed from where it was.  We recommend using 
     <KBD
CLASS="USERINPUT"
>hard,intr</KBD
> on all NFS mounted file systems.
    </P
></DD
></DL
></DIV
>
  </P
><P
>  Picking up the from previous example, the fstab entry would now 
  look like:
  <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>   # device             mountpoint  fs-type    options    dump fsckord
   ...
   master.foo.com:/home  /mnt/home   nfs      rw,hard,intr  0     0
   ...
  </PRE
></FONT
></TD
></TR
></TABLE
>
 </P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="BLOCKSIZE"
>4.3.2. Setting Block Size to Optimize Transfer Speeds</A
></H4
><P
>   The <KBD
CLASS="USERINPUT"
>rsize</KBD
> and <KBD
CLASS="USERINPUT"
>wsize</KBD
> mount 
   options specify the size of the chunks of data that the client and 
   server pass back and forth to each other.
 </P
><P
>  The defaults may be too big or to small; there is no size that works
  well on all or most setups.  On the one hand, some combinations of 
  Linux kernels and network cards (largely on older machines) cannot
  handle blocks that large. On the other hand, if they can handle 
  larger blocks, a bigger size might be faster.
 </P
><P
>   Getting the block size right is an important factor in performance and
   is a must if you are planning to use the NFS server in a production
   environment.  See <A
HREF="#PERFORMANCE"
>Section 5</A
> for details.
 </P
></DIV
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="PERFORMANCE"
>5. Optimizing NFS Performance</A
></H2
><P
>Careful analysis of your environment, both from the client and from the server
point of view, is the first step necessary for optimal NFS performance. The
first sections will address issues that are generally important to the client.
Later (<A
HREF="#FRAG-OVERFLOW"
>Section 5.3</A
> and beyond), server side issues 
will be discussed. In both
cases, these issues will not be limited exclusively to one side or the other,
but it is useful to separate the two in order to get a clearer picture of
cause and effect.</P
><P
>Aside from the general network configuration - appropriate network capacity,
faster NICs, full duplex settings in order to reduce collisions, agreement in
network speed among the switches and hubs, etc. - one of the most important
client optimization settings are the NFS data transfer buffer sizes, specified
by the <B
CLASS="COMMAND"
>mount</B
> command options <KBD
CLASS="USERINPUT"
>rsize</KBD
>
and <KBD
CLASS="USERINPUT"
>wsize</KBD
>.</P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="BLOCKSIZES"
>5.1. Setting Block Size to Optimize Transfer Speeds</A
></H3
><P
>The <B
CLASS="COMMAND"
>mount</B
> command options <KBD
CLASS="USERINPUT"
>rsize</KBD
> 
and <KBD
CLASS="USERINPUT"
>wsize</KBD
> specify the size of the chunks of
data that the client and server pass back and forth 
to each other. If no <KBD
CLASS="USERINPUT"
>rsize</KBD
>
and <KBD
CLASS="USERINPUT"
>wsize</KBD
> options are specified, 
the default varies by which version of NFS we
are using. The most common default is 4K (4096 bytes), although for TCP-based
mounts in 2.2 kernels, and for all mounts beginning with 2.4 kernels, the
server specifies the default block size.</P
><P
>The theoretical limit for the NFS V2 protocol is 8K. For the V3 protocol, the
limit is specific to the server. On the Linux server, the maximum block size
is defined by the value of the kernel constant 
<KBD
CLASS="USERINPUT"
>NFSSVC_MAXBLKSIZE</KBD
>, found in the
Linux kernel source file <TT
CLASS="FILENAME"
>./include/linux/nfsd/const.h</TT
>.
The current maximum block size for the kernel, as of 2.4.17, is 8K (8192 bytes),
but the patch set implementing NFS over TCP/IP transport in the 2.4 
series, as of this writing, uses a value of 32K (defined in the
patch as 32*1024) for the maximum block size.</P
><P
>All 2.4 clients currently support up to 32K block transfer sizes, allowing the
standard 32K block transfers across NFS mounts from other servers, such as
Solaris, without client modification.</P
><P
>The defaults may be too big or too small, depending on the specific
combination of hardware and kernels. On the one hand, some combinations of
Linux kernels and network cards (largely on older machines) cannot handle
blocks that large. On the other hand, if they can handle larger blocks, a
bigger size might be faster.</P
><P
>You will want to experiment and find an <KBD
CLASS="USERINPUT"
>rsize</KBD
>
and <KBD
CLASS="USERINPUT"
>wsize</KBD
> that works and is as
fast as possible. You can test the speed of your options with some simple
commands, if your network environment is not heavily used. Note that your
results may vary widely unless you resort to using more complex benchmarks,
such as <SPAN
CLASS="APPLICATION"
>Bonnie</SPAN
>, <SPAN
CLASS="APPLICATION"
>Bonnie++</SPAN
>,
or <SPAN
CLASS="APPLICATION"
>IOzone</SPAN
>.</P
><P
>The first of these commands transfers 16384 blocks of 16k each from the
special file <TT
CLASS="FILENAME"
>/dev/zero</TT
> (which if 
you read it just spits out zeros <EM
>really</EM
>
fast) to the mounted partition. We will time it to see how long it takes. So,
from the client machine, type:</P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>    # time dd if=/dev/zero of=/mnt/home/testfile bs=16k count=16384</PRE
></FONT
></TD
></TR
></TABLE
><P
>This creates a 256Mb file of zeroed bytes. In general, you should create a
file that's at least twice as large as the system RAM on the server, but make
sure you have enough disk space! Then read back the file into the great black
hole on the client machine (<TT
CLASS="FILENAME"
>/dev/null</TT
>) by 
typing the following:</P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>    # time dd if=/mnt/home/testfile of=/dev/null bs=16k</PRE
></FONT
></TD
></TR
></TABLE
><P
>Repeat this a few times and average how long it takes. Be sure to unmount and
remount the filesystem each time (both on the client and, if you are zealous,
locally on the server as well), which should clear out any caches.</P
><P
>Then unmount, and mount again with a larger and smaller block size. They
should be multiples of 1024, and not larger than the maximum block size
allowed by your system. Note that NFS Version 2 is limited to a maximum of 8K,
regardless of the maximum block size defined by 
<KBD
CLASS="USERINPUT"
>NFSSVC_MAXBLKSIZE</KBD
>; Version 3
will support up to 64K, if permitted. The block size should be a power of two
since most of the parameters that would constrain it (such as file system
block sizes and network packet size) are also powers of two. However, some
users have reported better successes with block sizes that are not powers of
two but are still multiples of the file system block size and the network
packet size.</P
><P
>Directly after mounting with a larger size, cd into the mounted
file system and do things like <B
CLASS="COMMAND"
>ls</B
>, explore 
the filesystem a bit to make sure everything is as it
should. If the <KBD
CLASS="USERINPUT"
>rsize</KBD
>/<KBD
CLASS="USERINPUT"
>wsize</KBD
>
 is too large the symptoms are very odd and not 100%
obvious. A typical symptom is incomplete file lists when doing 
<B
CLASS="COMMAND"
>ls</B
>, and no
error messages, or reading files failing mysteriously with no error messages.
After establishing that the given <KBD
CLASS="USERINPUT"
>rsize</KBD
>/
<KBD
CLASS="USERINPUT"
>wsize</KBD
> works you can do the speed tests
again. Different server platforms are likely to have different optimal sizes.</P
><P
>Remember to edit <TT
CLASS="FILENAME"
>/etc/fstab</TT
> to reflect the 
<KBD
CLASS="USERINPUT"
>rsize</KBD
>/<KBD
CLASS="USERINPUT"
>wsize</KBD
> you found
to be the most desirable.</P
><P
>If your results seem inconsistent, or doubtful, you may need to analyze your
network more extensively while varying the <KBD
CLASS="USERINPUT"
>rsize</KBD
>
and <KBD
CLASS="USERINPUT"
>wsize</KBD
> values. In that
case, here are several pointers to benchmarks that may prove useful:</P
><P
></P
><UL
><LI
><P
>     Bonnie                            <A
HREF="http://www.textuality.com/bonnie/"
TARGET="_top"
>http://www.textuality.com/bonnie/</A
></P
></LI
><LI
><P
>     Bonnie++                          <A
HREF="http://www.coker.com.au/bonnie++/"
TARGET="_top"
>http://www.coker.com.au/bonnie++/</A
></P
></LI
><LI
><P
>     IOzone file system benchmark      <A
HREF="http://www.iozone.org/"
TARGET="_top"
>http://www.iozone.org/</A
></P
></LI
><LI
><P
>     The official NFS benchmark,
     SPECsfs97                         <A
HREF="http://www.spec.org/osg/sfs97/"
TARGET="_top"
>http://www.spec.org/osg/sfs97/</A
></P
></LI
></UL
><P
>The easiest benchmark with the widest coverage, including an extensive spread
of file sizes, and of IO types - reads, &#38; writes, rereads &#38; rewrites, random
access, etc. - seems to be IOzone. A recommended invocation of IOzone (for
which you must have root privileges) includes unmounting and remounting the
directory under test, in order to clear out the caches between tests, and
including the file close time in the measurements. Assuming you've already
exported <TT
CLASS="FILENAME"
>/tmp</TT
> to everyone from the server
<SAMP
CLASS="COMPUTEROUTPUT"
>foo</SAMP
>, 
and that you've installed IOzone in the local directory, this should work:</P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>    # echo "foo:/tmp /mnt/foo nfs rw,hard,intr,rsize=8192,wsize=8192 0 0"
    &#62;&#62; /etc/fstab
    # mkdir /mnt/foo
    # mount /mnt/foo
    # ./iozone -a -R -c -U /mnt/foo -f /mnt/foo/testfile &#62; logfile</PRE
></FONT
></TD
></TR
></TABLE
><P
>The benchmark should take 2-3 hours at most, but of course you will need to
run it for each value of rsize and wsize that is of interest. The web site
gives full documentation of the parameters, but the specific options used
above are:</P
><P
></P
><UL
><LI
><P
>     <KBD
CLASS="USERINPUT"
>-a</KBD
> Full automatic mode, which tests file sizes of 64K to 512M, using
     record sizes of 4K to 16M</P
></LI
><LI
><P
>     <KBD
CLASS="USERINPUT"
>-R</KBD
> Generate report in excel spreadsheet form (The "surface plot"
     option for graphs is best)</P
></LI
><LI
><P
>     <KBD
CLASS="USERINPUT"
>-c</KBD
> Include the file close time in the tests, which will pick up the
     NFS version 3 commit time</P
></LI
><LI
><P
>     <KBD
CLASS="USERINPUT"
>-U</KBD
> Use the given mount point to unmount and remount between tests;
     it clears out caches</P
></LI
><LI
><P
>     <KBD
CLASS="USERINPUT"
>-f</KBD
> When using unmount, you have to locate the test file in the
     mounted file system</P
></LI
></UL
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="PACKET-AND-NETWORK"
>5.2. Packet Size and Network Drivers</A
></H3
><P
>  While many Linux network card drivers are excellent, some are quite shoddy,
  including a few drivers for some fairly standard cards.  It is worth
  experimenting with your network card directly to find out how it can
  best handle traffic.</P
><P
>   Try <B
CLASS="COMMAND"
>ping</B
>ing back and forth 
   between the two machines with large packets using
   the <KBD
CLASS="USERINPUT"
>-f</KBD
> and <KBD
CLASS="USERINPUT"
>-s</KBD
>
   options with <B
CLASS="COMMAND"
>ping</B
> (see <EM
>ping(8)</EM
> 
   for more details) and see if a
   lot of packets get dropped, or if they take a long time for a reply. If so,
   you may have a problem with the performance of your network card.</P
><P
>For a more extensive analysis of NFS behavior in particular, use the <B
CLASS="COMMAND"
>nfsstat</B
> command to look at nfs transactions, client and server statistics, network
statistics, and so forth. The <KBD
CLASS="USERINPUT"
>"-o net"</KBD
> option will show you the number of
dropped packets in relation to the total number of transactions. In UDP
transactions, the most important statistic is the number of retransmissions,
due to dropped packets, socket buffer overflows, general server congestion,
timeouts, etc. This will have a tremendously important effect on NFS
performance, and should be carefully monitored. 
Note that <B
CLASS="COMMAND"
>nfsstat</B
> does not yet
implement the <KBD
CLASS="USERINPUT"
>-z</KBD
> option, which would zero out all counters, so you must look
at the current <B
CLASS="COMMAND"
>nfsstat</B
> counter values prior to running the benchmarks.</P
><P
>To correct network problems, you may wish to reconfigure the packet size that
your network card uses. Very often there is a constraint somewhere else in the
network (such as a router) that causes a smaller maximum packet size between
two machines than what the network cards on the machines are actually capable
of. TCP should autodiscover the appropriate packet size for a network, but UDP
will simply stay at a default value. So determining the appropriate packet
size is especially important if you are using NFS over UDP.</P
><P
>You can test for the network packet size using the <B
CLASS="COMMAND"
>tracepath</B
> command: From
the client machine, just type <KBD
CLASS="USERINPUT"
>tracepath</KBD
> 
<EM
>server</EM
> <KBD
CLASS="USERINPUT"
>2049</KBD
> and the path MTU should
be reported at the bottom. You can then set the MTU on your network card equal
to the path MTU, by using the <KBD
CLASS="USERINPUT"
>MTU</KBD
> 
option to <B
CLASS="COMMAND"
>ifconfig</B
>, and see if fewer packets
get dropped. See the <B
CLASS="COMMAND"
>ifconfig</B
> man pages for details on how to reset the MTU.</P
><P
>In addition, <B
CLASS="COMMAND"
>netstat -s</B
> will give the statistics collected for traffic across
all supported protocols. You may also look at 
<TT
CLASS="FILENAME"
>/proc/net/snmp</TT
> for information
about current network behavior; see the next section for more details.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="FRAG-OVERFLOW"
>5.3. Overflow of Fragmented Packets</A
></H3
><P
>Using an <KBD
CLASS="USERINPUT"
>rsize</KBD
> or <KBD
CLASS="USERINPUT"
>wsize</KBD
> 
larger than your network's MTU (often set to 1500, in
many networks) will cause IP packet fragmentation when using NFS over UDP. IP
packet fragmentation and reassembly require a significant amount of CPU
resource at both ends of a network connection. In addition, packet
fragmentation also exposes your network traffic to greater unreliability,
since a complete RPC request must be retransmitted if a UDP packet fragment is
dropped for any reason. Any increase of RPC retransmissions, along with the
possibility of increased timeouts, are the single worst impediment to
performance for NFS over UDP.</P
><P
>Packets may be dropped for many reasons. If your network topography is
complex, fragment routes may differ, and may not all arrive at the Server for
reassembly. NFS Server capacity may also be an issue, since the kernel has a
limit of how many fragments it can buffer before it starts throwing away
packets. With kernels that support the <TT
CLASS="FILENAME"
>/proc</TT
> 
filesystem, you can monitor the
files <TT
CLASS="FILENAME"
>/proc/sys/net/ipv4/ipfrag_high_thresh</TT
> and
<TT
CLASS="FILENAME"
>/proc/sys/net/ipv4/ipfrag_low_thresh</TT
>. Once the number of unprocessed,
fragmented packets reaches the number specified by <TT
CLASS="FILENAME"
>ipfrag_high_thresh</TT
> (in
bytes), the kernel will simply start throwing away fragmented packets until
the number of incomplete packets reaches the number specified by
<TT
CLASS="FILENAME"
>ipfrag_low_thresh.</TT
></P
><P
>Another counter to monitor is <KBD
CLASS="USERINPUT"
>IP: ReasmFails</KBD
> 
in the file <TT
CLASS="FILENAME"
>/proc/net/snmp</TT
>; this
is the number of fragment reassembly failures. if it goes up too quickly
during heavy file activity, you may have problem.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="NFS-TCP"
>5.4. NFS over TCP</A
></H3
><P
>A new feature, available for both 2.4 and 2.5 kernels but not yet
integrated into the mainstream kernel at the time of 
this writing, is NFS over TCP. Using TCP
has a distinct advantage and a distinct disadvantage over UDP.  The advantage
is that it works far better than UDP on lossy networks.
When using TCP, a single dropped packet can be retransmitted, without
the retransmission of the entire RPC request, resulting in better performance
on lossy networks. In addition, TCP will handle network speed differences
better than UDP, due to the underlying flow control at the network level. </P
><P
>The disadvantage of using TCP is that it is not a stateless protocol like
UDP.  If your server crashes in the middle of a packet transmission,
the client will hang and any shares will need to be unmounted and remounted.</P
><P
>The overhead incurred by the TCP protocol will result in
somewhat slower performance than UDP under ideal network
conditions, but the cost is not severe, and is often not
noticable without careful measurement. If you are using 
gigabit ethernet from end to end, you might also investigate the
usage of jumbo frames, since the high speed network may
allow the larger frame sizes without encountering increased 
collision rates, particularly if you have set the network 
to full duplex.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="TIMEOUT"
>5.5. Timeout and Retransmission Values</A
></H3
><P
>Two mount command options, <KBD
CLASS="USERINPUT"
>timeo</KBD
> 
and <KBD
CLASS="USERINPUT"
>retrans</KBD
>, control the behavior of UDP
requests when encountering client timeouts due to dropped packets, network
congestion, and so forth. The <KBD
CLASS="USERINPUT"
>-o timeo</KBD
> 
option allows designation of the length
of time, in tenths of seconds, that the client will wait until it decides it
will not get a reply from the server, and must try to send the request again.
The default value is 7 tenths of a second. The 
<KBD
CLASS="USERINPUT"
>-o retrans</KBD
> option allows
designation of the number of timeouts allowed before the client gives up, and
displays the <SAMP
CLASS="COMPUTEROUTPUT"
>Server not responding</SAMP
> 
message. The default value is 3 attempts.
Once the client displays this message, it will continue to try to send
the request, but only once before displaying the error message if
another timeout occurs.  When the client reestablishes contact, it 
will fall back to using the correct <KBD
CLASS="USERINPUT"
>retrans</KBD
> 
value, and will display the <SAMP
CLASS="COMPUTEROUTPUT"
>Server OK</SAMP
> message.</P
><P
>If you are already encountering excessive retransmissions (see the output of
the <B
CLASS="COMMAND"
>nfsstat</B
> command), or want to increase the block transfer size without
encountering timeouts and retransmissions, you may want to adjust these
values. The specific adjustment will depend upon your environment, and in most
cases, the current defaults are appropriate.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="NFSD-INSTANCE"
>5.6. Number of Instances of the NFSD Server Daemon</A
></H3
><P
>Most startup scripts, Linux and otherwise, start 8 instances of 
<B
CLASS="COMMAND"
>nfsd</B
>. In the
early days of NFS, Sun decided on this number as a rule of thumb, and everyone
else copied. There are no good measures of how many instances are optimal, but
a more heavily-trafficked server may require more. 
You should use at the very least one daemon per processor, but
four to eight per processor may be a better rule of thumb.
If you are using a 2.4 or
higher kernel and you want to see how heavily each 
<B
CLASS="COMMAND"
>nfsd</B
> thread is being used,
you can look at the file <TT
CLASS="FILENAME"
>/proc/net/rpc/nfsd</TT
>.
The last ten numbers on the <KBD
CLASS="USERINPUT"
>th</KBD
>
line in that file indicate the number of seconds that the thread usage was at
that percentage of the maximum allowable. If you have a large number in the
top three deciles, you may wish to increase the number 
of <B
CLASS="COMMAND"
>nfsd</B
> instances. This
is done upon starting <B
CLASS="COMMAND"
>nfsd</B
> using the
number of instances as the command line
option, and is specified in the NFS startup script 
(<TT
CLASS="FILENAME"
>/etc/rc.d/init.d/nfs</TT
> on
Red Hat) as <KBD
CLASS="USERINPUT"
>RPCNFSDCOUNT</KBD
>. 
See the <EM
>nfsd(8)</EM
> man page for more information.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="MEMLIMITS"
>5.7. Memory Limits on the Input Queue</A
></H3
><P
>On 2.2 and 2.4 kernels, the socket input queue, where requests sit while they
are currently being processed, has a small default size limit (<TT
CLASS="FILENAME"
>rmem_default</TT
>)
of 64k. This queue is important for clients with heavy read loads, and servers
with heavy write loads. As an example, if you are running 8 instances of nfsd
on the server, each will only have 8k to store write requests while it
processes them. In addition, the socket output queue - important for clients
with heavy write loads and servers with heavy read loads - also has a small
default size (<TT
CLASS="FILENAME"
>wmem_default</TT
>).</P
><P
>Several published runs of the NFS benchmark 
<A
HREF="http://www.spec.org/osg/sfs97/"
TARGET="_top"
>SPECsfs</A
> 
specify usage of a much higher value for both
the read and write value sets, <TT
CLASS="FILENAME"
>[rw]mem_default</TT
> and 
<TT
CLASS="FILENAME"
>[rw]mem_max</TT
>. You might
consider increasing these values to at least 256k. The read and write limits
are set in the proc file system using (for example) the files
<TT
CLASS="FILENAME"
>/proc/sys/net/core/rmem_default</TT
> and 
<TT
CLASS="FILENAME"
>/proc/sys/net/core/rmem_max</TT
>. The
<TT
CLASS="FILENAME"
>rmem_default</TT
> value can be increased in three steps; the following method is a
bit of a hack but should work and should not cause any problems:</P
><P
></P
><UL
><LI
><P
>     Increase the size listed in the file:</P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>     # echo 262144 &#62; /proc/sys/net/core/rmem_default
     # echo 262144 &#62; /proc/sys/net/core/rmem_max</PRE
></FONT
></TD
></TR
></TABLE
></LI
><LI
><P
>     Restart NFS.  For example, on Red Hat systems,</P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>     # /etc/rc.d/init.d/nfs restart</PRE
></FONT
></TD
></TR
></TABLE
></LI
><LI
><P
>     You might return the size limits to their normal size in case other
     kernel systems depend on it:</P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>     # echo 65536 &#62; /proc/sys/net/core/rmem_default
     # echo 65536 &#62; /proc/sys/net/core/rmem_max</PRE
></FONT
></TD
></TR
></TABLE
></LI
></UL
><P
>     This last step may be necessary because machines have been reported to
     crash if these values are left changed for long periods of time.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="AUTONEGOTIATION"
>5.8. Turning Off Autonegotiation of NICs and Hubs</A
></H3
><P
>If network cards auto-negotiate badly with hubs and switches, and ports run at
different speeds, or with different duplex configurations, performance will be
severely impacted due to excessive collisions, dropped packets, etc. If you
see excessive numbers of dropped packets in the 
<B
CLASS="COMMAND"
>nfsstat</B
> output, or poor
network performance in general, try playing around with the network speed and
duplex settings. If possible, concentrate on establishing a 100BaseT full
duplex subnet; the virtual elimination of collisions in full duplex will
remove the most severe performance inhibitor for NFS over UDP.  Be careful
when turning off autonegotiation on a card: The hub or switch that the card
is attached to will then resort to other mechanisms (such as parallel detection)
to determine the duplex settings, and some cards default to half duplex
because it is more likely to be supported by an old hub.  The best solution, 
if the driver supports it, is to force the card to negotiate 100BaseT 
full duplex.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SYNC-ASYNC"
>5.9. Synchronous vs. Asynchronous Behavior in NFS</A
></H3
><P
>The default export behavior for both NFS Version 2 and Version 3 protocols,
used by <B
CLASS="COMMAND"
>exportfs</B
> in <SPAN
CLASS="APPLICATION"
>nfs-utils</SPAN
> 
versions prior to Version 1.11 (the latter is in the CVS tree, 
but not yet released in a package, as of January, 2002) is
"asynchronous". This default permits the server to reply to client requests as
soon as it has processed the request and handed it off to the local file
system, without waiting for the data to be written to stable storage. This is
indicated by the <KBD
CLASS="USERINPUT"
>async</KBD
> option denoted in the server's export list. It yields
better performance at the cost of possible data corruption if the server
reboots while still holding unwritten data and/or metadata in its caches. This
possible data corruption is not detectable at the time of occurrence, since
the <KBD
CLASS="USERINPUT"
>async</KBD
> option instructs the server to lie to the client, telling the
client that all data has indeed been written to the stable storage, regardless
of the protocol used.</P
><P
>In order to conform with "synchronous" behavior, used as the default for most
proprietary systems supporting NFS (Solaris, HP-UX, RS/6000, etc.), and now
used as the default in the latest version of <B
CLASS="COMMAND"
>exportfs</B
>, the Linux Server's
file system must be exported with the <KBD
CLASS="USERINPUT"
>sync</KBD
> option. Note that specifying
synchronous exports will result in no option being seen in the server's export
list:</P
><P
></P
><UL
><LI
><P
>     Export a couple file systems to everyone, using slightly different
     options:</P
><P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>    # /usr/sbin/exportfs -o rw,sync *:/usr/local
    # /usr/sbin/exportfs -o rw *:/tmp</PRE
></FONT
></TD
></TR
></TABLE
></P
></LI
><LI
><P
>     Now we can see what the exported file system parameters look like:</P
><P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>    # /usr/sbin/exportfs -v
    /usr/local *(rw)
    /tmp *(rw,async)</PRE
></FONT
></TD
></TR
></TABLE
></P
></LI
></UL
><P
>If your kernel is compiled with the <TT
CLASS="FILENAME"
>/proc</TT
> filesystem,
then the file <TT
CLASS="FILENAME"
>/proc/fs/nfs/exports</TT
> will also show the
full list of export options.</P
><P
>When synchronous behavior is specified, the server will not complete (that is,
reply to the client) an NFS version 2 protocol request until the local file
system has written all data/metadata to the disk. The server
<EM
>will</EM
> complete a
synchronous NFS version 3 request without this delay, and will return the
status of the data in order to inform the client as to what data should be
maintained in its caches, and what data is safe to discard. There are 3
possible status values, defined an enumerated type, <KBD
CLASS="USERINPUT"
>nfs3_stable_how</KBD
>, in
<TT
CLASS="FILENAME"
>include/linux/nfs.h</TT
>. The values, along with the subsequent actions taken due
to these results, are as follows:</P
><P
></P
><UL
><LI
><P
>NFS_UNSTABLE - Data/Metadata was not committed to stable storage on the
server, and must be cached on the client until a subsequent client commit
request assures that the server does send data to stable storage.</P
></LI
><LI
><P
>NFS_DATA_SYNC - Metadata was not sent to stable storage, and must be cached
on the client. A subsequent commit is necessary, as is required above.</P
></LI
><LI
><P
>NFS_FILE_SYNC - No data/metadata need be cached, and a subsequent commit
need not be sent for the range covered by this request.</P
></LI
></UL
><P
>In addition to the above definition of synchronous behavior, the client may
explicitly insist on total synchronous behavior, regardless of the protocol,
by opening all files with the <KBD
CLASS="USERINPUT"
>O_SYNC</KBD
> option. In this case, all replies to
client requests will wait until the data has hit the server's disk, regardless
of the protocol used (meaning that, in NFS version 3, all requests will be
<KBD
CLASS="USERINPUT"
>NFS_FILE_SYNC</KBD
> requests, and will require that the Server returns this status).
In that case, the performance of NFS Version 2 and NFS Version 3 will be
virtually identical.</P
><P
>If, however, the old default <KBD
CLASS="USERINPUT"
>async</KBD
> 
behavior is used, the <KBD
CLASS="USERINPUT"
>O_SYNC</KBD
> option has
no effect at all in either version of NFS, since the server will reply to the
client without waiting for the write to complete. In that case the performance
differences between versions will also disappear.</P
><P
>Finally, note that, for NFS version 3 protocol requests, a subsequent commit
request from the NFS client at file close time, or at <B
CLASS="COMMAND"
>fsync()</B
> time, will force
the server to write any previously unwritten data/metadata to the disk, and
the server will not reply to the client until this has been completed, as long
as <KBD
CLASS="USERINPUT"
>sync</KBD
> behavior is followed. If <KBD
CLASS="USERINPUT"
>async</KBD
> is used, the commit is essentially
a no-op, since the server once again lies to the client, telling the client that
the data has been sent to stable storage. This again exposes the client and
server to data corruption, since cached data may be discarded on the client
due to its belief that the server now has the data maintained in stable
storage.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="NON-NFS-PERFORMANCE"
>5.10. Non-NFS-Related Means of Enhancing Server Performance</A
></H3
><P
>In general, server performance and server disk access speed will have an
important effect on NFS performance.
Offering general guidelines for setting up a well-functioning file server is
outside the scope of this document, but a few hints may be worth mentioning:</P
><P
></P
><UL
><LI
><P
>If you have access to RAID arrays, use RAID 1/0 for both write speed and
redundancy; RAID 5 gives you good read speeds but lousy write speeds.</P
></LI
><LI
><P
>A journalling filesystem will drastically reduce your reboot time in the
event of a system crash. Currently,
<A
HREF="ftp://ftp.uk.linux.org/pub/linux/sct/fs/jfs/"
TARGET="_top"
>ext3</A
> will work correctly with NFS
version 3. In addition, Reiserfs version 3.6 will work with NFS version 3 on
2.4.7 or later kernels (patches are available for previous kernels). Earlier versions
of Reiserfs did not include room for generation numbers in the inode, exposing
the possibility of undetected data corruption during a server reboot.</P
></LI
><LI
><P
>Additionally, journalled file systems can be configured to maximize
performance by taking advantage of the fact that journal updates are all that
is necessary for data protection. One example is using ext3 with <KBD
CLASS="USERINPUT"
>data=journal</KBD
>
so that all updates go first to the journal, and later to the main file
system. Once the journal has been updated, the NFS server can safely issue the
reply to the clients, and the main file system update can occur at the
server's leisure.</P
><P
>The journal in a journalling file system may also reside on a separate device
such as a flash memory card so that journal updates normally require no seeking. With only rotational
delay imposing a cost, this gives reasonably good synchronous IO performance.
Note that ext3 currently supports journal relocation, and ReiserFS will
(officially) support it soon. The Reiserfs tool package found at <A
HREF="ftp://ftp.namesys.com/pub/reiserfsprogs/reiserfsprogs-3.x.0k.tar.gz"
TARGET="_top"
>ftp://ftp.namesys.com/pub/reiserfsprogs/reiserfsprogs-3.x.0k.tar.gz </A
>
contains
the <B
CLASS="COMMAND"
>reiserfstune</B
> tool, which will allow journal relocation. It does, however,
require a kernel patch which has not yet been officially released as of
January, 2002.</P
></LI
><LI
><P
>Using an automounter (such as <SPAN
CLASS="APPLICATION"
>autofs</SPAN
> or <SPAN
CLASS="APPLICATION"
>amd</SPAN
>) may prevent hangs if you
cross-mount files on your machines (whether on purpose or by oversight) and
one of those machines goes down. See the <A
HREF="http://www.linuxdoc.org/HOWTO/mini/Automount.html"
TARGET="_top"
>Automount Mini-HOWTO</A
> for details.</P
></LI
><LI
><P
>Some manufacturers (Network Appliance, Hewlett Packard, and others) provide NFS
accelerators in the form of Non-Volatile RAM. NVRAM will boost access speed to
stable storage up to the equivalent of <KBD
CLASS="USERINPUT"
>async</KBD
> access.</P
></LI
></UL
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="SECURITY"
>6. Security and NFS</A
></H2
><P
>   This list of security tips and explanations will not make your site 
   completely secure. <EM
>NOTHING</EM
> will make your site completely secure. Reading this section
   may help you get an idea of the security problems with NFS. This is not 
   a comprehensive guide and it will always be undergoing changes. If you 
   have any tips or hints to give us please send them to the HOWTO 
   maintainer.
 </P
><P
>   If you are on a network with no access to the outside world (not even a 
   modem) and you trust all the internal machines and all your users then 
   this section will be of no use to you.  However, its our belief that 
   there are relatively few networks in this situation so we would suggest 
   reading this section thoroughly for anyone setting up NFS.
 </P
><P
>   With NFS, there are two steps required for a client to gain access to 
   a file contained in a remote directory on the server. The first step is mount
   access. Mount access is achieved by the client machine attempting to 
   attach to the server. The security for this is provided by the 
   <TT
CLASS="FILENAME"
>/etc/exports</TT
> file. This file lists the names or IP addresses for machines
   that are allowed to access a share point. If the client's ip address 
   matches one of the entries in the access list then it will be allowed to
   mount. This is not terribly secure. If someone is capable of spoofing or
   taking over a trusted address then they can access your mount points. To
   give a real-world example of this type of "authentication": This is 
   equivalent to someone introducing themselves to you and you believing they
   are who they claim to be because they are wearing a sticker that says
   "Hello, My Name is ...."  Once the machine has mounted a volume, its
   operating system will have access to all files on the volume (with the
   possible exception of those owned by root; see below) and write access
   to those files as well, if the volume was exported with the 
   <KBD
CLASS="USERINPUT"
>rw</KBD
> option.
 </P
><P
>   The second step is file access. This is a function of normal file system
   access controls on the client and not a specialized function of NFS. 
   Once the drive is mounted the user and group permissions on the files
   determine access control. 
 </P
><P
>   An example: bob on the server maps to the UserID 9999. Bob
   makes a file on the server that is only accessible the user 
   (the equivalent to typing 
   <KBD
CLASS="USERINPUT"
>chmod 600</KBD
> <EM
>filename</EM
>).
   A client is allowed to mount the drive where the file is stored.
   On the client mary maps to UserID 9999. This means that the client
   user mary can access bob's file that is marked as only accessible by him.
   It gets worse: If someone has become superuser on the client machine they can
   <B
CLASS="COMMAND"
>su - </B
> <EM
>username</EM
> 
   and become <EM
>any</EM
> user. NFS will be none the wiser. 
 </P
><P
>   Its not all terrible. There are a few measures you can take on the server
   to offset the danger of the clients. We will cover those shortly.
 </P
><P
>   If you don't think the security measures apply to you, you're probably
   wrong. In <A
HREF="#PORTMAPPER-SECURITY"
>Section 6.1</A
> we'll cover securing the portmapper, 
   server and client security in <A
HREF="#SERVER.SECURITY"
>Section 6.2</A
> and <A
HREF="#CLIENT.SECURITY"
>Section 6.3</A
> respectively. 
   Finally, in <A
HREF="#FIREWALLS"
>Section 6.4</A
> we'll briefly talk about proper firewalling for
   your nfs server. 
 </P
><P
>   Finally, it is critical that all of your nfs daemons and client programs
   are current. If you think that a flaw is too recently announced for it to
   be a problem for you, then you've probably already been compromised.
 </P
><P
>   A good way to keep up to date on security alerts is to subscribe to the 
   bugtraq mailinglists.  You can read up on how to subscribe and various 
   other information about bugtraq here:   
 
  <A
HREF="http://www.securityfocus.com/forums/bugtraq/faq.html"
TARGET="_top"
>http://www.securityfocus.com/forums/bugtraq/faq.html</A
>
 </P
><P
>   Additionally searching for <EM
>NFS</EM
> at 
   <A
HREF="http://www.securityfocus.com"
TARGET="_top"
>securityfocus.com's</A
> search engine will
   show you all security reports pertaining to NFS.
 </P
><P
>  You should also regularly check CERT advisories.  See the CERT web page
  at <A
HREF="http://www.cert.org"
TARGET="_top"
>www.cert.org</A
>.
 </P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="PORTMAPPER-SECURITY"
>6.1. The portmapper</A
></H3
><P
>    The portmapper keeps a list of what services are running on what ports.
    This list is used by a connecting machine to see what ports it wants to
    talk to access certain services. 
  </P
><P
>  
    The portmapper is not in as bad a shape as a few years ago but it is
    still a point of worry for many sys admins.  The portmapper, like NFS and
    NIS, should not really have connections made to it outside of a trusted
    local area network. If you have to expose them to the outside world - 
    be careful and keep up diligent monitoring of those systems.
  </P
><P
>    Not all Linux distributions were created equal.  Some seemingly up-to-date 
    distributions do not include a securable portmapper. 
    The easy way to check if your portmapper is good or not is to run 
    <EM
>strings(1)</EM
> and see if it reads the relevant files, <TT
CLASS="FILENAME"
>/etc/hosts.deny</TT
> and
    <TT
CLASS="FILENAME"
>/etc/hosts.allow</TT
>.  Assuming your portmapper is <TT
CLASS="FILENAME"
>/sbin/portmap</TT
> you can 
    check it with this command: 
     <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>     strings /sbin/portmap | grep hosts.  
     </PRE
></FONT
></TD
></TR
></TABLE
>
   </P
><P
>     On a securable machine it comes up something like this:
  <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>   /etc/hosts.allow
   /etc/hosts.deny
   @(#) hosts_ctl.c 1.4 94/12/28 17:42:27
   @(#) hosts_access.c 1.21 97/02/12 02:13:22
  </PRE
></FONT
></TD
></TR
></TABLE
>
  </P
><P
>   First we edit <TT
CLASS="FILENAME"
>/etc/hosts.deny</TT
>.  It should contain the line
  </P
><P
>  <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>   portmap: ALL
  </PRE
></FONT
></TD
></TR
></TABLE
>
  </P
><P
>    which will deny access to everyone.  While it is closed run:
  <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>   rpcinfo -p
  </PRE
></FONT
></TD
></TR
></TABLE
>
    just to check that your portmapper really reads and obeys
    this file.  Rpcinfo should give no output, or possibly an error message.
    The files <TT
CLASS="FILENAME"
>/etc/hosts.allow</TT
> and <TT
CLASS="FILENAME"
>/etc/hosts.deny</TT
>
    take effect immediately after you save them. No daemon needs to be restarted.
  </P
><P
>    Closing the portmapper for everyone is a bit drastic, so we open it
    again by editing <TT
CLASS="FILENAME"
>/etc/hosts.allow</TT
>.  But first 
    we need to figure out what to put in it.  It should basically list 
    all machines that should  have access to your portmapper.  On a run of 
    the mill Linux system there are very few machines that need any access 
    for any reason.  The portmapper administers <B
CLASS="COMMAND"
>nfsd</B
>, 
    <B
CLASS="COMMAND"
>mountd</B
>, <B
CLASS="COMMAND"
>ypbind</B
>/<B
CLASS="COMMAND"
>ypserv</B
>,
    <B
CLASS="COMMAND"
>rquotad</B
>, <B
CLASS="COMMAND"
>lockd</B
> (which shows up
    as <SAMP
CLASS="COMPUTEROUTPUT"
>nlockmgr</SAMP
>), <B
CLASS="COMMAND"
>statd</B
>
    (which shows up as <SAMP
CLASS="COMPUTEROUTPUT"
>status</SAMP
>) 
    and 'r' services like <B
CLASS="COMMAND"
>ruptime</B
> 
    and <B
CLASS="COMMAND"
>rusers</B
>.
    Of these only <B
CLASS="COMMAND"
>nfsd</B
>, <B
CLASS="COMMAND"
>mountd</B
>, 
    <B
CLASS="COMMAND"
>ypbind</B
>/<B
CLASS="COMMAND"
>ypserv</B
> and perhaps 
    <B
CLASS="COMMAND"
>rquotad</B
>,<B
CLASS="COMMAND"
>lockd</B
> 
    and <B
CLASS="COMMAND"
>statd</B
> are of any consequence.  All machines that need 
    to access services on your machine should be allowed to do that.  Let's 
    say that your machine's address is <EM
>192.168.0.254</EM
> and 
    that it lives on the subnet <EM
>192.168.0.0</EM
>, and that all 
    machines on the subnet should have access to it (for an overview of those
    terms see the <A
HREF="http://www.linuxdoc.org/HOWTO/Networking-Overview-HOWTO.html"
TARGET="_top"
>Networking-Overview-HOWTO</A
>).  Then we write:
   <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>   portmap: 192.168.0.0/255.255.255.0
   </PRE
></FONT
></TD
></TR
></TABLE
>
   in <TT
CLASS="FILENAME"
>/etc/hosts.allow</TT
>.  If you are not sure what your
   network or netmask are, you can use the <B
CLASS="COMMAND"
>ifconfig</B
> command to
   determine the netmask and the <B
CLASS="COMMAND"
>netstat</B
> command to
   determine the network.  For, example, for the 
   device eth0 on the above machine <B
CLASS="COMMAND"
>ifconfig</B
> should show:
   </P
><P
>   <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>   ...
   eth0   Link encap:Ethernet  HWaddr 00:60:8C:96:D5:56
          inet addr:192.168.0.254  Bcast:192.168.0.255 Mask:255.255.255.0
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:360315 errors:0 dropped:0 overruns:0
          TX packets:179274 errors:0 dropped:0 overruns:0
          Interrupt:10 Base address:0x320
   ...
   </PRE
></FONT
></TD
></TR
></TABLE
>

   and <B
CLASS="COMMAND"
>netstat -rn</B
> should show:

   <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>   Kernel routing table
   Destination     Gateway         Genmask         Flags Metric Ref Use    Iface
   ...
   192.168.0.0     0.0.0.0         255.255.255.0   U     0      0   174412 eth0
   ...
   </PRE
></FONT
></TD
></TR
></TABLE
>
   (The network address is in the first column).
  </P
><P
>    The <TT
CLASS="FILENAME"
>/etc/hosts.deny</TT
> and <TT
CLASS="FILENAME"
>/etc/hosts.allow</TT
> files are 
    described in the manual pages of the same names.
  </P
><P
>   <EM
>    IMPORTANT: Do not put anything but IP NUMBERS in the portmap lines of
    these files.  Host name lookups can indirectly cause portmap activity
    which will trigger host name lookups which can indirectly cause
    portmap activity which will trigger...
   </EM
>
  </P
><P
>    Versions 0.2.0 and higher of the nfs-utils package also use the 
    <TT
CLASS="FILENAME"
>hosts.allow</TT
> and <TT
CLASS="FILENAME"
>hosts.deny</TT
> 
    files, so you should put in entries for <B
CLASS="COMMAND"
>lockd</B
>, 
    <B
CLASS="COMMAND"
>statd</B
>, <B
CLASS="COMMAND"
>mountd</B
>, and 
    <B
CLASS="COMMAND"
>rquotad</B
> in these files too.  For a complete example,
    see <A
HREF="#HOSTS"
>Section 3.2.2</A
>.
  </P
><P
>    The above things should make your server tighter.  The only remaining
    problem is if someone gains administrative access to one of your trusted
    client machines and is able to send bogus NFS requests.  The next section
    deals with safeguards against this problem. 
  </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SERVER.SECURITY"
>6.2. Server security: nfsd and mountd</A
></H3
><P
>    On the server we can decide that we don't want to trust any requests
    made as root on the client.  We can do that by using the 
    <KBD
CLASS="USERINPUT"
>root_squash</KBD
> option in <TT
CLASS="FILENAME"
>/etc/exports</TT
>:
   <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>   /home slave1(rw,root_squash)
   </PRE
></FONT
></TD
></TR
></TABLE
>
   </P
><P
>     This is, in fact, the default. It should always be turned on unless you
     have a <EM
>very</EM
> good reason to turn it off. To turn it off use the 
     <KBD
CLASS="USERINPUT"
>no_root_squash</KBD
> option.
   </P
><P
>     Now, if a user with <EM
>UID</EM
> 0 (i.e., root's user ID number) 
     on the client attempts to access (read, write, delete) the file system, 
     the server substitutes the <EM
>UID</EM
> of the server's 'nobody' 
     account.  Which means that the root user on the client can't access or 
     change files that only root on the server can access or change.  That's 
     good, and you should  probably use <KBD
CLASS="USERINPUT"
>root_squash</KBD
> on 
     all the file systems you export.  "But the root user on the client can 
     still use <B
CLASS="COMMAND"
>su</B
> to become any other user and 
     access and change that users files!" say you.  To which the answer is: 
     Yes, and that's the way it is, and has to be with Unix and NFS.  This 
     has one important implication: All important binaries and files should be
     owned by root, and not bin or other non-root account, since the only
     account the clients root user cannot access is the servers root
     account.  In the <EM
>exports(5)</EM
> man page there are several other squash 
     options listed so that you can decide to mistrust whomever you (don't) 
     like on the clients.  
   </P
><P
>     The TCP ports 1-1024 are reserved for root's use (and therefore sometimes
     referred to as "secure ports") A non-root user cannot bind these ports.
     Adding the <KBD
CLASS="USERINPUT"
>secure</KBD
> option to an 
     <TT
CLASS="FILENAME"
>/etc/exports</TT
> means that it will only listed to
     requests coming from ports 1-1024 on the client, so that a malicious
     non-root user on the client cannot come along and open up a spoofed 
     NFS dialogue on a non-reserved port. This option is set by default.
   </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="CLIENT.SECURITY"
>6.3. Client Security</A
></H3
><DIV
CLASS="SECT3"
><H4
CLASS="SECT3"
><A
NAME="NOSUID"
>6.3.1. The nosuid mount option</A
></H4
><P
>      On the client we can decide that we don't want to trust the server too
      much a couple of ways with options to mount.  For example we can
      forbid suid programs to work off the NFS file system with the 
      <KBD
CLASS="USERINPUT"
>nosuid</KBD
>
      option. Some unix programs, such as passwd, are called "suid" programs: 
      They set the id of the person running them to whomever is the owner of 
      the file. If a file is owned by root and is suid, then the program will
      execute as root, so that they can perform operations (such as writing to
      the password file) that only root is allowed to do. Using the 
      <KBD
CLASS="USERINPUT"
>nosuid</KBD
> 
      option is a good idea and you should consider using this with all NFS 
      mounted disks.  It means that the server's root user 
      cannot make a suid-root 
      program on the file system, log in to the client as a normal user 
      and then use the suid-root program to become root on the client too.  
      One could also forbid execution of files on the mounted file system 
      altogether with the <KBD
CLASS="USERINPUT"
>noexec</KBD
> option.  
      But this is more likely to be impractical than 
      <KBD
CLASS="USERINPUT"
>nosuid</KBD
> since a file 
      system is likely to at least contain some scripts or programs that need 
      to be executed.
    </P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="BROKENSUID"
>6.3.2. The broken_suid mount option</A
></H4
><P
>      Some older programs (<B
CLASS="COMMAND"
>xterm</B
> being one of them) used to rely on the idea
      that root can write everywhere. This is will break under new kernels on
      NFS mounts.  The security implications are that programs that do this
      type of suid action can potentially be used to change your apparent uid
      on nfs servers doing uid mapping. So the default has been to disable this
     <KBD
CLASS="USERINPUT"
>broken_suid</KBD
> in the linux kernel. 
    </P
><P
>      The long and short of it is this: If you're using an old linux
      distribution, some sort of old suid program or an older unix of some
      type you <EM
>might</EM
> have to mount from your clients with the
      <KBD
CLASS="USERINPUT"
>broken_suid</KBD
> option to <B
CLASS="COMMAND"
>mount</B
>.  
      However, most recent unixes and linux distros have <B
CLASS="COMMAND"
>xterm</B
> and such programs 
      just as a normal executable with no suid status, they call programs to do their setuid work.
    </P
><P
>      You enter the above options in the options column, with the <KBD
CLASS="USERINPUT"
>rsize</KBD
> and
      <KBD
CLASS="USERINPUT"
>wsize</KBD
>, separated by commas.
    </P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="SECURING-DAEMONS"
>6.3.3. Securing portmapper, rpc.statd, and rpc.lockd on the client</A
></H4
><P
>     In the current (2.2.18+) implementation of NFS, full file locking is 
     supported. This means that <B
CLASS="COMMAND"
>rpc.statd</B
> and <B
CLASS="COMMAND"
>rpc.lockd</B
>
     must be running on the client in order for locks to function correctly. 
     These services require the portmapper to be running. So, most of the 
     problems you will find with nfs on the server you may also be plagued with 
     on the client. Read through the portmapper section above for information on 
     securing the portmapper.
   </P
></DIV
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="FIREWALLS"
>6.4. NFS and firewalls (ipchains and netfilter)</A
></H3
><P
>    IPchains (under the 2.2.X kernels) and netfilter (under the 2.4.x 
    kernels) allow a good level of security - instead of relying on the 
    daemon (or  perhaps its TCP wrapper) to 
    determine which machines can connect, 
    the connection attempt is allowed or disallowed at a lower level. In 
    this case, you can stop the connection much earlier and more globally, which
    can protect you from all sorts of attacks.
  </P
><P
>    Describing how to set up a Linux firewall is well beyond the scope of
    this document. Interested readers may wish to read the <A
HREF="http://www.linuxdoc.org/HOWTO/Firewall-HOWTO.html"
TARGET="_top"
>Firewall-HOWTO</A
> 
    or the <A
HREF="http://www.linuxdoc.org/HOWTO/IPCHAINS-HOWTO.HTML"
TARGET="_top"
>IPCHAINS-HOWTO</A
>.
    For users of kernel 2.4 and above you might want to visit the netfilter webpage at: 
    <A
HREF="http://netfilter.filewatcher.org"
TARGET="_top"
>http://netfilter.filewatcher.org</A
>.
    If you are already familiar with the workings of ipchains or netfilter 
    this section will give you a few tips on how to better setup your 
    NFS daemons to more easily firewall and protect them.
  </P
><P
>    A good rule to follow for your firewall configuration is to deny all, and
    allow only some - this helps to keep you from accidentally allowing more 
    than you intended.
  </P
><P
> In order to understand how to firewall the NFS daemons, it will help
 to breifly review how they bind to ports.</P
><P
>When a daemon starts up, it requests a free port from the portmapper. 
The portmapper gets the port for the daemon and keeps track of 
the port currently used by that daemon. When other hosts or processes
need to communicate with the daemon, they request the port number
from the portmapper in order to find the
daemon. So the ports will perpetually float because different ports may 
be free at different times and so the portmapper will allocate them 
differently each time. This is a pain for setting up a firewall. If 
you never know where the daemons are going to be then you don't 
know precisely which ports to allow access to. This might not be a big deal 
for many people running on a protected or isolated LAN. For those 
people on a public network, though, this is horrible.</P
><P
>In kernels 2.4.13 and later with nfs-utils 0.3.3 or later you no 
longer have to worry about the floating of ports in the portmapper. 
Now all of the daemons pertaining to nfs can be "pinned" to a port. 
Most of them nicely take a <B
CLASS="COMMAND"
>-p</B
> option when they are started;
those daemons that are started by the kernel take some kernel arguments 
or module options. They are described below.</P
><P
>Some of the daemons involved in sharing data via nfs are already 
bound to a port. <B
CLASS="COMMAND"
>portmap</B
> is always on port 
111 tcp and udp. <B
CLASS="COMMAND"
>nfsd</B
> is 
always on port 2049 TCP and UDP (however, as of kernel 2.4.17, NFS over
TCP is considered experimental and is not for use on production machines).</P
><P
>The other daemons, <B
CLASS="COMMAND"
>statd</B
>, <B
CLASS="COMMAND"
>mountd</B
>,
<B
CLASS="COMMAND"
>lockd</B
>, and <B
CLASS="COMMAND"
>rquotad</B
>, will normally move
around to the first available port they are informed of by the portmapper.</P
><P
>To force <B
CLASS="COMMAND"
>statd</B
> to bind to a particular port, use the 
<KBD
CLASS="USERINPUT"
>-p</KBD
>
<EM
>portnum</EM
> option. To force <B
CLASS="COMMAND"
>statd</B
> to 
respond on a  particular port, additionally use the 
<KBD
CLASS="USERINPUT"
>-o</KBD
> <EM
>portnum</EM
> option when starting it.</P
><P
>To force <B
CLASS="COMMAND"
>mountd</B
> to bind to a particular port use the 
<KBD
CLASS="USERINPUT"
>-p</KBD
> <EM
>portnum</EM
> option.</P
><P
>For example, to have statd broadcast of port 32765 and listen on port
32766, and mountd listen on port 32767, you would type:</P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
># statd -p 32765 -o 32766
# mountd -p 32767</PRE
></FONT
></TD
></TR
></TABLE
><P
><B
CLASS="COMMAND"
>lockd</B
> is started by the kernel when it is needed. 
Therefore you need 
to pass module options (if you have it built as a module) or kernel 
options to force <B
CLASS="COMMAND"
>lockd</B
> to listen and respond 
only on certain ports.</P
><P
>If you are using loadable modules and you would like to specify these
options in your <TT
CLASS="FILENAME"
>/etc/modules.conf</TT
> file add 
a line like this to the file:</P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>options lockd nlm_udpport=32768 nlm_tcpport=32768</PRE
></FONT
></TD
></TR
></TABLE
><P
>The above line would specify the udp and tcp port for 
<B
CLASS="COMMAND"
>lockd</B
> to be 32768.</P
><P
>If you are not using loadable modules or if you have compiled 
<B
CLASS="COMMAND"
>lockd</B
> into the kernel instead of building it
 as a module then you will need to pass it an option on the kernel boot line.</P
><P
>It should look something like this:</P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> vmlinuz 3 root=/dev/hda1 lockd.udpport=32768 lockd.tcpport=32768</PRE
></FONT
></TD
></TR
></TABLE
><P
>The port numbers do not have to match but it would simply add 
unnecessary confusion if they didn't.</P
><P
>If you are using quotas and using <B
CLASS="COMMAND"
>rpc.quotad</B
> to make these 
quotas viewable over nfs you will need to also take it into
account when setting up your firewall. There are two 
<B
CLASS="COMMAND"
>rpc.rquotad</B
> 
source trees. One of those is maintained in the 
<SPAN
CLASS="APPLICATION"
>nfs-utils</SPAN
> tree. 
The other in the <SPAN
CLASS="APPLICATION"
>quota-tools</SPAN
> tree. 
They do not operate identically. 
The one provided with <SPAN
CLASS="APPLICATION"
>nfs-utils</SPAN
> supports 
binding the daemon to  a port with the <KBD
CLASS="USERINPUT"
>-p</KBD
> 
directive. The one in <SPAN
CLASS="APPLICATION"
>quota-tools</SPAN
> does not. 
Consult your distribution's documentation to determine if yours does. </P
><P
>For the sake of this discussion lets describe a network and setup a 
firewall to protect our nfs server. 
Our nfs server is 192.168.0.42 our client is 192.168.0.45 only.
As in the example above, <B
CLASS="COMMAND"
>statd</B
> has been 
started so that it only
binds to port 32765 for incoming requests and it must answer on 
port 32766.  <B
CLASS="COMMAND"
>mountd</B
> is forced to bind to port 32767.
<B
CLASS="COMMAND"
>lockd</B
>'s module parameters have been set to bind to 32768.
<B
CLASS="COMMAND"
>nfsd</B
> is, of course, on port 2049 and the portmapper is on port 111.</P
><P
>We are not using quotas.</P
><P
>Using <SPAN
CLASS="APPLICATION"
>IPCHAINS</SPAN
>, a simple firewall 
might look something like this:</P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>ipchains -A input -f -j ACCEPT -s 192.168.0.45
ipchains -A input -s 192.168.0.45 -d 0/0 32765:32768 -p 6 -j ACCEPT
ipchains -A input -s 192.168.0.45 -d 0/0 32765:32768 -p 17 -j ACCEPT
ipchains -A input -s 192.168.0.45 -d 0/0 2049 -p 17 -j ACCEPT
ipchains -A input -s 192.168.0.45 -d 0/0 2049 -p 6 -j ACCEPT
ipchains -A input -s 192.168.0.45 -d 0/0 111 -p 6 -j ACCEPT
ipchains -A input -s 192.168.0.45 -d 0/0 111 -p 17 -j ACCEPT
ipchains -A input -s 0/0 -d 0/0 -p 6 -j DENY -y -l
ipchains -A input -s 0/0 -d 0/0 -p 17 -j DENY -l</PRE
></FONT
></TD
></TR
></TABLE
><P
>The equivalent set of commands in <SPAN
CLASS="APPLICATION"
>netfilter</SPAN
> is:</P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>iptables -A INPUT -f -j ACCEPT -s 192.168.0.45
iptables -A INPUT -s 192.168.0.45 -d 0/0 32765:32768 -p 6 -j ACCEPT
iptables -A INPUT -s 192.168.0.45 -d 0/0 32765:32768 -p 17 -j ACCEPT
iptables -A INPUT -s 192.168.0.45 -d 0/0 2049 -p 17 -j ACCEPT
iptables -A INPUT -s 192.168.0.45 -d 0/0 2049 -p 6 -j ACCEPT
iptables -A INPUT -s 192.168.0.45 -d 0/0 111 -p 6 -j ACCEPT
iptables -A INPUT -s 192.168.0.45 -d 0/0 111 -p 17 -j ACCEPT
iptables -A INPUT -s 0/0 -d 0/0 -p 6 -j DENY --syn --log-level 5
iptables -A INPUT -s 0/0 -d 0/0 -p 17 -j DENY --log-level 5</PRE
></FONT
></TD
></TR
></TABLE
><P
>The first line says to accept all packet fragments (except the 
first packet fragment which will be treated as a normal packet). 
In theory no packet will pass through until it is reassembled, 
and it won't be reassembled unless the first packet fragment 
is passed. Of course there are attacks that can be generated 
by overloading a machine with packet fragments. But NFS won't 
work correctly unless you let fragments through. See <A
HREF="#SYMPTOM8"
>Section 7.8</A
>
for details.</P
><P
>The other lines allow specific connections from any port on our 
client host to the specific ports we have made available on 
our server.  This means that if, say, 192.158.0.46 attempts to contact 
the NFS server it will not be able to mount or see what mounts 
are available.</P
><P
>With the new port pinning capabilities it is obviously much easier
to control what hosts are allowed to mount your NFS shares. It is
worth mentioning that NFS is not an encrypted protocol and anyone
on the same physical network could sniff the traffic and reassemble
the information being passed back and forth.
 </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="NFS-SSH"
>6.5. Tunneling NFS through SSH</A
></H3
><P
>One method of encrypting NFS traffic over a network is to 
use the port-forwarding capabilities of <B
CLASS="COMMAND"
>ssh</B
>.
However, as we shall see, doing so has a serious drawback if you do not
utterly and completely trust the local users on your server.</P
><P
>The first step will be to export files to the localhost.  For example, to
export the <TT
CLASS="FILENAME"
>/home</TT
> partition, enter the following into 
<TT
CLASS="FILENAME"
>/etc/exports</TT
>:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>/home   127.0.0.1(rw)</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>The next step is to use <B
CLASS="COMMAND"
>ssh</B
> to forward ports.  For example,
<B
CLASS="COMMAND"
>ssh</B
> can tell the server to forward to any port on any
machine from a port on the client.  Let us assume, as in the previous
section, that our server is 192.168.0.42, and that we have pinned 
<B
CLASS="COMMAND"
>mountd</B
> to port 32767
using the argument <KBD
CLASS="USERINPUT"
>-p 32767</KBD
>. Then, on the client,
we'll type:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>     # ssh root@192.168.0.42 -L 250:localhost:2049  -f sleep 60m
     # ssh root@192.168.0.42 -L 251:localhost:32767 -f sleep 60m</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>The above command causes <B
CLASS="COMMAND"
>ssh</B
> on the client to take
any request directed at the client's port 250 and forward it, 
first through <B
CLASS="COMMAND"
>sshd</B
> on the server, and then on
 to the server's port 2049.  The second line
causes a similar type of forwarding between requests to port 251 on
the client and port 32767 on the server.  The 
<KBD
CLASS="USERINPUT"
>localhost</KBD
> is relative to the server; that is,
the forwarding will be done to the server itself.  The port could otherwise
have been made to forward to any other machine, and the requests would look to
the outside world as if they were coming from the server.  Thus, the requests
will appear to NFSD on the server as if they are coming from the server itself. 
Note that in order to bind to a port below 1024 on the client, we have 
to run this command as root on the client.  Doing this will be necessary 
if we have exported our filesystem with the default 
<KBD
CLASS="USERINPUT"
>secure</KBD
> option.</P
><P
>Finally, we are pulling a little trick with the last option,
<KBD
CLASS="USERINPUT"
>-f sleep 60m</KBD
>.  Normally, when
we use <B
CLASS="COMMAND"
>ssh</B
>, even with the <KBD
CLASS="USERINPUT"
>-L</KBD
> option,
we will open up a shell on the remote machine.  But instead, we just want
the port forwarding to execute in the background so that we get our shell
on the client back.  So, we tell <B
CLASS="COMMAND"
>ssh</B
> to execute a command
in the background on the server to sleep for 60 minutes.  This will cause
the port to be forwarded for 60 minutes until it gets a connection; at that
point, the port will continue to be forwarded until the connection dies or
until the 60 minutes are up, whichever happens later.  The above command
could be put in our startup scripts on the client, right after the network
is started.</P
><P
>  Next, we have to mount the filesystem on the client.  To do this, we tell
  the client to mount a filesystem on the localhost, but at a different
  port from the usual 2049.  Specifically, an entry in <TT
CLASS="FILENAME"
>/etc/fstab</TT
>
  would look like:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  localhost:/home  /mnt/home  nfs  rw,hard,intr,port=250,mountport=251  0 0</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>Having done this, we can see why the above will be incredibly insecure
if we have <EM
>any</EM
> ordinary users who are able to log in
to the server locally.  If they can, there is nothing preventing them from
doing what we did and using <B
CLASS="COMMAND"
>ssh</B
> to forward a privileged 
port on their own client machine (where they are legitimately root) to ports
2049 and 32767 on the server.  Thus, any ordinary user on the server can
mount our filesystems with the same rights as root on our client.</P
><P
>If you are using an NFS server that does not have a way for ordinary users
to log in, and you wish to use this method, there are two additional caveats:
First, the connection travels from the client to the server via
<B
CLASS="COMMAND"
>sshd</B
>; therefore you will have to leave port 22 (where
<B
CLASS="COMMAND"
>sshd</B
> listens) open to your client on the firewall.  However
you do not need to leave the other ports, such as 2049 and 32767, open
anymore.  Second, file locking will no longer work.  It is not possible
to ask <B
CLASS="COMMAND"
>statd</B
> or the locking manager to make requests
to a particular port for a particular mount; therefore, any locking requests
will cause <B
CLASS="COMMAND"
>statd</B
> to connect to <B
CLASS="COMMAND"
>statd</B
>
on localhost, i.e., itself, and it will fail with an error.  Any attempt
to correct this would require a major rewrite of NFS.</P
><P
>It may also be possible to use <SPAN
CLASS="APPLICATION"
>IPSec</SPAN
> to encrypt
network traffic between your client and your server, without compromising
any local security on the server; this will not be taken up here.
See the <A
HREF="http://www.freeswan.org/"
TARGET="_top"
>FreeS/WAN</A
> home page
for details on using IPSec under Linux.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SUMMARY"
>6.6. Summary</A
></H3
><P
>    If you use the <TT
CLASS="FILENAME"
>hosts.allow</TT
>, <TT
CLASS="FILENAME"
>hosts.deny</TT
>,
   <KBD
CLASS="USERINPUT"
>root_squash</KBD
>, <KBD
CLASS="USERINPUT"
>nosuid</KBD
> and privileged
    port features in the portmapper/NFS software, you avoid many of the
    presently known bugs in NFS and can almost feel secure about that at
    least.  But still, after all that: When an intruder has access to your
    network, s/he can make strange commands appear in your <TT
CLASS="FILENAME"
>.forward</TT
> or
    read your mail when <TT
CLASS="FILENAME"
>/home</TT
> or <TT
CLASS="FILENAME"
>/var/mail</TT
> is 
    NFS exported.  For the same reason, you should never access your PGP private key 
    over NFS. Or at least you should know the risk involved.  And now you know a bit
    of it.
  </P
><P
>    NFS and the portmapper makes up a complex subsystem and therefore it's
    not totally unlikely that new bugs will be discovered, either in the
    basic design or the implementation we use.  There might even be holes
    known now, which someone is abusing.  But that's life.  
  </P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="TROUBLESHOOTING"
>7. Troubleshooting</A
></H2
><BLOCKQUOTE
CLASS="ABSTRACT"
><DIV
CLASS="ABSTRACT"
><P
></P
><A
NAME="AEN951"
></A
><P
>   This is intended as a step-by-step guide to what to do when
   things go wrong using NFS.  Usually trouble first rears its
   head on the client end, so this diagnostic will begin there.
 </P
><P
></P
></DIV
></BLOCKQUOTE
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SYMPTOM1"
>7.1. Unable to See Files on a Mounted File System</A
></H3
><P
>       First, check to see if the file system is actually mounted.
       There are several ways of doing this.  The most reliable
       way is to look at the file <TT
CLASS="FILENAME"
>/proc/mounts</TT
>, 
       which will list all mounted filesystems and give details about them.  If
       this doesn't work (for example if you don't 
       have the <TT
CLASS="FILENAME"
>/proc</TT
>
       filesystem compiled into your kernel), you can type 
       <KBD
CLASS="USERINPUT"
>mount -f</KBD
> although you get less information.  
     </P
><P
>       If the file system appears to be mounted, then you may
       have mounted another file system on top of it (in which
       case you should unmount and remount both volumes), or you 
       may have exported the file system on the server before you 
       mounted it there, in which case NFS is exporting the underlying 
       mount point (if so then you need to restart NFS on the 
       server).
     </P
><P
>       If the file system is not mounted, then attempt to mount it.
       If this does not work, see <A
HREF="#SYMPTOM3"
><I
>Symptom 3</I
></A
>.
     </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SYMPTOM2"
>7.2. File requests hang or timeout waiting for access to the file.</A
></H3
><P
>      This usually means that the client is unable to communicate with
      the server.  See <A
HREF="#SYMPTOM3"
><I
>Symptom 3</I
></A
> letter b.
     </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SYMPTOM3"
>7.3. Unable to mount a file system</A
></H3
><P
>       There are two common errors that mount produces when
       it is unable to mount a volume.  These are:
      <P
></P
><OL
TYPE="a"
><LI
><P
>          failed, reason given by server: 
          <SAMP
CLASS="COMPUTEROUTPUT"
>Permission denied</SAMP
>
        </P
><P
>   
          This means that the server does not recognize that you
          have access to the volume.  
        </P
><P
></P
><OL
TYPE="i"
><LI
><P
>              Check your <TT
CLASS="FILENAME"
>/etc/exports</TT
> file and make sure that the
              volume is exported and that your client has the right
              kind of access to it.  For example, if a client only
              has read access then you have to mount the volume
              with the <KBD
CLASS="USERINPUT"
>ro</KBD
> option rather 
              than the <KBD
CLASS="USERINPUT"
>rw</KBD
> option.
            </P
></LI
><LI
><P
>             Make sure that you have told NFS to register any
             changes you made to <TT
CLASS="FILENAME"
>/etc/exports</TT
> since starting nfsd
             by running the exportfs command.  Be sure to type
             <B
CLASS="COMMAND"
>exportfs -ra</B
> to be extra certain that the exports are
             being re-read.
           </P
></LI
><LI
><P
>             Check the file <TT
CLASS="FILENAME"
>/proc/fs/nfs/exports</TT
> and make sure the 
             volume and client are listed correctly.  (You can also
             look at the file <TT
CLASS="FILENAME"
>/var/lib/nfs/xtab</TT
> for an unabridged
             list of how all the active export options are set.)  If they 
             are not, then you have not re-exported properly.  If they
             are listed, make sure the server recognizes your
             client as being the machine you think it is.  For
             example, you may have an old listing for the client
             in <TT
CLASS="FILENAME"
>/etc/hosts</TT
> that is throwing off the server, or
             you may not have listed the client's complete address
             and it may be resolving to a machine in a different 
             domain.  One trick is login to the server from the
             client via <B
CLASS="COMMAND"
>ssh</B
> or <B
CLASS="COMMAND"
>telnet</B
>;
             if you then type <B
CLASS="COMMAND"
>who</B
>, one of the listings
             should be your login session and the name of your client
             machine as the server sees it.  Try using this machine name
             in your <TT
CLASS="FILENAME"
>/etc/exports</TT
> entry.
             Finally, try to ping the client from the server, and try
             to <B
CLASS="COMMAND"
>ping</B
> the server from the client.  If this doesn't work,
             or if there is packet loss, you may have lower-level network 
             problems.
           </P
></LI
><LI
><P
>             It is not possible to export both a directory and its child
             (for example both 
             <TT
CLASS="FILENAME"
>/usr</TT
> and <TT
CLASS="FILENAME"
>/usr/local</TT
>).
             You should export the parent directory with the necessary
             permissions, and all of its subdirectories can then be
             mounted with those same permissions.
           </P
></LI
></OL
></LI
><LI
><P
><SAMP
CLASS="COMPUTEROUTPUT"
>          RPC: Program Not Registered</SAMP
>: (or another "RPC" error):</P
><P
>          This means that the client does not detect NFS running
          on the server.  This could be for several reasons.
        </P
><P
></P
><OL
TYPE="i"
><LI
><P
>            First, check that NFS actually is running on the
            server by typing <B
CLASS="COMMAND"
>rpcinfo -p</B
> on the server.
            You should see something like this:
         <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>   program vers proto   port
    100000    2   tcp    111  portmapper
    100000    2   udp    111  portmapper
    100011    1   udp    749  rquotad
    100011    2   udp    749  rquotad
    100005    1   udp    759  mountd
    100005    1   tcp    761  mountd
    100005    2   udp    764  mountd
    100005    2   tcp    766  mountd
    100005    3   udp    769  mountd
    100005    3   tcp    771  mountd
    100003    2   udp   2049  nfs
    100003    3   udp   2049  nfs
    300019    1   tcp    830  amd
    300019    1   udp    831  amd
    100024    1   udp    944  status
    100024    1   tcp    946  status
    100021    1   udp   1042  nlockmgr
    100021    3   udp   1042  nlockmgr
    100021    4   udp   1042  nlockmgr
    100021    1   tcp   1629  nlockmgr
    100021    3   tcp   1629  nlockmgr
    100021    4   tcp   1629  nlockmgr
  </PRE
></FONT
></TD
></TR
></TABLE
>

            This says that we have NFS versions 2 and 3, rpc.statd 
            version 1, network lock manager (the service name for 
            <B
CLASS="COMMAND"
>rpc.lockd</B
>) versions 1, 3, and 4.
            There are also different
            service listings depending on whether NFS is travelling over 
            TCP or UDP.  UDP is usually (but not always) the default 
            unless TCP is explicitly requested.
          </P
><P
>            If you do not see at least <SAMP
CLASS="COMPUTEROUTPUT"
>portmapper</SAMP
>,            <SAMP
CLASS="COMPUTEROUTPUT"
>nfs</SAMP
>, and
            <SAMP
CLASS="COMPUTEROUTPUT"
>mountd</SAMP
>, then 
            you need to restart NFS.  If you are not able to restart 
            successfully, proceed to <A
HREF="#SYMPTOM9"
><I
>Symptom 9</I
></A
>.
          </P
></LI
><LI
><P
>            Now check to make sure you can see it from the client.
            On the client, type <B
CLASS="COMMAND"
>rpcinfo -p </B
> 
            <EM
>server</EM
> where <EM
>server</EM
>
            is the DNS name or IP address of your server.  
          </P
><P
>            If you get a listing, then make sure that the type
            of mount you are trying to perform is supported. For
            example, if you are trying to mount using Version 3
            NFS, make sure Version 3 is listed; if you are trying
            to mount using NFS over TCP, make sure that is
            registered.  (Some non-Linux clients default to TCP).
            Type <KBD
CLASS="USERINPUT"
>man rpcinfo</KBD
> for more details on how
            to read the output.  If the type of mount you are
            trying to perform is not listed, try a different
            type of mount.
          </P
><P
>            If you get the error 
            <SAMP
CLASS="COMPUTEROUTPUT"
>No Remote Programs Registered</SAMP
>,
            then you need to check your <TT
CLASS="FILENAME"
>/etc/hosts.allow</TT
> and
            <TT
CLASS="FILENAME"
>/etc/hosts.deny</TT
> files on the server and make sure
            your client actually is allowed access.  Again, if the
            entries appear correct, check <TT
CLASS="FILENAME"
>/etc/hosts</TT
> (or your
            DNS server) and make sure that the machine is listed
            correctly, and make sure you can ping the server from
            the client.  Also check the error logs on the system
            for helpful messages: Authentication errors from bad
            <TT
CLASS="FILENAME"
>/etc/hosts.allow</TT
> entries will usually appear in 
            <TT
CLASS="FILENAME"
>/var/log/messages</TT
>, 
            but may appear somewhere else depending
            on how your system logs are set up.  The man pages
            for <SAMP
CLASS="COMPUTEROUTPUT"
>syslog</SAMP
> can 
            help you figure out how your logs are
            set up.  Finally, some older operating systems may
            behave badly when routes between the two machines
            are asymmetric.  Try typing <B
CLASS="COMMAND"
>tracepath [server]</B
> from
            the client and see if the word "asymmetric" shows up
            anywhere in the output.  If it does then this may
            be causing packet loss.  However asymmetric routes are
            not usually a problem on recent linux distributions.
          </P
><P
>            If you get the error 
            <SAMP
CLASS="COMPUTEROUTPUT"
>Remote system error - No route to host</SAMP
>,
            but you can ping the server correctly,
            then you are the victim of an overzealous
            firewall.  Check any firewalls that may be set up,
            either on the server or on any routers in between
            the client and the server.  Look at the man pages
            for <B
CLASS="COMMAND"
>ipchains</B
>, <B
CLASS="COMMAND"
>netfilter</B
>, 
            and <B
CLASS="COMMAND"
>ipfwadm</B
>, as well as the 
            <A
HREF="http://www.linuxdoc.org/HOWTO/IPCHAINS-HOWTO.html"
TARGET="_top"
>IPChains-HOWTO</A
>
            and the <A
HREF="http://www.linuxdoc.org/HOWTO/Firewall-HOWTO.html"
TARGET="_top"
>Firewall-HOWTO</A
> for help.
          </P
></LI
></OL
></LI
></OL
>
    </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SYMPTOM4"
>7.4. I do not have permission to access files on the mounted volume.</A
></H3
><P
>     This could be one of two problems.  
   </P
><P
>     If it is a write permission problem, check the export 
     options on the server by looking at <TT
CLASS="FILENAME"
>/proc/fs/nfs/exports</TT
>
     and make sure the filesystem is not exported read-only.  
     If it is you will need to re-export it read/write 
     (don't forget to run <B
CLASS="COMMAND"
>exportfs -ra</B
> after editing
     <TT
CLASS="FILENAME"
>/etc/exports</TT
>).  Also, check
     <TT
CLASS="FILENAME"
>/proc/mounts</TT
> and make sure the volume 
     is mounted read/write (although if it is mounted read-only 
     you ought to get a more specific error message).  If not then 
     you need to re-mount with the <KBD
CLASS="USERINPUT"
>rw</KBD
> option.
   </P
><P
>     The second problem has to do with username mappings, and is
     different depending on whether you are trying to do this
     as root or as a non-root user.
   </P
><P
>     If you are not root, then usernames may not be in sync on 
     the client and the server. Type <B
CLASS="COMMAND"
>id [user]</B
>
     on both the client and the server and make sure they give the 
     same <EM
>UID</EM
> number. If they don't then 
     you are having problems with NIS, NIS+, rsync, or whatever 
     system you use to sync usernames.  Check group names to make 
     sure that they match as well. Also, make sure you are not 
     exporting with the <KBD
CLASS="USERINPUT"
>all_squash</KBD
> option.  
     If the user names match then the user has a more general
     permissions problem unrelated to NFS.
   </P
><P
>     If you are root, then you are probably not exporting with
     the <KBD
CLASS="USERINPUT"
>no_root_squash</KBD
> option; check <TT
CLASS="FILENAME"
>/proc/fs/nfs/exports</TT
>
     or <TT
CLASS="FILENAME"
>/var/lib/nfs/xtab</TT
> on the server and make sure the option 
     is listed.  In general, being able to write to the NFS
     server as root is a bad idea unless you have an urgent need --
     which is why Linux NFS prevents it by default.  See
     <A
HREF="#SECURITY"
>Section 6</A
> for details.  
   </P
><P
>     If you have root squashing, you want to keep it, and you're only 
     trying to get root to have the same permissions on the file that
     the user <EM
>nobody</EM
> should have, then remember that it is the server
     that determines which uid root gets mapped to.  By default, the
     server uses the <EM
>UID</EM
> and <EM
>GID</EM
> of 
     <EM
>nobody</EM
> in the <TT
CLASS="FILENAME"
>/etc/passwd</TT
> file,
     but this can also be overridden with the <KBD
CLASS="USERINPUT"
>anonuid</KBD
> and 
     <KBD
CLASS="USERINPUT"
>anongid</KBD
> options in the <TT
CLASS="FILENAME"
>/etc/exports</TT
> 
     file.  Make sure that the client and the server agree about which 
     <EM
>UID</EM
> <EM
>nobody</EM
> gets mapped to.
   </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SYMPTOM5"
>7.5. When I transfer really big files, NFS takes over
   all the CPU cycles on the server and it screeches to a halt.</A
></H3
><P
>   This is a problem with the <CODE
CLASS="FUNCTION"
>fsync()</CODE
> function in 2.2 kernels that
   causes all sync-to-disk requests to be cumulative, resulting
   in a write time that is quadratic in the file size.  If you
   can, upgrading to a 2.4 kernel should solve the problem.
   Also, exporting with the <KBD
CLASS="USERINPUT"
>no_wdelay</KBD
> option 
   forces the program to use <CODE
CLASS="FUNCTION"
>o_sync()</CODE
> instead, which may prove faster.
  </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SYMPTOM6"
>7.6. Strange error or log messages</A
></H3
><P
>  <P
></P
><OL
TYPE="a"
><LI
><P
>  Messages of the following format:
  </P
><P
>  <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
> Jan 7 09:15:29 server kernel: fh_verify: mail/guest permission failure, acc=4, error=13
 Jan 7 09:23:51 server kernel: fh_verify: ekonomi/test permission failure, acc=4, error=13
  </PRE
></FONT
></TD
></TR
></TABLE
>
  </P
><P
>   These happen when a NFS <SAMP
CLASS="COMPUTEROUTPUT"
>setattr</SAMP
>
   operation is attempted on a 
   file you don't have write access to. The messages are 
   harmless.
  </P
></LI
><LI
><P
> The following messages frequently appear in the logs:
 </P
><P
> <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
> kernel: nfs: server server.domain.name not responding, still trying
 kernel: nfs: task 10754 can't get a request slot
 kernel: nfs: server server.domain.name OK
 </PRE
></FONT
></TD
></TR
></TABLE
>
 </P
><P
>   The "can't get a request slot" message means that the client-side
   RPC code has detected a lot of timeouts (perhaps due to 
   network congestion, perhaps due to an overloaded server), and
   is throttling back the number of concurrent outstanding 
   requests in an attempt to lighten the load.  The cause of
   these messages is basically sluggish performance.  See
   <A
HREF="#PERFORMANCE"
>Section 5</A
> for details.
 </P
></LI
><LI
><P
> After mounting, the following message appears on the client:
  </P
><P
>  <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>nfs warning: mount version older than kernel
  </PRE
></FONT
></TD
></TR
></TABLE
>
  </P
><P
>    It means what it says: You should upgrade your mount package and/or 
    am-utils. (If for some reason upgrading is a problem, you may be able
    to get away with just recompiling them so that the newer kernel features
    are recognized at compile time).
  </P
></LI
><LI
><P
>    Errors in startup/shutdown log for <B
CLASS="COMMAND"
>lockd</B
>
  </P
><P
>  You may see a message of the following kind in your boot log:
 <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>nfslock: rpc.lockd startup failed
 </PRE
></FONT
></TD
></TR
></TABLE
>
 </P
><P
>   They are harmless.  Older versions of <B
CLASS="COMMAND"
>rpc.lockd</B
> needed to be
    started up manually, but newer versions are started automatically
    by <B
CLASS="COMMAND"
>nfsd</B
>.  Many of the 
    default startup scripts still try to start
    up <B
CLASS="COMMAND"
>lockd</B
> by hand, in case 
    it is necessary.  You can alter your
    startup scripts if you want the messages to go away.
 </P
></LI
><LI
><P
>   The following message appears in the logs:
   </P
><P
>   <TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>kmem_create: forcing size word alignment - nfs_fh
   </PRE
></FONT
></TD
></TR
></TABLE
>
   </P
><P
>     This results from the file handle being 16 bits instead of a
     mulitple of 32 bits, which makes the kernel grimace.  It is 
     harmless.
   </P
></LI
></OL
>
 </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SYMPTOM7"
>7.7. Real permissions don't match what's in <TT
CLASS="FILENAME"
>/etc/exports</TT
>.</A
></H3
><P
>  <TT
CLASS="FILENAME"
>/etc/exports</TT
> is <EM
>very</EM
> sensitive to whitespace - so the 
  following statements are not the same:
 <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>/export/dir hostname(rw,no_root_squash) 
/export/dir hostname (rw,no_root_squash) 
 </PRE
></FONT
></TD
></TR
></TABLE
>
  The first will grant <KBD
CLASS="USERINPUT"
>hostname rw</KBD
>
  access to <TT
CLASS="FILENAME"
>/export/dir</TT
>
  without squashing root privileges. The second will grant 
  <KBD
CLASS="USERINPUT"
>hostname rw</KBD
> privileges with 
  <KBD
CLASS="USERINPUT"
>root squash</KBD
> and it will grant 
  <EM
>everyone</EM
> else read/write access, without 
  squashing root privileges. Nice huh? 
 </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SYMPTOM8"
>7.8. Flaky and unreliable behavior</A
></H3
><P
>    Simple commands such as <B
CLASS="COMMAND"
>ls</B
> work, but anything that transfers
    a large amount of information causes the mount point to lock.
  </P
><P
>    This could be one of two problems:
  </P
><P
></P
><OL
TYPE="i"
><LI
><P
>  
      It will happen if you have ipchains on at the server and/or the 
      client and you are not allowing fragmented packets through the
      chains.  Allow fragments from the remote host and you'll be able 
      to function again. See <A
HREF="#FIREWALLS"
>Section 6.4</A
> for details on how to do this.
    </P
></LI
><LI
><P
>      You may be using a larger <KBD
CLASS="USERINPUT"
>rsize</KBD
> 
      and <KBD
CLASS="USERINPUT"
>wsize</KBD
> in your mount options
      than the server supports.  Try reducing <KBD
CLASS="USERINPUT"
>rsize</KBD
>
      and <KBD
CLASS="USERINPUT"
>wsize</KBD
> to 1024 and 
      seeing if the problem goes away.  If it does, then increase them 
      slowly to a more reasonable value.
    </P
></LI
></OL
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SYMPTOM9"
>7.9. nfsd won't start</A
></H3
><P
>     Check the file <TT
CLASS="FILENAME"
>/etc/exports</TT
> and make sure root has read permission.
     Check the binaries and make sure they are executable.  Make sure
     your kernel was compiled with NFS server support.  You may need
     to reinstall your binaries if none of these ideas helps.
   </P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SYMPTOM10"
>7.10. File Corruption When Using Multiple Clients</A
></H3
><P
>     If a file has been modified within one second of its 
     previous modification and left the same size, it will 
     continue to generate the same inode number.  Because
     of this, constant reads and writes to a file by
     multiple clients may cause file corruption.  Fixing
     this bug requires changes deep within the filesystem
     layer, and therefore it is a 2.5 item.
   </P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="INTEROP"
>8. Using Linux NFS with Other OSes</A
></H2
><P
>   Every operating system, Linux included, has quirks and deviations
   in the behavior of its NFS implementation -- sometimes because
   the protocols are vague, sometimes because they leave gaping
   security holes.  Linux will work properly with all major vendors'
   NFS implementations, as far as we know.  However, there may be
   extra steps involved to make sure the two OSes are communicating
   clearly with one another.  This section details those steps.
 </P
><P
>   In general, it is highly ill-advised to attempt to use a Linux
   machine with a kernel before 2.2.18 as an NFS server for non-Linux
   clients.  Implementations with older kernels may work fine as 
   clients; however if you are using one of these kernels and get
   stuck, the first piece of advice we would give is to upgrade
   your kernel and see if the problems go away.  The user-space NFS
   implementations also do not work well with non-Linux clients.
 </P
><P
>   Following is a list of known issues for using Linux together with
   major operating systems.
 </P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="AIX"
>8.1. AIX</A
></H3
><DIV
CLASS="SECT3"
><H4
CLASS="SECT3"
><A
NAME="AIXSERVER"
>8.1.1. Linux Clients and AIX Servers</A
></H4
><P
>      The format for the <TT
CLASS="FILENAME"
>/etc/exports</TT
> file for our example in <A
HREF="#SERVER"
>Section 3</A
> is:
    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  /usr   slave1.foo.com:slave2.foo.com,access=slave1.foo.com:slave2.foo.com
  /home  slave1.foo.com:slave2.foo.com,rw=slave1.foo.com:slave2.foo.com
    </PRE
></FONT
></TD
></TR
></TABLE
>
  </P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="AIXCLIENTS"
>8.1.2. AIX clients and Linux Servers</A
></H4
><P
>     AIX uses the file <TT
CLASS="FILENAME"
>/etc/filesystems</TT
> instead of <TT
CLASS="FILENAME"
>/etc/fstab</TT
>.
     A sample entry, based on the example in <A
HREF="#CLIENT"
>Section 4</A
>, looks like this:
   <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>/mnt/home:
        dev             = "/home"
        vfs             = nfs
        nodename        = master.foo.com
        mount           = true
        options         = bg,hard,intr,rsize=1024,wsize=1024,vers=2,proto=udp
        account         = false
  </PRE
></FONT
></TD
></TR
></TABLE
>
  </P
><P
>   <P
></P
><OL
TYPE="i"
><LI
><P
>       Version 4.3.2 of AIX, and possibly earlier versions as well,
       requires that file systems be exported with
       the <KBD
CLASS="USERINPUT"
>insecure</KBD
> option, which 
       causes NFS to listen to requests from 
       insecure ports (i.e., ports above 1024, to which non-root users can 
       bind).  Older versions of AIX do not seem to require this.
     </P
></LI
><LI
><P
>       AIX clients will default to mounting version 3 NFS over TCP.
       If your Linux server does not support this, then you may need
       to specify <KBD
CLASS="USERINPUT"
>vers=2</KBD
> and/or 
       <KBD
CLASS="USERINPUT"
>proto=udp</KBD
> in your mount options.
     </P
></LI
><LI
><P
>       Using netmasks in <TT
CLASS="FILENAME"
>/etc/exports</TT
>
       seems to sometimes cause clients
       to lose mounts when another client is reset.  This can be fixed
       by listing out hosts explicitly.
     </P
></LI
><LI
><P
>       Apparently automount in AIX 4.3.2 is rather broken.
     </P
></LI
></OL
>
 </P
></DIV
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="BSD"
>8.2. BSD</A
></H3
><DIV
CLASS="SECT3"
><H4
CLASS="SECT3"
><A
NAME="BSDSERVER"
>8.2.1. BSD servers and Linux clients</A
></H4
><P
>       BSD kernels tend to work better with larger block sizes.
     </P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="BSDCLIENT"
>8.2.2. Linux servers and BSD clients</A
></H4
><P
>       Some versions of BSD may make requests to the server from insecure ports,
       in which case you will need to export your volumes with the 
       <KBD
CLASS="USERINPUT"
>insecure</KBD
>
       option.  See the man page for <EM
>exports(5)</EM
> 
       for more details.
     </P
></DIV
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="TRU64"
>8.3. Tru64 Unix</A
></H3
><DIV
CLASS="SECT3"
><H4
CLASS="SECT3"
><A
NAME="TRU64SERVER"
>8.3.1. Tru64 Unix Servers and Linux Clients</A
></H4
><P
>      In general, Tru64 Unix servers work quite smoothly with Linux clients.
      The format for the <TT
CLASS="FILENAME"
>/etc/exports</TT
> file for our example in <A
HREF="#SERVER"
>Section 3</A
> is:
   <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>     
/usr         slave1.foo.com:slave2.foo.com \
     -access=slave1.foo.com:slave2.foo.com \

/home        slave1.foo.com:slave2.foo.com \
         -rw=slave1.foo.com:slave2.foo.com \
       -root=slave1.foo.com:slave2.foo.com 
   </PRE
></FONT
></TD
></TR
></TABLE
>
   </P
><P
> (The <KBD
CLASS="USERINPUT"
>root</KBD
> option is listed in the last 
 entry for informational purposes only; its use is not recommended
 unless necessary.)
     </P
><P
>     Tru64 checks the <TT
CLASS="FILENAME"
>/etc/exports</TT
> file every time there is a mount request
     so you do not need to run the <B
CLASS="COMMAND"
>exportfs</B
> command; in fact on many
     versions of Tru64 Unix the command does not exist.
    </P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="TRU64CLIENT"
>8.3.2. Linux Servers and Tru64 Unix Clients</A
></H4
><P
>      There are two issues to watch out for here.  First, Tru64 Unix mounts
      using Version 3 NFS by default.  You will see mount errors if your
      Linux server does not support Version 3 NFS.  Second, in Tru64 Unix
      4.x, NFS locking requests are made by 
      <SAMP
CLASS="COMPUTEROUTPUT"
>daemon</SAMP
>.  You will therefore
      need to specify the <KBD
CLASS="USERINPUT"
>insecure_locks</KBD
> option on all volumes you export
      to a Tru64 Unix 4.x client; see the <B
CLASS="COMMAND"
>exports</B
> man pages for details.
    </P
></DIV
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="HPUX"
>8.4. HP-UX</A
></H3
><DIV
CLASS="SECT3"
><H4
CLASS="SECT3"
><A
NAME="HPUXSERVER"
>8.4.1. HP-UX Servers and Linux Clients</A
></H4
><P
>       A sample <TT
CLASS="FILENAME"
>/etc/exports</TT
> entry on HP-UX looks like this:
    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>/usr -ro,access=slave1.foo.com:slave2.foo.com
/home -rw=slave1.foo.com:slave2.fo.com:root=slave1.foo.com:slave2.foo.com
    </PRE
></FONT
></TD
></TR
></TABLE
>
       (The <KBD
CLASS="USERINPUT"
>root</KBD
> option is listed in the last entry for informational
       purposes only; its use is not recommended unless necessary.)
     </P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="HPUXCLIENT"
>8.4.2. Linux Servers and HP-UX Clients</A
></H4
><P
>     HP-UX diskless clients will require at least a kernel version 2.2.19
     (or patched 2.2.18) for device files to export correctly.  Also, any
     exports to an HP-UX client will need to be exported with the 
     <KBD
CLASS="USERINPUT"
>insecure_locks</KBD
> option.
   </P
></DIV
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="IRIX"
>8.5. IRIX</A
></H3
><DIV
CLASS="SECT3"
><H4
CLASS="SECT3"
><A
NAME="IRIXSERVER"
>8.5.1. IRIX Servers and Linux Clients</A
></H4
><P
>    A sample <TT
CLASS="FILENAME"
>/etc/exports</TT
> entry on IRIX looks like this:
  <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>/usr -ro,access=slave1.foo.com:slave2.foo.com
/home -rw=slave1.foo.com:slave2.fo.com:root=slave1.foo.com:slave2.foo.com
  </PRE
></FONT
></TD
></TR
></TABLE
>
    (The <KBD
CLASS="USERINPUT"
>root</KBD
> option is listed in the last entry for informational
    purposes only; its use is not recommended unless necessary.)
  </P
><P
>   There are reportedly problems when using the nohide option on
   exports to linux 2.2-based systems.  This problem is fixed in the
   2.4 kernel.  As a workaround, you can export and mount lower-down
   file systems separately.
  </P
><P
>   As of Kernel 2.4.17, there continue to be several minor interoperability
   issues that may require a kernel upgrade.  In particular:
  <P
></P
><UL
><LI
><P
>      Make sure that Trond Myklebust's <SPAN
CLASS="APPLICATION"
>seekdir</SPAN
> 
      (or <SPAN
CLASS="APPLICATION"
>dir</SPAN
>) kernel patch is applied.
      The latest version (for 2.4.17) is located at:
     </P
><P
>      <A
HREF="http://www.fys.uio.no/~trondmy/src/2.4.17/linux-2.4.17-seekdir.dif"
TARGET="_top"
>      http://www.fys.uio.no/~trondmy/src/2.4.17/linux-2.4.17-seekdir.dif</A
>
     </P
></LI
><LI
><P
>      IRIX servers do not always use the same 
      <SAMP
CLASS="COMPUTEROUTPUT"
>fsid</SAMP
> attribute field across
      reboots, which results in <SAMP
CLASS="COMPUTEROUTPUT"
>inode number mismatch</SAMP
> 
      errors on a Linux
      client if the mounted IRIX server reboots. A patch is available from:
     </P
><P
><A
HREF="http://www.geocrawler.com/lists/3/SourceForge/789/0/7777454/"
TARGET="_top"
>      http://www.geocrawler.com/lists/3/SourceForge/789/0/7777454/</A
>
     </P
></LI
><LI
><P
>      Linux kernels v2.4.9 and above have problems reading large directories
      (hundreds of files) from exported IRIX XFS file systems that were made
      with <KBD
CLASS="USERINPUT"
>naming version=1</KBD
>.  
      The reason for the problem can be found at:
     </P
><P
>      <A
HREF="http://www.geocrawler.com/archives/3/789/2001/9/100/6531172/"
TARGET="_top"
>       http://www.geocrawler.com/archives/3/789/2001/9/100/6531172/</A
>
     </P
><P
>       The naming version can be found by using (on the IRIX server):
     </P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>	xfs_growfs -n mount_point
     </PRE
></FONT
></TD
></TR
></TABLE
><P
> 
      The workaround is to export these file systems using the 
      <KBD
CLASS="USERINPUT"
>-32bitclients</KBD
>
      option in the <TT
CLASS="FILENAME"
>/etc/exports</TT
> file.
      The fix is to convert the file system to 'naming version=2'.
      Unfortunately the only way to do this is by a 
      <KBD
CLASS="USERINPUT"
>backup</KBD
>/<KBD
CLASS="USERINPUT"
>mkfs</KBD
>/<KBD
CLASS="USERINPUT"
>restore</KBD
>.
      </P
><P
>       <B
CLASS="COMMAND"
>mkfs_xfs</B
> on IRIX 6.5.14 (and above) 
       creates <KBD
CLASS="USERINPUT"
>naming version=2</KBD
> XFS file
       systems by default. On IRIX 6.5.5 to 6.5.13, use:
      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>	mkfs_xfs -n version=2 device
      </PRE
></FONT
></TD
></TR
></TABLE
>
      </P
><P
>        Versions of IRIX prior to 6.5.5 do not support 
        <KBD
CLASS="USERINPUT"
>naming version=2</KBD
> XFS file systems.
      </P
></LI
></UL
>
   </P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="IRIXCLIENT"
>8.5.2. IRIX clients and Linux servers</A
></H4
><P
>   Irix versions up to 6.5.12 have problems mounting file systems exported
   from Linux boxes - the mount point "gets lost," e.g.,
  <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>	# mount linux:/disk1 /mnt
	# cd /mnt/xyz/abc
	# pwd
	/xyz/abc
   </PRE
></FONT
></TD
></TR
></TABLE
>
   </P
><P
>    This is known IRIX bug (SGI bug 815265 - IRIX not liking file handles of
    less than 32 bytes), which is fixed in <SPAN
CLASS="APPLICATION"
>IRIX 6.5.13</SPAN
>.
    If it is not possible 
    to upgrade to <SPAN
CLASS="APPLICATION"
>IRIX 6.5.13</SPAN
>, then the unofficial 
    workaround is to force the Linux <B
CLASS="COMMAND"
>nfsd</B
> 
    to always use 32 byte file handles. 
   </P
><P
>    A number of patches exist - see:
   <P
></P
><UL
><LI
><P
>    <A
HREF="http://www.geocrawler.com/archives/3/789/2001/8/50/6371896/"
TARGET="_top"
>    http://www.geocrawler.com/archives/3/789/2001/8/50/6371896/</A
>
   </P
></LI
><LI
><P
>   <A
HREF="http://oss.sgi.com/projects/xfs/mail_archive/0110/msg00006.html"
TARGET="_top"
>    http://oss.sgi.com/projects/xfs/mail_archive/0110/msg00006.html</A
>
   </P
></LI
></UL
>
  </P
></DIV
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SOLARIS"
>8.6. Solaris</A
></H3
><DIV
CLASS="SECT3"
><H4
CLASS="SECT3"
><A
NAME="SOLARISSERVER"
>8.6.1. Solaris Servers</A
></H4
><P
>     Solaris has a slightly different format on the server end from
     other operating systems.  Instead of 
     <TT
CLASS="FILENAME"
>/etc/exports</TT
>, the configuration
     file is <TT
CLASS="FILENAME"
>/etc/dfs/dfstab</TT
>.  Entries are of 
     the form of a <B
CLASS="COMMAND"
>share</B
> command, where the syntax
     for the example in <A
HREF="#SERVER"
>Section 3</A
> would look like
   <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>share -o rw=slave1,slave2 -d "Master Usr" /usr
   </PRE
></FONT
></TD
></TR
></TABLE
>
     and instead of running <B
CLASS="COMMAND"
>exportfs</B
> after editing, you run <B
CLASS="COMMAND"
>shareall</B
>.
   </P
><P
>     Solaris servers are especially sensitive to packet size. If you
     are using a Linux client with a Solaris server, be sure to set
     <KBD
CLASS="USERINPUT"
>rsize</KBD
> and <KBD
CLASS="USERINPUT"
>wsize</KBD
>
     to 32768 at mount time.
   </P
><P
>     Finally, there is an issue with root squashing on Solaris: root gets
     mapped to the user <SAMP
CLASS="COMPUTEROUTPUT"
>noone</SAMP
>, which 
     is not the same as the user <SAMP
CLASS="COMPUTEROUTPUT"
>nobody</SAMP
>.
     If you are having trouble with file permissions as root on the client
     machine, be sure to check that the mapping works as you expect.
   </P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="SOLARISCLIENT"
>8.6.2. Solaris Clients</A
></H4
><P
>     Solaris clients will regularly produce the following message:
    </P
><TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>svc: unknown program 100227 (me 100003)
  </PRE
></FONT
></TD
></TR
></TABLE
><P
>    This happens because Solaris clients, when they mount, try to obtain
    ACL information - which Linux obviously does not have. The messages
    can safely be ignored. 
  </P
><P
>    There are two known issues with diskless Solaris clients: First, a kernel
    version of at least 2.2.19 is needed to get <TT
CLASS="FILENAME"
>/dev/null</TT
> to export 
    correctly.  Second, the packet size may need to be set extremely
    small (i.e., 1024) on diskless sparc clients because the clients
    do not know how to assemble packets in reverse order.  This can be
    done from <TT
CLASS="FILENAME"
>/etc/bootparams</TT
> on the clients.
   </P
></DIV
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SUNOS"
>8.7. SunOS</A
></H3
><P
>    SunOS only has NFS Version 2 over UDP.
  </P
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="SUNOSSERVER"
>8.7.1. SunOS Servers</A
></H4
><P
>     On the server end, SunOS uses the most traditional format for its 
     <TT
CLASS="FILENAME"
>/etc/exports</TT
> file. The example in <A
HREF="#SERVER"
>Section 3</A
> would look like:
   <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>/usr    -access=slave1.foo.com,slave2.foo.com
/home   -rw=slave1.foo.com,slave2.foo.com, root=slave1.foo.com,slave2.foo.com
   </PRE
></FONT
></TD
></TR
></TABLE
>
  </P
><P
>   Again, the <KBD
CLASS="USERINPUT"
>root</KBD
> option is listed for informational
   purposes and is not recommended unless necessary.
  </P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="SUNOSCLIENT"
>8.7.2. SunOS Clients</A
></H4
><P
>    Be advised that SunOS makes all NFS locking requests 
    as <SAMP
CLASS="COMPUTEROUTPUT"
>daemon</SAMP
>, and 
    therefore you will need to add the <KBD
CLASS="USERINPUT"
>insecure_locks</KBD
> option to any 
    volumes you export to a SunOS machine.  See the <B
CLASS="COMMAND"
>exports</B
> man page
    for details.
  </P
></DIV
></DIV
></DIV
></DIV
></BODY
></HTML
>