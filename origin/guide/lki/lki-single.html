<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<HTML>
<HEAD>
 <META NAME="GENERATOR" CONTENT="LinuxDoc-Tools 0.9.72">
 <TITLE>Linux Kernel 2.4 Internals</TITLE>
</HEAD>
<BODY>
<H1>Linux Kernel 2.4 Internals</H1>

<H2>Tigran Aivazian <CODE>tigran@veritas.com</CODE></H2>7 August 2002 (29 Av 6001)
<HR>
<EM>Introduction to the Linux 2.4 kernel. The latest copy of this document
can be always downloaded from:
<A HREF="http://www.moses.uklinux.net/patches/lki.sgml">http://www.moses.uklinux.net/patches/lki.sgml</A>
This guide is now part of the Linux Documentation Project and can also be
downloaded in various formats from:
<A HREF="http://www.linuxdoc.org/guides.html">http://www.linuxdoc.org/guides.html</A>
or can be read online (latest version) at:
<A HREF="http://www.moses.uklinux.net/patches/lki.html">http://www.moses.uklinux.net/patches/lki.html</A>
This documentation is free software; you can redistribute
it and/or modify it under the terms of the GNU General Public
License as published by the Free Software Foundation; either
version 2 of the License, or (at your option) any later version.
The author is working as senior Linux kernel engineer at VERITAS Software
Ltd and wrote this book for the purpose of supporting the short training
course/lectures he gave on this subject, internally at VERITAS.
Thanks to Juan J. Quintela <CODE>(quintela@fi.udc.es)</CODE>,
Francis Galiegue <CODE>(fg@mandrakesoft.com)</CODE>,
Hakjun Mun <CODE>(juniorm@orgio.net)</CODE>,
Matt Kraai <CODE>(kraai@alumni.carnegiemellon.edu)</CODE>,
Nicholas Dronen <CODE>(ndronen@frii.com)</CODE>,
Samuel S Chessman <CODE>(chessman@tux.org)</CODE>,
Nadeem Hasan <CODE>(nhasan@nadmm.com)</CODE>,
Michael Svetlik <CODE>(m.svetlik@ssi-schaefer-peem.com)</CODE>
for various corrections and suggestions.
The Linux Page Cache chapter was written by: Christoph Hellwig <CODE>(hch@caldera.de)</CODE>.
The IPC Mechanisms chapter was written by: Russell Weight <CODE>(weightr@us.ibm.com)</CODE> and Mingming Cao <CODE>(mcao@us.ibm.com)</CODE></EM>
<HR>
<H2><A NAME="s1">1. Booting</A></H2>


<H2><A NAME="ss1.1">1.1 Building the Linux Kernel Image</A>
</H2>

<P>This section explains the steps taken during compilation of the Linux kernel
and the output produced at each stage.
The build process depends on the architecture so I would like to emphasize
that we only consider building a Linux/x86 kernel.</P>
<P>When the user types 'make zImage' or 'make bzImage' the resulting bootable
kernel image is stored as
<CODE>arch/i386/boot/zImage</CODE> or
<CODE>arch/i386/boot/bzImage</CODE> respectively.
Here is how the image is built:
<OL>
<LI> C and assembly source files are compiled into ELF relocatable object format (.o) and
some of them are grouped logically into archives (.a) using
<B>ar(1)</B>.
</LI>
<LI> Using <B>ld(1)</B>, the above .o and .a are linked into <CODE>vmlinux</CODE> which is a
statically linked, non-stripped ELF 32-bit LSB 80386 executable file.
</LI>
<LI> <CODE>System.map</CODE> is produced by <B>nm vmlinux</B>, irrelevant or uninteresting
symbols are grepped out.
</LI>
<LI> Enter directory <CODE>arch/i386/boot</CODE>.
</LI>
<LI> Bootsector asm code <CODE>bootsect.S</CODE> is preprocessed either with or without
<B>-D__BIG_KERNEL__</B>, depending on whether the target is
bzImage or zImage, into <CODE>bbootsect.s</CODE> or <CODE>bootsect.s</CODE> respectively.
</LI>
<LI> <CODE>bbootsect.s</CODE> is assembled and then converted into 'raw binary' form
called <CODE>bbootsect</CODE> (or <CODE>bootsect.s</CODE> assembled and raw-converted into
<CODE>bootsect</CODE> for zImage).
</LI>
<LI> Setup code <CODE>setup.S</CODE> (<CODE>setup.S</CODE> includes <CODE>video.S</CODE>) is preprocessed into
<CODE>bsetup.s</CODE> for bzImage or <CODE>setup.s</CODE> for zImage. In the same way as the
bootsector code, the difference is marked by -<B>D__BIG_KERNEL__</B> present
for bzImage.  The result is then converted into 'raw binary' form
called <CODE>bsetup</CODE>.
</LI>
<LI> Enter directory <CODE>arch/i386/boot/compressed</CODE> and convert 
<CODE>/usr/src/linux/vmlinux</CODE> to $tmppiggy (tmp filename) in raw binary
format, removing <CODE>.note</CODE> and <CODE>.comment</CODE> ELF sections.
</LI>
<LI> <B>gzip -9 &lt; $tmppiggy > $tmppiggy.gz</B>
</LI>
<LI> Link $tmppiggy.gz into ELF relocatable (<B>ld -r</B>) <CODE>piggy.o</CODE>.
</LI>
<LI> Compile compression routines <CODE>head.S</CODE> and <CODE>misc.c</CODE> (still in 
<CODE>arch/i386/boot/compressed</CODE> directory) into ELF objects <CODE>head.o</CODE> and
<CODE>misc.o</CODE>.
</LI>
<LI> Link together <CODE>head.o</CODE>, <CODE>misc.o</CODE> and <CODE>piggy.o</CODE> into <CODE>bvmlinux</CODE> (or <CODE>vmlinux</CODE> for
zImage, don't mistake this for <CODE>/usr/src/linux/vmlinux</CODE>!). Note the
difference between <B>-Ttext 0x1000</B> used for <CODE>vmlinux</CODE> and <B>-Ttext 0x100000</B>
for <CODE>bvmlinux</CODE>, i.e. for bzImage compression loader is high-loaded.
</LI>
<LI> Convert <CODE>bvmlinux</CODE> to 'raw binary' <CODE>bvmlinux.out</CODE> removing <CODE>.note</CODE> and 
<CODE>.comment</CODE> ELF sections.
</LI>
<LI> Go back to <CODE>arch/i386/boot</CODE> directory and, using the program <B>tools/build</B>,
cat together <CODE>bbootsect</CODE>, <CODE>bsetup</CODE> and <CODE>compressed/bvmlinux.out</CODE> into <CODE>bzImage</CODE>
(delete extra 'b' above for <CODE>zImage</CODE>). This writes important variables
like <CODE>setup_sects</CODE> and <CODE>root_dev</CODE> at the end of the bootsector.</LI>
</OL>

The size of the bootsector is always 512 bytes. The size of the setup must
be greater than 4 sectors but is limited above by about 12K - the rule
is:</P>
<P>0x4000 bytes >= 512 + setup_sects * 512 + room for stack while running 
bootsector/setup</P>
<P>We will see later where this limitation comes from.</P>
<P>The upper limit on the bzImage size produced at this step is about 2.5M for
booting with LILO and 0xFFFF paragraphs (0xFFFF0 = 1048560 bytes) for
booting raw image, e.g. from floppy disk or CD-ROM (El-Torito emulation mode).</P>
<P>Note that while <B>tools/build</B> does validate the size of boot sector, kernel image
and lower bound of setup size, it does not check the *upper* bound of said
setup size. Therefore it is easy to build a broken kernel by just adding some
large ".space" at the end of <CODE>setup.S</CODE>.</P>

<H2><A NAME="ss1.2">1.2 Booting: Overview</A>
</H2>


<P>The boot process details are architecture-specific, so we shall
focus our attention on the IBM PC/IA32 architecture.
Due to old design and backward compatibility, the PC firmware boots the
operating system in an old-fashioned manner.
This process can be separated into the following six logical stages:</P>
<P>
<OL>
<LI> BIOS selects the boot device.</LI>
<LI> BIOS loads the bootsector from the boot device.</LI>
<LI> Bootsector loads setup, decompression routines and compressed kernel
image.</LI>
<LI> The kernel is uncompressed in protected mode.</LI>
<LI> Low-level initialisation is performed by asm code.</LI>
<LI> High-level C initialisation.</LI>
</OL>
</P>

<H2><A NAME="ss1.3">1.3 Booting: BIOS POST</A>
</H2>


<P>
<OL>
<LI> The power supply starts the clock generator and asserts #POWERGOOD
signal on the bus.</LI>
<LI> CPU #RESET line is asserted (CPU now in real 8086 mode).</LI>
<LI> %ds=%es=%fs=%gs=%ss=0, %cs=0xFFFF0000,%eip = 0x0000FFF0 (ROM BIOS POST code).</LI>
<LI> All POST checks are performed with interrupts disabled.</LI>
<LI> IVT (Interrupt Vector Table) initialised at address 0.</LI>
<LI> The BIOS Bootstrap Loader function is invoked via <B>int 0x19</B>,
with %dl containing the boot device 'drive number'. This loads 
track 0, sector 1 at physical address 0x7C00 (0x07C0:0000).</LI>
</OL>
</P>

<H2><A NAME="ss1.4">1.4 Booting: bootsector and setup</A>
</H2>


<P>The bootsector used to boot Linux kernel could be either:</P>
<P>
<UL>
<LI> Linux bootsector (<CODE>arch/i386/boot/bootsect.S</CODE>),</LI>
<LI> LILO (or other bootloader's) bootsector, or</LI>
<LI> no bootsector (loadlin etc)</LI>
</UL>
</P>
<P>We consider here the Linux bootsector in detail.
The first few lines initialise the convenience macros to be used for segment
values:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
29 SETUPSECS = 4                /* default nr of setup-sectors */
30 BOOTSEG   = 0x07C0           /* original address of boot-sector */
31 INITSEG   = DEF_INITSEG      /* we move boot here - out of the way */
32 SETUPSEG  = DEF_SETUPSEG     /* setup starts here */
33 SYSSEG    = DEF_SYSSEG       /* system loaded at 0x10000 (65536) */
34 SYSSIZE   = DEF_SYSSIZE      /* system size: # of 16-byte clicks */
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>(the numbers on the left are the line numbers of bootsect.S file)
The values of <CODE>DEF_INITSEG</CODE>, <CODE>DEF_SETUPSEG</CODE>, <CODE>DEF_SYSSEG</CODE> and <CODE>DEF_SYSSIZE</CODE> are taken
from <CODE>include/asm/boot.h</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/* Don't touch these, unless you really know what you're doing. */
#define DEF_INITSEG     0x9000
#define DEF_SYSSEG      0x1000
#define DEF_SETUPSEG    0x9020
#define DEF_SYSSIZE     0x7F00
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>Now, let us consider the actual code of <CODE>bootsect.S</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
    54          movw    $BOOTSEG, %ax
    55          movw    %ax, %ds
    56          movw    $INITSEG, %ax
    57          movw    %ax, %es
    58          movw    $256, %cx
    59          subw    %si, %si
    60          subw    %di, %di
    61          cld
    62          rep
    63          movsw
    64          ljmp    $INITSEG, $go
       
    65  # bde - changed 0xff00 to 0x4000 to use debugger at 0x6400 up (bde).  We
    66  # wouldn't have to worry about this if we checked the top of memory.  Also
    67  # my BIOS can be configured to put the wini drive tables in high memory
    68  # instead of in the vector table.  The old stack might have clobbered the
    69  # drive table.
       
    70  go:     movw    $0x4000-12, %di         # 0x4000 is an arbitrary value >=
    71                                          # length of bootsect + length of
    72                                          # setup + room for stack;
    73                                          # 12 is disk parm size.
    74          movw    %ax, %ds                # ax and es already contain INITSEG
    75          movw    %ax, %ss
    76          movw    %di, %sp                # put stack at INITSEG:0x4000-12.
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>Lines 54-63 move the bootsector code from address 0x7C00 to 0x90000.
This is achieved by:</P>
<P>
<OL>
<LI> set %ds:%si to $BOOTSEG:0 (0x7C0:0 = 0x7C00)
</LI>
<LI> set %es:%di to $INITSEG:0 (0x9000:0 = 0x90000)
</LI>
<LI> set the number of 16bit words in %cx (256 words = 512 bytes = 1 sector)
</LI>
<LI> clear DF (direction) flag in EFLAGS to auto-increment addresses (cld)
</LI>
<LI> go ahead and copy 512 bytes (rep movsw)</LI>
</OL>
</P>
<P>The reason this code does not use <CODE>rep movsd</CODE> is intentional (hint - .code16).</P>
<P>Line 64 jumps to label <CODE>go:</CODE> in the newly made copy of the
bootsector, i.e. in segment 0x9000. This and the following three
instructions (lines 64-76) prepare the stack at $INITSEG:0x4000-0xC, i.e. 
%ss = $INITSEG (0x9000) and %sp = 0x3FF4 (0x4000-0xC). This is where the
limit on setup size comes from that we mentioned earlier (see Building the
Linux Kernel Image).</P>
<P>Lines 77-103 patch the disk parameter table for the first disk to
allow multi-sector reads:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
    77  # Many BIOS's default disk parameter tables will not recognise
    78  # multi-sector reads beyond the maximum sector number specified
    79  # in the default diskette parameter tables - this may mean 7
    80  # sectors in some cases.
    81  #
    82  # Since single sector reads are slow and out of the question,
    83  # we must take care of this by creating new parameter tables
    84  # (for the first disk) in RAM.  We will set the maximum sector
    85  # count to 36 - the most we will encounter on an ED 2.88.  
    86  #
    87  # High doesn't hurt.  Low does.
    88  #
    89  # Segments are as follows: ds = es = ss = cs - INITSEG, fs = 0,
    90  # and gs is unused.
       
    91          movw    %cx, %fs                # set fs to 0
    92          movw    $0x78, %bx              # fs:bx is parameter table address
    93          pushw   %ds
    94          ldsw    %fs:(%bx), %si          # ds:si is source
    95          movb    $6, %cl                 # copy 12 bytes
    96          pushw   %di                     # di = 0x4000-12.
    97          rep                             # don't need cld -> done on line 66
    98          movsw
    99          popw    %di
   100          popw    %ds
   101          movb    $36, 0x4(%di)           # patch sector count
   102          movw    %di, %fs:(%bx)
   103          movw    %es, %fs:2(%bx)
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The floppy disk controller is reset using BIOS service int 0x13 function 0 
(reset FDC) and setup sectors are loaded immediately after the 
bootsector, i.e. at physical address 0x90200 ($INITSEG:0x200), again using
BIOS service int 0x13, function 2 (read sector(s)).
This happens during lines 107-124:
<BLOCKQUOTE><CODE>
<HR>
<PRE>
   107  load_setup:
   108          xorb    %ah, %ah                # reset FDC 
   109          xorb    %dl, %dl
   110          int     $0x13   
   111          xorw    %dx, %dx                # drive 0, head 0
   112          movb    $0x02, %cl              # sector 2, track 0
   113          movw    $0x0200, %bx            # address = 512, in INITSEG
   114          movb    $0x02, %ah              # service 2, "read sector(s)"
   115          movb    setup_sects, %al        # (assume all on head 0, track 0)
   116          int     $0x13                   # read it
   117          jnc     ok_load_setup           # ok - continue
       
   118          pushw   %ax                     # dump error code
   119          call    print_nl
   120          movw    %sp, %bp
   121          call    print_hex
   122          popw    %ax     
   123          jmp     load_setup
       
   124  ok_load_setup:
</PRE>
<HR>
</CODE></BLOCKQUOTE>

If loading failed for some reason (bad floppy or someone pulled the diskette
out during the operation), we dump error code and retry in an endless
loop. 
The only way to get out of it is to reboot the machine, unless retry succeeds
but usually it doesn't (if something is wrong it will only get worse).</P>
<P>If loading setup_sects sectors of setup code succeeded we jump to label
<CODE>ok_load_setup:</CODE>.</P>
<P>Then we proceed to load the compressed kernel image at physical
address 0x10000. This
is done to preserve the firmware data areas in low memory (0-64K).
After the kernel is loaded, we jump to $SETUPSEG:0 (<CODE>arch/i386/boot/setup.S</CODE>).
Once the data is no longer needed (e.g. no more calls to BIOS) it is
overwritten by moving the entire (compressed) kernel image from 0x10000 to
0x1000 (physical addresses, of course).
This is done by <CODE>setup.S</CODE> which sets things up for protected mode and jumps
to 0x1000 which is the head of the compressed kernel, i.e.
<CODE>arch/386/boot/compressed/{head.S,misc.c}</CODE>.
This sets up stack and calls <CODE>decompress_kernel()</CODE> which uncompresses the
kernel to address 0x100000 and jumps to it.</P>
<P>Note that old bootloaders (old versions of LILO) could only load the
first 4 sectors of setup, which is why there is code in setup to load the rest of
itself if needed. Also, the code in setup has to take care of various
combinations of loader type/version vs zImage/bzImage and is therefore
highly complex.</P>
<P>Let us examine the kludge in the bootsector code that allows to load a big
kernel, known also as "bzImage".
The setup sectors are loaded as usual at 0x90200, but the kernel is loaded
64K chunk at a time using a special helper routine that calls BIOS to move
data from low to high memory. This helper routine is referred to by
<CODE>bootsect_kludge</CODE> in <CODE>bootsect.S</CODE> and is defined as <CODE>bootsect_helper</CODE> in <CODE>setup.S</CODE>.
The <CODE>bootsect_kludge</CODE> label in <CODE>setup.S</CODE> contains the value of setup segment
and the offset of <CODE>bootsect_helper</CODE> code in it so that bootsector can use the <CODE>lcall</CODE>
instruction to jump to it (inter-segment jump).
The reason why it is in <CODE>setup.S</CODE> is simply because there is no more space left
in bootsect.S (which is strictly not true - there are approximately 4 spare bytes
and at least 1 spare byte in <CODE>bootsect.S</CODE> but that is not enough, obviously).
This routine uses BIOS service int 0x15 (ax=0x8700) to move to high memory
and resets %es to always point to 0x10000. This ensures that the code in <CODE>bootsect.S</CODE>
doesn't run out of low memory when copying data from disk.</P>

<H2><A NAME="ss1.5">1.5 Using LILO as a bootloader </A>
</H2>


<P>There are several advantages in using a specialised bootloader (LILO) over
a bare bones Linux bootsector:
<OL>
<LI> Ability to choose between multiple Linux kernels or even multiple OSes.</LI>
<LI> Ability to pass kernel command line parameters (there is a patch
called BCP that adds this ability to bare-bones bootsector+setup).</LI>
<LI> Ability to load much larger bzImage kernels - up to 2.5M vs 1M.</LI>
</OL>

Old versions of LILO (v17 and earlier) could not load bzImage kernels. The
newer versions (as of a couple of years ago or earlier) use the same
technique as bootsect+setup of moving data from low into high memory by
means of BIOS services. Some people (Peter Anvin notably) argue that zImage
support should be removed. The main reason (according to Alan Cox) it stays
is that there are apparently some broken BIOSes that make it impossible to
boot bzImage kernels while loading zImage ones fine.</P>
<P>The last thing LILO does is to jump to <CODE>setup.S</CODE> and things proceed as normal.</P>

<H2><A NAME="ss1.6">1.6 High level initialisation </A>
</H2>


<P>By "high-level initialisation" we consider anything which is not directly
related to bootstrap, even though parts of the code to perform this are
written in asm, namely <CODE>arch/i386/kernel/head.S</CODE> which is the head of the
uncompressed kernel. The following steps are performed:</P>
<P>
<OL>
<LI> Initialise segment values (%ds = %es = %fs = %gs = __KERNEL_DS = 0x18).</LI>
<LI> Initialise page tables.</LI>
<LI> Enable paging by setting PG bit in %cr0.</LI>
<LI> Zero-clean BSS (on SMP, only first CPU does this).</LI>
<LI> Copy the first 2k of bootup parameters (kernel commandline).</LI>
<LI> Check CPU type using EFLAGS and, if possible, cpuid, able to detect
386 and higher.</LI>
<LI> The first CPU calls <CODE>start_kernel()</CODE>, all others call
<CODE>arch/i386/kernel/smpboot.c:initialize_secondary()</CODE> if ready=1,
which just reloads esp/eip and doesn't return.</LI>
</OL>
</P>
<P>The <CODE>init/main.c:start_kernel()</CODE> is written in C and does the following:</P>
<P>
<OL>
<LI> Take a global kernel lock (it is needed so that only one CPU
goes through initialisation).</LI>
<LI> Perform arch-specific setup (memory layout analysis, copying
boot command line again, etc.).</LI>
<LI> Print Linux kernel "banner" containing the version, compiler used to
build it etc. to the kernel ring buffer for messages. This is taken
from the variable linux_banner defined in init/version.c and is the
same string as displayed by <B>cat /proc/version</B>.</LI>
<LI> Initialise traps.</LI>
<LI> Initialise irqs.</LI>
<LI> Initialise data required for scheduler.</LI>
<LI> Initialise time keeping data.</LI>
<LI> Initialise softirq subsystem.</LI>
<LI> Parse boot commandline options.</LI>
<LI> Initialise console.</LI>
<LI> If module support was compiled into the kernel, initialise dynamical
module loading facility.</LI>
<LI> If "profile=" command line was supplied, initialise profiling buffers.</LI>
<LI> <CODE>kmem_cache_init()</CODE>, initialise most of slab allocator.</LI>
<LI> Enable interrupts.</LI>
<LI> Calculate BogoMips value for this CPU.</LI>
<LI> Call <CODE>mem_init()</CODE> which calculates <CODE>max_mapnr</CODE>, <CODE>totalram_pages</CODE> and
<CODE>high_memory</CODE> and prints out the "Memory: ..." line.</LI>
<LI> <CODE>kmem_cache_sizes_init()</CODE>, finish slab allocator initialisation.</LI>
<LI> Initialise data structures used by procfs.</LI>
<LI> <CODE>fork_init()</CODE>, create <CODE>uid_cache</CODE>, initialise <CODE>max_threads</CODE> based on
the amount of memory available and configure <CODE>RLIMIT_NPROC</CODE> for
<CODE>init_task</CODE> to be <CODE>max_threads/2</CODE>.</LI>
<LI> Create various slab caches needed for VFS, VM, buffer cache, etc.</LI>
<LI> If System V IPC support is compiled in, initialise the IPC subsystem.
Note that for System V shm, this includes mounting an internal
(in-kernel) instance of shmfs filesystem.</LI>
<LI> If quota support is compiled into the kernel, create and initialise
a special slab cache for it.</LI>
<LI> Perform arch-specific "check for bugs" and, whenever possible,
activate workaround for processor/bus/etc bugs. Comparing various
architectures reveals that "ia64 has no bugs" and "ia32 has quite a
few bugs", good example is "f00f bug" which is only checked if kernel
is compiled for less than 686 and worked around accordingly.</LI>
<LI> Set a flag to indicate that a schedule should be invoked at "next
opportunity" and create a kernel thread <CODE>init()</CODE> which execs
execute_command if supplied via "init=" boot parameter, or tries to
exec <B>/sbin/init</B>, <B>/etc/init</B>, <B>/bin/init</B>, <B>/bin/sh</B> in this order; if
all these fail, panic with "suggestion" to use "init=" parameter.</LI>
<LI> Go into the idle loop, this is an idle thread with pid=0.</LI>
</OL>
</P>
<P>Important thing to note here that the <CODE>init()</CODE> kernel thread calls
<CODE>do_basic_setup()</CODE> which in turn calls <CODE>do_initcalls()</CODE> which goes through the
list of functions registered by means of <CODE>__initcall</CODE> or <CODE>module_init()</CODE> macros
and invokes them. These functions either do not depend on each other
or their dependencies have been manually fixed by the link order in the
Makefiles. This means that, depending on
the position of directories in the trees and the structure of the Makefiles,
the order in which initialisation functions are invoked can change. Sometimes, this
is important because you can imagine two subsystems A and B with B depending
on some initialisation done by A. If A is compiled statically and B is a
module then B's entry point is guaranteed to be invoked after A prepared
all the necessary environment. If A is a module, then B is also necessarily
a module so there are no problems. But what if both A and B are statically
linked into the kernel? The order in which they are invoked depends on the relative 
entry point offsets in the <CODE>.initcall.init</CODE> ELF section of the kernel image.
Rogier Wolff proposed to introduce a hierarchical "priority" infrastructure
whereby modules could let the linker know in what (relative) order they
should be linked, but so far there are no patches available that implement
this in a sufficiently elegant manner to be acceptable into the kernel.
Therefore, make sure your link order is correct. If, in the example above,
A and B work fine when compiled statically once, they will always work,
provided they are listed sequentially in the same Makefile. If they don't
work, change the order in which their object files are listed.</P>
<P>Another thing worth noting is Linux's ability to execute an "alternative
init program" by means of passing "init=" boot commandline. This is useful
for recovering from accidentally overwritten <B>/sbin/init</B> or debugging the
initialisation (rc) scripts and <CODE>/etc/inittab</CODE> by hand, executing them
one at a time.</P>

<H2><A NAME="ss1.7">1.7 SMP Bootup on x86</A>
</H2>


<P>On SMP, the BP goes through the normal sequence of bootsector, setup etc
until it reaches the <CODE>start_kernel()</CODE>, and then on to <CODE>smp_init()</CODE> and
especially <CODE>src/i386/kernel/smpboot.c:smp_boot_cpus()</CODE>. The <CODE>smp_boot_cpus()</CODE>
goes in a loop for each apicid (until <CODE>NR_CPUS</CODE>) and calls <CODE>do_boot_cpu()</CODE> on
it. What <CODE>do_boot_cpu()</CODE> does is create (i.e. <CODE>fork_by_hand</CODE>) an idle task for
the target cpu and write in well-known locations defined by the Intel MP
spec (0x467/0x469) the EIP of trampoline code found in <CODE>trampoline.S</CODE>. Then
it generates STARTUP IPI to the target cpu which makes this AP execute the
code in <CODE>trampoline.S</CODE>.</P>
<P>The boot CPU  creates a copy of trampoline code for each CPU in
low memory. The AP code writes a magic number in its own code which is
verified by the BP to make sure that AP is executing the trampoline code.
The requirement that trampoline code must be in low memory is enforced by
the Intel MP specification.</P>
<P>The trampoline code simply sets %bx register to 1, enters protected mode
and jumps to startup_32 which is the main entry to <CODE>arch/i386/kernel/head.S</CODE>.</P>
<P>Now, the AP starts executing <CODE>head.S</CODE> and discovering that it is not a BP,
it skips the code that clears BSS and then enters <CODE>initialize_secondary()</CODE>
which just enters the idle task for this CPU - recall that <CODE>init_tasks[cpu]</CODE>
was already initialised by BP executing <CODE>do_boot_cpu(cpu)</CODE>.</P>
<P>Note that init_task can be shared but each idle thread must have its own
TSS. This is why <CODE>init_tss[NR_CPUS]</CODE> is an array.</P>

<H2><A NAME="ss1.8">1.8 Freeing initialisation data and code</A>
</H2>


<P>When the operating system initialises itself, most of the code and data
structures are never needed again.
Most operating systems (BSD, FreeBSD etc.) cannot dispose of this unneeded
information, thus wasting precious physical kernel memory.
The excuse they use (see McKusick's 4.4BSD book) is that "the relevant code
is spread around various subsystems and so it is not feasible to free it".
Linux, of course, cannot use such excuses because under Linux "if something
is possible in principle, then it is already implemented or somebody is
working on it".</P>
<P>So, as I said earlier, Linux kernel can only be compiled as an ELF binary, and
now we find out the reason (or one of the reasons) for that. The reason
related to throwing away initialisation code/data is that Linux provides two
macros to be used:</P>
<P>
<UL>
<LI> <CODE>__init</CODE> - for initialisation code</LI>
<LI> <CODE>__initdata</CODE> - for data</LI>
</UL>
</P>
<P>These evaluate to gcc attribute specificators (also known as "gcc magic")
as defined in <CODE>include/linux/init.h</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
#ifndef MODULE
#define __init        __attribute__ ((__section__ (".text.init")))
#define __initdata    __attribute__ ((__section__ (".data.init")))
#else
#define __init
#define __initdata
#endif
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>What this means is that if the code is compiled statically into the kernel
(i.e. MODULE is not defined) then it is placed in the special ELF section
<CODE>.text.init</CODE>, which is declared in the linker map in <CODE>arch/i386/vmlinux.lds</CODE>.
Otherwise (i.e. if it is a module) the macros evaluate to nothing.</P>
<P>What happens during boot is that the "init" kernel thread (function
<CODE>init/main.c:init()</CODE>) calls the arch-specific function <CODE>free_initmem()</CODE> which
frees all the pages between addresses <CODE>__init_begin</CODE> and <CODE>__init_end</CODE>.</P>
<P>On a typical system (my workstation), this results in freeing about 260K of
memory.</P>
<P>The functions registered via <CODE>module_init()</CODE> are placed in <CODE>.initcall.init</CODE>
which is also freed in the static case. The current trend in Linux, when
designing a subsystem (not necessarily a module), is to provide
init/exit entry points from the early stages of design so that in the
future, the subsystem in question can be modularised if needed. Example of
this is pipefs, see <CODE>fs/pipe.c</CODE>. Even if a given subsystem will never become a
module, e.g. bdflush (see <CODE>fs/buffer.c</CODE>), it is still nice and tidy to use
the <CODE>module_init()</CODE> macro against its initialisation function, provided it does
not matter when exactly is the function called.</P>
<P>There are two more macros which work in a similar manner, called <CODE>__exit</CODE> and
<CODE>__exitdata</CODE>, but they are more directly connected to the module support and
therefore will be explained in a later section.</P>

<H2><A NAME="ss1.9">1.9 Processing kernel command line</A>
</H2>


<P>Let us recall what happens to the commandline passed to kernel during boot:</P>
<P>
<OL>
<LI> LILO (or BCP) accepts the commandline using BIOS keyboard services
and stores it at a well-known location in physical memory, as well
as a signature saying that there is a valid commandline there.
</LI>
<LI> <CODE>arch/i386/kernel/head.S</CODE> copies the first 2k of it out to the zeropage.
</LI>
<LI> <CODE>arch/i386/kernel/setup.c:parse_mem_cmdline()</CODE> (called by
<CODE>setup_arch()</CODE>, itself called by <CODE>start_kernel()</CODE>) copies 256 bytes from zeropage
into <CODE>saved_command_line</CODE> which is displayed by <CODE>/proc/cmdline</CODE>. This
same routine processes the "mem=" option if present and makes appropriate
adjustments to VM parameters.
</LI>
<LI> We return to commandline in <CODE>parse_options()</CODE> (called by <CODE>start_kernel()</CODE>)
which processes some "in-kernel" parameters (currently "init=" and
environment/arguments for init) and passes each word to <CODE>checksetup()</CODE>.
</LI>
<LI> <CODE>checksetup()</CODE> goes through the code in ELF section <CODE>.setup.init</CODE> and
invokes each function, passing it the word if it matches. Note that
using the return value of 0 from the function registered via <CODE>__setup()</CODE>,
it is possible to pass the same "variable=value" to more than one
function with "value" invalid to one and valid to another.
Jeff Garzik commented: "hackers who do that get spanked :)"
Why? Because this is clearly ld-order specific, i.e. kernel linked
in one order will have functionA invoked before functionB and another
will have it in reversed order, with the result depending on the order.
</LI>
</OL>
</P>
<P>So, how do we write code that processes boot commandline? We use the <CODE>__setup()</CODE>
macro defined in <CODE>include/linux/init.h</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>

/*
 * Used for kernel command line parameter setup
 */
struct kernel_param {
        const char *str;
        int (*setup_func)(char *);
};

extern struct kernel_param __setup_start, __setup_end;

#ifndef MODULE
#define __setup(str, fn) \
   static char __setup_str_##fn[] __initdata = str; \
   static struct kernel_param __setup_##fn __initsetup = \
   { __setup_str_##fn, fn }

#else
#define __setup(str,func) /* nothing */
endif
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>So, you would typically use it in your code like this
(taken from code of real driver, BusLogic HBA <CODE>drivers/scsi/BusLogic.c</CODE>):</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
static int __init
BusLogic_Setup(char *str)
{
        int ints[3];

        (void)get_options(str, ARRAY_SIZE(ints), ints);

        if (ints[0] != 0) {
                BusLogic_Error("BusLogic: Obsolete Command Line Entry "
                                "Format Ignored\n", NULL);
                return 0;
        }
        if (str == NULL || *str == '\0')
                return 0;
        return BusLogic_ParseDriverOptions(str);
}

__setup("BusLogic=", BusLogic_Setup);
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>Note that <CODE>__setup()</CODE> does nothing for modules, so the code that wishes to
process boot commandline and can be either a module or statically linked
must invoke its parsing function manually in the module initialisation
routine. This also means that it is possible to write code that
processes parameters when compiled as a module but not when it is static or
vice versa.</P>

<H2><A NAME="s2">2. Process and Interrupt Management</A></H2>



<H2><A NAME="ss2.1">2.1 Task Structure and Process Table</A>
</H2>


<P>Every process under Linux is dynamically allocated a <CODE>struct task_struct</CODE>
structure. The maximum number of processes which can be created on Linux
is limited only by the amount of physical memory present, and is
equal to (see <CODE>kernel/fork.c:fork_init()</CODE>):</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
        /*
         * The default maximum number of threads is set to a safe
         * value: the thread structures can take up at most half
         * of memory.
         */
        max_threads = mempages / (THREAD_SIZE/PAGE_SIZE) / 2;
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>which, on IA32 architecture, basically means <CODE>num_physpages/4</CODE>. As an example,
on a 512M machine, you can create 32k threads. This is a considerable
improvement over the 4k-epsilon limit for older (2.2 and earlier) kernels.
Moreover, this can be changed at runtime using the KERN_MAX_THREADS <B>sysctl(2)</B>,
or simply using procfs interface to kernel tunables:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
# cat /proc/sys/kernel/threads-max 
32764
# echo 100000 > /proc/sys/kernel/threads-max 
# cat /proc/sys/kernel/threads-max 
100000
# gdb -q vmlinux /proc/kcore
Core was generated by `BOOT_IMAGE=240ac18 ro root=306 video=matrox:vesa:0x118'.
#0  0x0 in ?? ()
(gdb) p max_threads
$1 = 100000
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The set of processes on the Linux system is represented as a collection of
<CODE>struct task_struct</CODE> structures which are linked in two ways:</P>
<P>
<OL>
<LI> as a hashtable, hashed by pid, and</LI>
<LI> as a circular, doubly-linked list using <CODE>p->next_task</CODE> and <CODE>p->prev_task</CODE>
pointers.</LI>
</OL>
</P>
<P>The hashtable is called <CODE>pidhash[]</CODE> and is defined in
<CODE>include/linux/sched.h</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/* PID hashing. (shouldnt this be dynamic?) */
#define PIDHASH_SZ (4096 >> 2)
extern struct task_struct *pidhash[PIDHASH_SZ];

#define pid_hashfn(x)   ((((x) >> 8) ^ (x)) &amp; (PIDHASH_SZ - 1))
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The tasks are hashed by their pid value and the above hashing function is
supposed to distribute the elements uniformly in their domain
(<CODE>0</CODE> to <CODE>PID_MAX-1</CODE>). The hashtable is used to quickly find a task by given pid,
using <CODE>find_task_pid()</CODE> inline from <CODE>include/linux/sched.h</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
static inline struct task_struct *find_task_by_pid(int pid)
{
        struct task_struct *p, **htable = &amp;pidhash[pid_hashfn(pid)];

        for(p = *htable; p &amp;&amp; p->pid != pid; p = p->pidhash_next)
                ;

        return p;
}
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The tasks on each hashlist (i.e. hashed to the same value) are linked
by <CODE>p->pidhash_next/pidhash_pprev</CODE> which are used by <CODE>hash_pid()</CODE> and
<CODE>unhash_pid()</CODE> to insert and remove a given process into the hashtable.
These are done under protection of the read-write spinlock called <CODE>tasklist_lock</CODE>
taken for WRITE.</P>
<P>The circular doubly-linked list that uses <CODE>p->next_task/prev_task</CODE> is
maintained so that one could go through all tasks on the system easily.
This is achieved by the <CODE>for_each_task()</CODE> macro from <CODE>include/linux/sched.h</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
#define for_each_task(p) \
        for (p = &amp;init_task ; (p = p->next_task) != &amp;init_task ; )
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>Users of <CODE>for_each_task()</CODE> should take tasklist_lock for READ.
Note that <CODE>for_each_task()</CODE> is using <CODE>init_task</CODE> to mark the beginning (and end)
of the list - this is safe because the idle task (pid 0) never exits.</P>
<P>The modifiers of the process hashtable or/and the process table links,
notably <CODE>fork()</CODE>, <CODE>exit()</CODE> and <CODE>ptrace()</CODE>, must take <CODE>tasklist_lock</CODE> for WRITE. What is
more interesting is that the writers must also disable interrupts on the 
local CPU. The reason for this is not trivial: the <CODE>send_sigio()</CODE> function walks the
task list and thus takes <CODE>tasklist_lock</CODE> for READ, and it is called from
<CODE>kill_fasync()</CODE> in interrupt context. This is why writers must disable
interrupts while readers don't need to.</P>
<P>Now that we understand how the <CODE>task_struct</CODE> structures are linked together,
let us examine the members of <CODE>task_struct</CODE>. They loosely correspond to the
members of UNIX 'struct proc' and 'struct user' combined together.</P>
<P>The other versions of UNIX separated the task state information into
one part which should be kept memory-resident at all times (called 'proc
structure' which includes process state, scheduling information etc.) and
another part which is only needed when the process is running (called 'u area' which
includes file descriptor table, disk quota information etc.). The only reason
for such ugly design was that memory was a very scarce resource. Modern
operating systems (well, only Linux at the moment but others, e.g. FreeBSD
seem to improve in this direction towards Linux) do not need such separation
and therefore maintain process state in a kernel memory-resident data
structure at all times.</P>
<P>The task_struct structure is declared in <CODE>include/linux/sched.h</CODE> and is
currently 1680 bytes in size.</P>
<P>The state field is declared as:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
volatile long state;    /* -1 unrunnable, 0 runnable, >0 stopped */

#define TASK_RUNNING            0
#define TASK_INTERRUPTIBLE      1
#define TASK_UNINTERRUPTIBLE    2
#define TASK_ZOMBIE             4
#define TASK_STOPPED            8
#define TASK_EXCLUSIVE          32
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>Why is <CODE>TASK_EXCLUSIVE</CODE> defined as 32 and not 16? Because 16 was used up by
<CODE>TASK_SWAPPING</CODE> and I forgot to shift <CODE>TASK_EXCLUSIVE</CODE> up when I removed
all references to <CODE>TASK_SWAPPING</CODE> (sometime in 2.3.x).</P>
<P>The <CODE>volatile</CODE> in <CODE>p->state</CODE> declaration means it can be modified
asynchronously (from interrupt handler):</P>
<P>
<OL>
<LI><B>TASK_RUNNING</B>: means the task is "supposed to be" on the run
queue.  The reason it may not yet be on the runqueue is that marking a task as
<CODE>TASK_RUNNING</CODE> and placing it on the runqueue is not atomic.  You need to hold
the <CODE>runqueue_lock</CODE> read-write spinlock for read in order to look at the
runqueue. If you do so, you will then see that every task on the runqueue is in
<CODE>TASK_RUNNING</CODE> state. However, the converse is not true for the reason explained
above. Similarly, drivers can mark themselves (or rather the process context they
run in) as <CODE>TASK_INTERRUPTIBLE</CODE> (or <CODE>TASK_UNINTERRUPTIBLE</CODE>) and then call <CODE>schedule()</CODE>,
which will then remove it from the runqueue (unless there is a pending signal, in which
case it is left on the runqueue).  </LI>
<LI><B>TASK_INTERRUPTIBLE</B>: means the task is sleeping but can be woken up
by a signal or by expiry of a timer.</LI>
<LI><B>TASK_UNINTERRUPTIBLE</B>: same as <CODE>TASK_INTERRUPTIBLE</CODE>, except it cannot
be woken up.</LI>
<LI><B>TASK_ZOMBIE</B>: task has terminated but has not had its status collected
(<CODE>wait()</CODE>-ed for) by the parent (natural or by adoption).</LI>
<LI><B>TASK_STOPPED</B>: task was stopped, either due to job control signals or
due to <B>ptrace(2)</B>.</LI>
<LI><B>TASK_EXCLUSIVE</B>: this is not a separate state but can be OR-ed to
either one of <CODE>TASK_INTERRUPTIBLE</CODE> or <CODE>TASK_UNINTERRUPTIBLE</CODE>.
This means that when
this task is sleeping on a wait queue with many other tasks, it will be
woken up alone instead of causing "thundering herd" problem by waking up all
the waiters.</LI>
</OL>
</P>
<P>Task flags contain information about the process states which are not
mutually exclusive:
<BLOCKQUOTE><CODE>
<HR>
<PRE>
unsigned long flags;    /* per process flags, defined below */
/*
 * Per process flags
 */
#define PF_ALIGNWARN    0x00000001      /* Print alignment warning msgs */
                                        /* Not implemented yet, only for 486*/
#define PF_STARTING     0x00000002      /* being created */
#define PF_EXITING      0x00000004      /* getting shut down */
#define PF_FORKNOEXEC   0x00000040      /* forked but didn't exec */
#define PF_SUPERPRIV    0x00000100      /* used super-user privileges */
#define PF_DUMPCORE     0x00000200      /* dumped core */
#define PF_SIGNALED     0x00000400      /* killed by a signal */
#define PF_MEMALLOC     0x00000800      /* Allocating memory */
#define PF_VFORK        0x00001000      /* Wake up parent in mm_release */
#define PF_USEDFPU      0x00100000      /* task used FPU this quantum (SMP) */
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The fields <CODE>p->has_cpu</CODE>, <CODE>p->processor</CODE>, <CODE>p->counter</CODE>, <CODE>p->priority</CODE>, <CODE>p->policy</CODE> and
<CODE>p->rt_priority</CODE> are related to the scheduler and will be looked at later.</P>
<P>The fields <CODE>p->mm</CODE> and <CODE>p->active_mm</CODE> point respectively to the process' address space
described by <CODE>mm_struct</CODE> structure and to the active address space if the
process doesn't have a real one (e.g. kernel threads). This helps minimise
TLB flushes on switching address spaces when the task is scheduled out.
So, if we are scheduling-in the kernel thread (which has no <CODE>p->mm</CODE>) then its
<CODE>next->active_mm</CODE> will be set to the <CODE>prev->active_mm</CODE> of the task that was
scheduled-out, which will be the same as <CODE>prev->mm</CODE> if <CODE>prev->mm != NULL</CODE>.
The address space can be shared between threads if <CODE>CLONE_VM</CODE> flag is passed
to the <B>clone(2)</B> system call or by means of <B>vfork(2)</B> system call.</P>
<P>The fields <CODE>p->exec_domain</CODE> and <CODE>p->personality</CODE> relate to the personality of
the task, i.e. to the way certain system calls behave in order to emulate the
"personality" of foreign flavours of UNIX.</P>
<P>The field <CODE>p->fs</CODE> contains filesystem information, which under Linux means
three pieces of information:</P>
<P>
<OL>
<LI>root directory's dentry and mountpoint,</LI>
<LI>alternate root directory's dentry and mountpoint,</LI>
<LI>current working directory's dentry and mountpoint.</LI>
</OL>
</P>
<P>This structure also includes a reference count because it can be shared
between cloned tasks when <CODE>CLONE_FS</CODE> flag is passed to the <B>clone(2)</B> system
call.</P>
<P>The field <CODE>p->files</CODE> contains the file descriptor table. This too can be
shared between tasks, provided <CODE>CLONE_FILES</CODE> is specified with <B>clone(2)</B> system
call.</P>
<P>The field <CODE>p->sig</CODE> contains signal handlers and can be shared between cloned
tasks by means of <CODE>CLONE_SIGHAND</CODE>.</P>

<H2><A NAME="ss2.2">2.2 Creation and termination of tasks and kernel threads</A>
</H2>


<P>Different books on operating systems define a "process" in different ways,
starting from "instance of a program in execution" and ending with "that 
which is produced by clone(2) or fork(2) system calls".
Under Linux, there are three kinds of processes:</P>
<P>
<UL>
<LI> the idle thread(s),</LI>
<LI> kernel threads,</LI>
<LI> user tasks.</LI>
</UL>
</P>
<P>The idle thread is created at compile time for the first CPU; it is then
"manually" created for each CPU by means of arch-specific
<CODE>fork_by_hand()</CODE> in <CODE>arch/i386/kernel/smpboot.c</CODE>, which unrolls the <B>fork(2)</B> system
call by hand (on some archs). Idle tasks share one init_task structure but
have a private TSS structure, in the per-CPU array <CODE>init_tss</CODE>. Idle tasks all have
pid = 0 and no other task can share pid, i.e. use <CODE>CLONE_PID</CODE> flag to <B>clone(2)</B>.</P>
<P>Kernel threads are created using <CODE>kernel_thread()</CODE> function which invokes
the <B>clone(2)</B> system call in kernel mode. Kernel threads usually have no user
address space, i.e. <CODE>p->mm = NULL</CODE>, because they explicitly do <CODE>exit_mm()</CODE>, e.g.
via <CODE>daemonize()</CODE> function. Kernel threads can always access kernel address
space directly. They are allocated pid numbers in the low range. Running at
processor's ring 0 (on x86, that is) implies that the kernel threads enjoy all I/O privileges
and cannot be pre-empted by the scheduler.</P>
<P>User tasks are created by means of <B>clone(2)</B> or <B>fork(2)</B> system calls, both of
which internally invoke <B>kernel/fork.c:do_fork()</B>.</P>
<P>Let us understand what happens when a user process makes a <B>fork(2)</B> system
call. Although <B>fork(2)</B> is architecture-dependent due to the
different ways of passing user stack and registers, the actual underlying
function <CODE>do_fork()</CODE> that does the job is portable and is located at
<CODE>kernel/fork.c</CODE>.</P>
<P>The following steps are done:</P>
<P>
<OL>
<LI> Local variable <CODE>retval</CODE> is set to <CODE>-ENOMEM</CODE>, as this is the value which <CODE>errno</CODE>
should be set to if <B>fork(2)</B> fails to allocate a new task structure.
</LI>
<LI> If <CODE>CLONE_PID</CODE> is set in <CODE>clone_flags</CODE> then return an error (<CODE>-EPERM</CODE>), unless
the caller is the idle thread (during boot only). So, normal user
threads cannot pass <CODE>CLONE_PID</CODE> to <B>clone(2)</B> and expect it to succeed.
For <B>fork(2)</B>, this is irrelevant as <CODE>clone_flags</CODE> is set to <CODE>SIFCHLD</CODE> - this
is only relevant when <CODE>do_fork()</CODE> is invoked from <CODE>sys_clone()</CODE> which
passes the <CODE>clone_flags</CODE> from the value requested from userspace.
</LI>
<LI> <CODE>current->vfork_sem</CODE> is initialised (it is later cleared in the child).
This is used by <CODE>sys_vfork()</CODE> (<B>vfork(2)</B> system call, corresponds to 
<CODE>clone_flags = CLONE_VFORK|CLONE_VM|SIGCHLD</CODE>) to make the parent sleep
until the child does <CODE>mm_release()</CODE>, for example as a result of <CODE>exec()</CODE>ing
another program or <B>exit(2)</B>-ing.
</LI>
<LI> A new task structure is allocated using arch-dependent
<CODE>alloc_task_struct()</CODE> macro. On x86 it is just a gfp at <CODE>GFP_KERNEL</CODE>
priority. This is the first reason why <B>fork(2)</B> system call may sleep.
If this allocation fails, we return <CODE>-ENOMEM</CODE>.
</LI>
<LI> All the values from current process' task structure are copied into
the new one, using structure assignment <CODE>*p = *current</CODE>. Perhaps this
should be replaced by a memcpy? Later on, the fields that should not
be inherited by the child are set to the correct values.
</LI>
<LI> Big kernel lock is taken as the rest of the code would otherwise be
non-reentrant.
</LI>
<LI> If the parent has user resources (a concept of UID, Linux is flexible
enough to make it a question rather than a fact), then verify if the
user exceeded <CODE>RLIMIT_NPROC</CODE> soft limit - if so, fail with <CODE>-EAGAIN</CODE>, if
not, increment the count of processes by given uid <CODE>p->user->count</CODE>.
</LI>
<LI> If the system-wide number of tasks exceeds the value of the tunable
max_threads, fail with <CODE>-EAGAIN</CODE>.
</LI>
<LI> If the binary being executed belongs to a modularised execution
domain, increment the corresponding module's reference count.
</LI>
<LI> If the binary being executed belongs to a modularised binary format,
increment the corresponding module's reference count.
</LI>
<LI> The child is marked as 'has not execed' (<CODE>p->did_exec = 0</CODE>)
</LI>
<LI> The child is marked as 'not-swappable' (<CODE>p->swappable = 0</CODE>)
</LI>
<LI> The child is put into 'uninterruptible sleep' state, i.e.
<CODE>p->state = TASK_UNINTERRUPTIBLE</CODE> (TODO: why is this done?
I think it's not needed - get rid of it, Linus confirms it is not
needed)
</LI>
<LI> The child's <CODE>p->flags</CODE> are set according to the value of clone_flags;
for plain <B>fork(2)</B>, this will be <CODE>p->flags = PF_FORKNOEXEC</CODE>.
</LI>
<LI> The child's pid <CODE>p->pid</CODE> is set using the fast algorithm in
<CODE>kernel/fork.c:get_pid()</CODE> (TODO: <CODE>lastpid_lock</CODE> spinlock can be made
redundant since <CODE>get_pid()</CODE> is always called under big kernel lock
from <CODE>do_fork()</CODE>, also remove flags argument of <CODE>get_pid()</CODE>, patch sent
to Alan on 20/06/2000 - followup later).
</LI>
<LI> The rest of the code in <CODE>do_fork()</CODE> initialises the rest of child's
task structure. At the very end, the child's task structure is
hashed into the <CODE>pidhash</CODE> hashtable and the child is woken up (TODO:
<CODE>wake_up_process(p)</CODE> sets <CODE>p->state = TASK_RUNNING</CODE> and adds the process
to the runq, therefore we probably didn't need to set <CODE>p->state</CODE> to
<CODE>TASK_RUNNING</CODE> earlier on in <CODE>do_fork()</CODE>). The interesting part is
setting <CODE>p->exit_signal</CODE> to <CODE>clone_flags &amp; CSIGNAL</CODE>, which for <B>fork(2)</B>
means just <CODE>SIGCHLD</CODE> and setting <CODE>p->pdeath_signal</CODE> to 0. The
<CODE>pdeath_signal</CODE> is used when a process 'forgets' the original parent 
(by dying) and can be set/get by means of <CODE>PR_GET/SET_PDEATHSIG</CODE>
commands of <B>prctl(2)</B> system call (You might argue that the way the
value of <CODE>pdeath_signal</CODE> is returned via userspace pointer argument
in <B>prctl(2)</B> is a bit silly - mea culpa, after Andries Brouwer 
updated the manpage it was too late to fix ;)</LI>
</OL>
</P>
<P>Thus tasks are created. There are several ways for tasks to terminate:</P>
<P>
<OL>
<LI> by making <B>exit(2)</B> system call;
</LI>
<LI> by being delivered a signal with default disposition to die;
</LI>
<LI> by being forced to die under certain exceptions;
</LI>
<LI> by calling <B>bdflush(2)</B> with <CODE>func == 1</CODE> (this is Linux-specific, for
compatibility with old distributions that still had the 'update'
line in <CODE>/etc/inittab</CODE> - nowadays the work of update is done by
kernel thread <CODE>kupdate</CODE>).</LI>
</OL>
</P>
<P>Functions implementing system calls under Linux are prefixed with <CODE>sys_</CODE>, 
but they are usually concerned only with argument checking or arch-specific
ways to pass some information and the actual work is done by <CODE>do_</CODE> functions.
So it is with <CODE>sys_exit()</CODE> which calls <CODE>do_exit()</CODE> to do the work. Although, 
other parts of the kernel sometimes invoke <CODE>sys_exit()</CODE> while they should really
call <CODE>do_exit()</CODE>.</P>
<P>The function <CODE>do_exit()</CODE> is found in <CODE>kernel/exit.c</CODE>. The points to note about
<CODE>do_exit()</CODE>:</P>
<P>
<UL>
<LI> Uses global kernel lock (locks but doesn't unlock).
</LI>
<LI> Calls <CODE>schedule()</CODE> at the end, which never returns.
</LI>
<LI> Sets the task state to <CODE>TASK_ZOMBIE</CODE>.
</LI>
<LI> Notifies any child with <CODE>current->pdeath_signal</CODE>, if not 0.
</LI>
<LI> Notifies the parent with a <CODE>current->exit_signal</CODE>, which is usually
equal to <CODE>SIGCHLD</CODE>.
</LI>
<LI> Releases resources allocated by fork, closes open files etc.
</LI>
<LI> On architectures that use lazy FPU switching (ia64, mips, mips64)
(TODO: remove 'flags' argument of 
sparc, sparc64), do whatever the hardware requires to pass the FPU
ownership (if owned by current) to "none".</LI>
</UL>
</P>

<H2><A NAME="ss2.3">2.3 Linux Scheduler</A>
</H2>


<P>The job of a scheduler is to arbitrate access to the current CPU between
multiple processes. The scheduler is implemented in the 'main kernel file'
<CODE>kernel/sched.c</CODE>. The corresponding header file <CODE>include/linux/sched.h</CODE> is
included (either explicitly or indirectly) by virtually every kernel source
file.</P>
<P>The fields of task structure relevant to scheduler include:</P>
<P>
<UL>
<LI> <CODE>p->need_resched</CODE>: this field is set if <CODE>schedule()</CODE> should be invoked at
the 'next opportunity'.
</LI>
<LI> <CODE>p->counter</CODE>: number of clock ticks left to run in this scheduling
slice, decremented by a timer. When this field becomes lower than or equal to zero, it is reset
to 0 and <CODE>p->need_resched</CODE> is set. This is also sometimes called 'dynamic
priority' of a process because it can change by itself.
</LI>
<LI> <CODE>p->priority</CODE>: the process' static priority, only changed through well-known
system calls like <B>nice(2)</B>, POSIX.1b <B>sched_setparam(2)</B> or 4.4BSD/SVR4
<B>setpriority(2)</B>.
</LI>
<LI> <CODE>p->rt_priority</CODE>: realtime priority
</LI>
<LI> <CODE>p->policy</CODE>: the scheduling policy, specifies which scheduling class the
task belongs to. Tasks can change their scheduling class using the
<B>sched_setscheduler(2)</B> system call. The valid values are <CODE>SCHED_OTHER</CODE>
(traditional UNIX process), <CODE>SCHED_FIFO</CODE> (POSIX.1b FIFO realtime
process) and <CODE>SCHED_RR</CODE> (POSIX round-robin realtime process). One can 
also OR <CODE>SCHED_YIELD</CODE> to any of these values to signify that the process
decided to yield the CPU, for example by calling <B>sched_yield(2)</B> system
call. A FIFO realtime process will run until either a) it blocks on I/O,
b) it explicitly yields the CPU or c) it is preempted by another realtime
process with a higher <CODE>p->rt_priority</CODE> value. <CODE>SCHED_RR</CODE> is the same as
<CODE>SCHED_FIFO</CODE>, except that when its timeslice expires it goes back to
the end of the runqueue.</LI>
</UL>
</P>
<P>The scheduler's algorithm is simple, despite the great apparent complexity
of the <CODE>schedule()</CODE> function. The function is complex because it implements
three scheduling algorithms in one and also because of the subtle 
SMP-specifics.</P>
<P>The apparently 'useless' gotos in <CODE>schedule()</CODE> are there for a purpose - to
generate the best optimised (for i386) code. Also, note that scheduler
(like most of the kernel) was completely rewritten for 2.4, therefore the
discussion below does not apply to 2.2 or earlier kernels.</P>
<P>Let us look at the function in detail:</P>
<P>
<OL>
<LI> If <CODE>current->active_mm == NULL</CODE> then something is wrong. Current
process, even a kernel thread (<CODE>current->mm == NULL</CODE>) must have a valid
<CODE>p->active_mm</CODE> at all times.
</LI>
<LI> If there is something to do on the <CODE>tq_scheduler</CODE> task queue, process it
now. Task queues provide a kernel mechanism to schedule execution of
functions at a later time. We shall look at it in details elsewhere.
</LI>
<LI> Initialise local variables <CODE>prev</CODE> and <CODE>this_cpu</CODE> to current task and
current CPU respectively.
</LI>
<LI> Check if <CODE>schedule()</CODE> was invoked from interrupt handler (due to a bug)
and panic if so.
</LI>
<LI> Release the global kernel lock.
</LI>
<LI> If there is some work to do via softirq mechanism, do it now.
</LI>
<LI> Initialise local pointer <CODE>struct schedule_data *sched_data</CODE> to point
to per-CPU (cacheline-aligned to prevent cacheline ping-pong)
scheduling data area, which contains the TSC value of <CODE>last_schedule</CODE> and the
pointer to last scheduled task structure (TODO: <CODE>sched_data</CODE> is used on
SMP only but why does <CODE>init_idle()</CODE> initialises it on UP as well?).
</LI>
<LI> <CODE>runqueue_lock</CODE> spinlock is taken. Note that we use <CODE>spin_lock_irq()</CODE>
because in <CODE>schedule()</CODE> we guarantee that interrupts are enabled. Therefore,
when we unlock <CODE>runqueue_lock</CODE>, we can just re-enable them instead of
saving/restoring eflags (<CODE>spin_lock_irqsave/restore</CODE> variant).
</LI>
<LI> task state machine: if the task is in <CODE>TASK_RUNNING</CODE> state, it is left
alone; if it is in <CODE>TASK_INTERRUPTIBLE</CODE> state and a signal is pending,
it is moved into <CODE>TASK_RUNNING</CODE> state. In all other cases, it is deleted
from the runqueue.
</LI>
<LI> <CODE>next</CODE> (best candidate to be scheduled) is set to the idle task of
this cpu. However, the goodness of this candidate is set to a very
low value (-1000), in hope that there is someone better than that.
</LI>
<LI> If the <CODE>prev</CODE> (current) task is in <CODE>TASK_RUNNING</CODE> state, then the
current goodness is set to its goodness and it is marked as a better
candidate to be scheduled than the idle task.
</LI>
<LI> Now the runqueue is examined and a goodness of each process that can
be scheduled on this cpu is compared with current value; the
process with highest goodness wins. Now the concept of "can be
scheduled on this cpu" must be clarified: on UP, every process on
the runqueue is eligible to be scheduled; on SMP, only process not
already running on another cpu is eligible to be scheduled on this
cpu. The goodness is calculated by a function called <CODE>goodness()</CODE>, which
treats realtime processes by making their goodness very high
(<CODE>1000 + p->rt_priority</CODE>), this being greater than 1000 guarantees that
no <CODE>SCHED_OTHER</CODE> process can win; so they only contend with other
realtime processes that may have a greater <CODE>p->rt_priority</CODE>. The
goodness function returns 0 if the process' time slice (<CODE>p->counter</CODE>) 
is over. For non-realtime processes, the initial value of goodness is
set to <CODE>p->counter</CODE> - this way, the process is less likely to get CPU if
it already had it for a while, i.e. interactive processes are favoured
more than CPU bound number crunchers. The arch-specific constant
<CODE>PROC_CHANGE_PENALTY</CODE> attempts to implement "cpu affinity" (i.e. give 
advantage to a process on the same CPU). It also gives a slight
advantage to processes with mm pointing to current <CODE>active_mm</CODE> or to
processes with no (user) address space, i.e. kernel threads.
</LI>
<LI> if the current value of goodness is 0 then the entire list of
processes (not just the ones on the runqueue!) is examined and their dynamic
priorities are recalculated using simple algorithm:

<BLOCKQUOTE><CODE>
<HR>
<PRE>

recalculate:
        {
                struct task_struct *p;
                spin_unlock_irq(&amp;runqueue_lock);
                read_lock(&amp;tasklist_lock);
                for_each_task(p)
                        p->counter = (p->counter >> 1) + p->priority;
                read_unlock(&amp;tasklist_lock);
                spin_lock_irq(&amp;runqueue_lock);
        }
</PRE>
<HR>
</CODE></BLOCKQUOTE>


Note that the we drop the <CODE>runqueue_lock</CODE> before we recalculate. The
reason is that we go through entire set of processes;  this can take
a long time, during which the <CODE>schedule()</CODE> could be called on another CPU and
select a process with goodness good enough for that CPU, whilst we on
this CPU were forced to recalculate. Ok, admittedly this is somewhat
inconsistent because while we (on this CPU) are selecting a process with
the best goodness, <CODE>schedule()</CODE> running on another CPU could be
recalculating dynamic priorities.
</LI>
<LI> From this point on it is certain that <CODE>next</CODE> points to the task to
be scheduled, so we initialise <CODE>next->has_cpu</CODE> to 1 and <CODE>next->processor</CODE>
to <CODE>this_cpu</CODE>. The <CODE>runqueue_lock</CODE> can now be unlocked.
</LI>
<LI> If we are switching back to the same task (<CODE>next == prev</CODE>) then we can
simply reacquire the global kernel lock and return, i.e. skip all the
hardware-level (registers, stack etc.) and VM-related (switch page
directory, recalculate <CODE>active_mm</CODE> etc.) stuff.
</LI>
<LI> The macro <CODE>switch_to()</CODE> is architecture specific. On i386, it is
concerned with a) FPU handling, b) LDT handling, c) reloading segment
registers, d) TSS handling and e) reloading debug registers.</LI>
</OL>
</P>

<H2><A NAME="ss2.4">2.4 Linux linked list implementation</A>
</H2>


<P>Before we go on to examine implementation of wait queues, we must
acquaint ourselves with the Linux standard doubly-linked list implementation.
Wait queues (as well as everything else in Linux) make heavy use
of them and they are called in jargon "list.h implementation" because the
most relevant file is <CODE>include/linux/list.h</CODE>.</P>
<P>The fundamental data structure here is <CODE>struct list_head</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct list_head {
        struct list_head *next, *prev;
};

#define LIST_HEAD_INIT(name) { &amp;(name), &amp;(name) }

#define LIST_HEAD(name) \
        struct list_head name = LIST_HEAD_INIT(name)

#define INIT_LIST_HEAD(ptr) do { \
        (ptr)->next = (ptr); (ptr)->prev = (ptr); \
} while (0)

#define list_entry(ptr, type, member) \
        ((type *)((char *)(ptr)-(unsigned long)(&amp;((type *)0)->member)))

#define list_for_each(pos, head) \
        for (pos = (head)->next; pos != (head); pos = pos->next)
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The first three macros are for initialising an empty list by pointing both
<CODE>next</CODE> and <CODE>prev</CODE> pointers to itself. It is obvious from C syntactical
restrictions which ones should be used where - for example, <CODE>LIST_HEAD_INIT()</CODE>
can be used for structure's element initialisation in declaration, the second
can be used for static variable initialising declarations and the third can
be used inside a function.</P>
<P>The macro <CODE>list_entry()</CODE> gives access to individual list element, for example
(from <CODE>fs/file_table.c:fs_may_remount_ro()</CODE>):</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct super_block {
   ...
   struct list_head s_files;
   ...
} *sb = &amp;some_super_block;

struct file {
   ...
   struct list_head f_list;
   ...
} *file;

struct list_head *p;

for (p = sb->s_files.next; p != &amp;sb->s_files; p = p->next) {
     struct file *file = list_entry(p, struct file, f_list);
     do something to 'file'
}
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>A good example of the use of <CODE>list_for_each()</CODE> macro is in the scheduler where
we walk the runqueue looking for the process with highest goodness:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
static LIST_HEAD(runqueue_head);
struct list_head *tmp;
struct task_struct *p;

list_for_each(tmp, &amp;runqueue_head) {
    p = list_entry(tmp, struct task_struct, run_list);
    if (can_schedule(p)) {
        int weight = goodness(p, this_cpu, prev->active_mm);
        if (weight > c)
            c = weight, next = p;
    }
}
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>Here, <CODE>p->run_list</CODE> is declared as <CODE>struct list_head run_list</CODE> inside
<CODE>task_struct</CODE> structure and serves as anchor to the list. Removing an element
from the list and adding (to head or tail of the list) is done by
<CODE>list_del()/list_add()/list_add_tail()</CODE> macros. The examples below are adding
and removing a task from runqueue:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
static inline void del_from_runqueue(struct task_struct * p)
{
        nr_running--;
        list_del(&amp;p->run_list);
        p->run_list.next = NULL;
}

static inline void add_to_runqueue(struct task_struct * p)
{
        list_add(&amp;p->run_list, &amp;runqueue_head);
        nr_running++;
}

static inline void move_last_runqueue(struct task_struct * p)
{
        list_del(&amp;p->run_list);
        list_add_tail(&amp;p->run_list, &amp;runqueue_head);
}

static inline void move_first_runqueue(struct task_struct * p)
{
        list_del(&amp;p->run_list);
        list_add(&amp;p->run_list, &amp;runqueue_head);
}
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>

<H2><A NAME="ss2.5">2.5 Wait Queues</A>
</H2>


<P>When a process requests the kernel to do something which is currently
impossible but that may become possible later, the process is put to sleep
and is woken up when the request is more likely to be satisfied. One of the
kernel mechanisms used for this is called a 'wait queue'. </P>
<P>Linux implementation allows wake-on semantics using <CODE>TASK_EXCLUSIVE</CODE> flag.
With waitqueues, you can either use a well-known queue and then simply
<CODE>sleep_on/sleep_on_timeout/interruptible_sleep_on/interruptible_sleep_on_timeout</CODE>,
or you can define your own waitqueue and use <CODE>add/remove_wait_queue</CODE> to add and
remove yourself from it and <CODE>wake_up/wake_up_interruptible</CODE> to wake up
when needed.</P>
<P>An example of the first usage of waitqueues is interaction between the page
allocator (in <CODE>mm/page_alloc.c:__alloc_pages()</CODE>) and the <CODE>kswapd</CODE> kernel daemon (in
<CODE>mm/vmscan.c:kswap()</CODE>), by means of wait queue <CODE>kswapd_wait,</CODE> declared in
<CODE>mm/vmscan.c</CODE>; the <CODE>kswapd</CODE> daemon sleeps on this queue, and it is woken up
whenever the page allocator needs to free up some pages.</P>
<P>An example of autonomous waitqueue usage is interaction between
user process requesting data via <B>read(2)</B> system call and kernel running in
the interrupt context to supply the data. An interrupt handler might look
like (simplified <CODE>drivers/char/rtc_interrupt()</CODE>):</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
static DECLARE_WAIT_QUEUE_HEAD(rtc_wait);

void rtc_interrupt(int irq, void *dev_id, struct pt_regs *regs)
{
        spin_lock(&amp;rtc_lock);       
        rtc_irq_data = CMOS_READ(RTC_INTR_FLAGS);
        spin_unlock(&amp;rtc_lock);     
        wake_up_interruptible(&amp;rtc_wait);
}
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>So, the interrupt handler obtains the data by reading from some
device-specific I/O port (<CODE>CMOS_READ()</CODE> macro turns into a couple <CODE>outb/inb</CODE>) and
then wakes up whoever is sleeping on the <CODE>rtc_wait</CODE> wait queue.</P>
<P>Now, the <B>read(2)</B> system call could be implemented as:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
ssize_t rtc_read(struct file file, char *buf, size_t count, loff_t *ppos)
{
        DECLARE_WAITQUEUE(wait, current);
        unsigned long data;
        ssize_t retval;

        add_wait_queue(&amp;rtc_wait, &amp;wait);
        current->state = TASK_INTERRUPTIBLE;
        do {
                spin_lock_irq(&amp;rtc_lock);
                data = rtc_irq_data;
                rtc_irq_data = 0;
                spin_unlock_irq(&amp;rtc_lock);

                if (data != 0)
                        break;

                if (file->f_flags &amp; O_NONBLOCK) {
                        retval = -EAGAIN;
                        goto out;
                }
                if (signal_pending(current)) {
                        retval = -ERESTARTSYS;
                        goto out;
                }
                schedule();
        } while(1);
        retval = put_user(data, (unsigned long *)buf);  
        if (!retval)
                retval = sizeof(unsigned long);

out:
        current->state = TASK_RUNNING;
        remove_wait_queue(&amp;rtc_wait, &amp;wait);
        return retval;
}
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>What happens in <CODE>rtc_read()</CODE> is this:</P>
<P>
<OL>
<LI> We declare a wait queue element pointing to current process context.
</LI>
<LI> We add this element to the <CODE>rtc_wait</CODE> waitqueue.
</LI>
<LI> We mark current context as <CODE>TASK_INTERRUPTIBLE</CODE> which means it will not
be rescheduled after the next time it sleeps.
</LI>
<LI> We check if there is no data available; if there is we break out,
copy data to user buffer, mark ourselves as <CODE>TASK_RUNNING</CODE>, remove
ourselves from the wait queue and return
</LI>
<LI> If there is no data yet, we check whether the user specified non-blocking I/O
and if so we fail with <CODE>EAGAIN</CODE> (which is the same as <CODE>EWOULDBLOCK</CODE>)
</LI>
<LI> We also check if a signal is pending and if so inform the "higher
layers" to restart the system call if necessary. By "if necessary"
I meant the details of signal disposition as specified in <B>sigaction(2)</B>
system call.
</LI>
<LI> Then we "switch out", i.e. fall asleep, until woken up by the
interrupt handler. If we didn't mark ourselves as <CODE>TASK_INTERRUPTIBLE</CODE>
then the scheduler could schedule us sooner than when the data is
available, thus causing unneeded processing.</LI>
</OL>
</P>
<P>It is also worth pointing out that, using wait queues, it is rather easy to
implement the <B>poll(2)</B> system call:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
static unsigned int rtc_poll(struct file *file, poll_table *wait)
{
        unsigned long l;

        poll_wait(file, &amp;rtc_wait, wait);

        spin_lock_irq(&amp;rtc_lock);
        l = rtc_irq_data;
        spin_unlock_irq(&amp;rtc_lock);

        if (l != 0)
                return POLLIN | POLLRDNORM;
        return 0;
}
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>All the work is done by the device-independent function <CODE>poll_wait()</CODE> which does
the necessary waitqueue manipulations; all we need to do is point it to the
waitqueue which is woken up by our device-specific interrupt handler.</P>

<H2><A NAME="ss2.6">2.6 Kernel Timers</A>
</H2>


<P>Now let us turn our attention to kernel timers. Kernel timers are used to
dispatch execution of a particular function (called 'timer handler') at a 
specified time in the future. The main data structure is <CODE>struct timer_list</CODE>
declared in <CODE>include/linux/timer.h</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct timer_list {
        struct list_head list;
        unsigned long expires;
        unsigned long data;
        void (*function)(unsigned long);
        volatile int running;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The <CODE>list</CODE> field is for linking into the internal list, protected by the
<CODE>timerlist_lock</CODE> spinlock. The <CODE>expires</CODE> field is the value of <CODE>jiffies</CODE> when
the <CODE>function</CODE> handler should be invoked with <CODE>data</CODE> passed as a parameter.
The <CODE>running</CODE> field is used on SMP to test if the timer handler is currently
running on another CPU.</P>
<P>The functions <CODE>add_timer()</CODE> and <CODE>del_timer()</CODE> add and remove a given timer to the
list. When a timer expires, it is removed automatically. Before a timer is
used, it MUST be initialised by means of <CODE>init_timer()</CODE> function. And before it
is added, the fields <CODE>function</CODE> and <CODE>expires</CODE> must be set.</P>

<H2><A NAME="ss2.7">2.7 Bottom Halves</A>
</H2>


<P>Sometimes it is reasonable to split the amount of work to be performed inside
an interrupt handler into immediate work (e.g. acknowledging the interrupt,
updating the stats etc.) and work which can be postponed until later, when
interrupts are enabled (e.g. to do some postprocessing on data, wake up
processes waiting for this data, etc).</P>
<P>Bottom halves are the oldest mechanism for deferred execution of kernel
tasks and have been available since Linux 1.x. In Linux 2.0, a new mechanism
was added, called 'task queues', which will be the subject of next section.</P>
<P>Bottom halves are serialised by the <CODE>global_bh_lock</CODE> spinlock, i.e.
there can only be one bottom half running on any CPU at a time. However,
when attempting to execute the handler, if <CODE>global_bh_lock</CODE> is not available,
the bottom half is marked (i.e. scheduled) for execution - so processing can
continue, as opposed to a busy loop on <CODE>global_bh_lock</CODE>.</P>
<P>There can only be 32 bottom halves registered in total. 
The functions required to manipulate bottom halves are as follows (all
exported to modules):</P>
<P>
<UL>
<LI> <CODE>void init_bh(int nr, void (*routine)(void))</CODE>: installs a bottom half
handler pointed to by <CODE>routine</CODE> argument into slot <CODE>nr</CODE>. The slot
ought to be enumerated in <CODE>include/linux/interrupt.h</CODE> in the form
<CODE>XXXX_BH</CODE>, e.g. <CODE>TIMER_BH</CODE> or <CODE>TQUEUE_BH</CODE>. Typically, a subsystem's
initialisation routine (<CODE>init_module()</CODE> for modules) installs the
required bottom half using this function.
</LI>
<LI> <CODE>void remove_bh(int nr)</CODE>: does the opposite of <CODE>init_bh()</CODE>, i.e.
de-installs bottom half installed at slot <CODE>nr</CODE>. There is no error
checking performed there, so, for example <CODE>remove_bh(32)</CODE> will
panic/oops the system. Typically, a subsystem's cleanup routine
(<CODE>cleanup_module()</CODE> for modules) uses this function to free up the slot
that can later be reused by some other subsystem. (TODO: wouldn't it
be nice to have <CODE>/proc/bottom_halves</CODE> list all registered bottom
halves on the system? That means <CODE>global_bh_lock</CODE> must be made
read/write, obviously)
</LI>
<LI> <CODE>void mark_bh(int nr)</CODE>: marks bottom half in slot <CODE>nr</CODE> for execution. Typically,
an interrupt handler will mark its bottom half (hence the name!) for
execution at a "safer time".
</LI>
</UL>
</P>
<P>Bottom halves are globally locked tasklets, so the question "when are bottom
half handlers executed?" is really "when are tasklets executed?". And the
answer is, in two places: a) on each <CODE>schedule()</CODE> and b) on each
interrupt/syscall return path in <CODE>entry.S</CODE> (TODO: therefore, the <CODE>schedule()</CODE>
case is really boring - it like adding yet another very very slow interrupt,
why not get rid of <CODE>handle_softirq</CODE> label from <CODE>schedule()</CODE> altogether?).</P>


<H2><A NAME="ss2.8">2.8 Task Queues</A>
</H2>


<P>Task queues can be though of as a dynamic extension to old bottom halves. In
fact, in the source code they are sometimes referred to as "new" bottom
halves. More specifically, the old bottom halves discussed in previous
section have these limitations:</P>
<P>
<OL>
<LI> There are only a fixed number (32) of them.
</LI>
<LI> Each bottom half can only be associated with one handler function.
</LI>
<LI> Bottom halves are consumed with a spinlock held so they cannot block.</LI>
</OL>
</P>
<P>So, with task queues, arbitrary number of functions can be chained and
processed one after another at a later time. One creates a new task queue
using the <CODE>DECLARE_TASK_QUEUE()</CODE> macro and queues a task onto it using
the <CODE>queue_task()</CODE> function. The task queue then can be processed using
<CODE>run_task_queue()</CODE>. Instead of creating your own task queue (and
having to consume it manually) you can use one of Linux' predefined
task queues which are consumed at well-known points:</P>
<P>
<OL>
<LI> <B>tq_timer</B>: the timer task queue, run on each timer interrupt
and when releasing a tty device (closing or releasing a half-opened
terminal device). Since the timer handler runs in interrupt context,
the <CODE>tq_timer</CODE> tasks also run in interrupt context and thus cannot block.
</LI>
<LI> <B>tq_scheduler</B>: the scheduler task queue, consumed by the scheduler (and also
when closing tty devices, like <CODE>tq_timer</CODE>). Since the scheduler executed
in the context of the process being re-scheduled, the <CODE>tq_scheduler</CODE>
tasks can do anything they like, i.e. block, use process context data
(but why would they want to), etc.
</LI>
<LI> <B>tq_immediate</B>: this is really a bottom half <CODE>IMMEDIATE_BH</CODE>, so
drivers can <CODE>queue_task(task, &amp;tq_immediate)</CODE> and then
<CODE>mark_bh(IMMEDIATE_BH)</CODE> to be consumed in interrupt context.
</LI>
<LI> <B>tq_disk</B>: used by low level block device access (and RAID) to start
the actual requests. This task queue is exported to modules but shouldn't
be used except for the special purposes which it was designed for.</LI>
</OL>
</P>
<P>Unless a driver uses its own task queues, it does not need to call
<CODE>run_tasks_queues()</CODE> to process the queue, except under circumstances explained
below.</P>
<P>The reason <CODE>tq_timer/tq_scheduler</CODE> task queues are consumed not only in the
usual places but elsewhere (closing tty device is but one example) becomes
clear if one remembers that the driver can schedule tasks on the queue, and these tasks 
only make sense while a particular instance of the device is still valid
- which usually means until the application closes it. So, the driver may
need to call <CODE>run_task_queue()</CODE> to flush the tasks it (and anyone else) has
put on the queue, because allowing them to run at a later time may make no
sense - i.e. the relevant data structures may have been freed/reused by a
different instance. This is the reason you see <CODE>run_task_queue()</CODE> on <CODE>tq_timer</CODE>
and <CODE>tq_scheduler</CODE> in places other than timer interrupt and <CODE>schedule()</CODE>
respectively.</P>

<H2><A NAME="ss2.9">2.9 Tasklets</A>
</H2>


<P>Not yet, will be in future revision.</P>

<H2><A NAME="ss2.10">2.10 Softirqs</A>
</H2>


<P>Not yet, will be in future revision.</P>

<H2><A NAME="ss2.11">2.11 How System Calls Are Implemented on i386 Architecture?</A>
</H2>


<P>There are two mechanisms under Linux for implementing system calls:</P>
<P>
<UL>
<LI> lcall7/lcall27 call gates;</LI>
<LI> int 0x80 software interrupt.</LI>
</UL>
</P>
<P>Native Linux programs use int 0x80 whilst binaries from foreign flavours
of UNIX (Solaris, UnixWare 7 etc.) use the lcall7 mechanism. The name 'lcall7' is
historically misleading because it also covers lcall27 (e.g. Solaris/x86), but
the handler function is called lcall7_func. </P>
<P>When the system boots, the function <CODE>arch/i386/kernel/traps.c:trap_init()</CODE> is
called which sets up the IDT so that vector 0x80 (of type 15, dpl 3) points to
the address of system_call entry from <CODE>arch/i386/kernel/entry.S</CODE>.</P>
<P>When a userspace application makes a system call, the arguments are passed via registers
and the application executes 'int 0x80' instruction. This causes a trap into
kernel mode and processor jumps to system_call entry point in <CODE>entry.S</CODE>.
What this does is:</P>
<P>
<OL>
<LI> Save registers.
</LI>
<LI> Set %ds and %es to KERNEL_DS, so that all data (and extra segment)
references are made in kernel address space.
</LI>
<LI> If the value of %eax is greater than <CODE>NR_syscalls</CODE> (currently 256),
fail with <CODE>ENOSYS</CODE> error.
</LI>
<LI> If the task is being ptraced (<CODE>tsk->ptrace &amp; PF_TRACESYS</CODE>), do special
processing. This is to support programs like strace (analogue of
SVR4 <B>truss(1)</B>) or debuggers.
</LI>
<LI> Call <CODE>sys_call_table+4*(syscall_number from %eax)</CODE>. This table is
initialised in the same file (<CODE>arch/i386/kernel/entry.S</CODE>) to point to
individual system call handlers which under Linux are (usually)
prefixed with <CODE>sys_</CODE>, e.g. <CODE>sys_open</CODE>, <CODE>sys_exit</CODE>, etc. These C system
call handlers will find their arguments on the stack where <CODE>SAVE_ALL</CODE>
stored them.
</LI>
<LI> Enter 'system call return path'. This is a separate label because it
is used not only by int 0x80 but also by lcall7, lcall27. This is
concerned with handling tasklets (including bottom halves), checking
if a <CODE>schedule()</CODE> is needed (<CODE>tsk->need_resched != 0</CODE>), checking if there
are signals pending and if so handling them.</LI>
</OL>
</P>
<P>Linux supports up to 6 arguments for system calls. They are passed in
%ebx, %ecx, %edx, %esi, %edi (and %ebp used temporarily, see <CODE>_syscall6()</CODE> in
<CODE>asm-i386/unistd.h</CODE>). The system call number is passed via %eax.</P>

<H2><A NAME="ss2.12">2.12 Atomic Operations</A>
</H2>


<P>There are two types of atomic operations: bitmaps and <CODE>atomic_t</CODE>. Bitmaps are
very convenient for maintaining a concept of "allocated" or "free" units
from some large collection where each unit is identified by some number, for
example free inodes or free blocks. They are also widely used for simple
locking, for example to provide exclusive access to open a device. An example
of this can be found in <CODE>arch/i386/kernel/microcode.c</CODE>:</P>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/*
 *  Bits in microcode_status. (31 bits of room for future expansion)
 */
#define MICROCODE_IS_OPEN       0       /* set if device is in use */

static unsigned long microcode_status;
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>There is no need to initialise <CODE>microcode_status</CODE> to 0 as BSS is zero-cleared
under Linux explicitly. </P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/*
 * We enforce only one user at a time here with open/close.
 */
static int microcode_open(struct inode *inode, struct file *file)
{
        if (!capable(CAP_SYS_RAWIO))
                return -EPERM;

        /* one at a time, please */
        if (test_and_set_bit(MICROCODE_IS_OPEN, &amp;microcode_status))
                return -EBUSY;

        MOD_INC_USE_COUNT;
        return 0;
}
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The operations on bitmaps are:</P>
<P>
<UL>
<LI> <B>void set_bit(int nr, volatile void *addr)</B>: set bit <CODE>nr</CODE>
in the bitmap pointed to by <CODE>addr</CODE>.
</LI>
<LI> <B>void clear_bit(int nr, volatile void *addr)</B>: clear bit
<CODE>nr</CODE> in the bitmap pointed to by <CODE>addr</CODE>.
</LI>
<LI> <B>void change_bit(int nr, volatile void *addr)</B>: toggle bit
<CODE>nr</CODE> (if set clear, if clear set) in the bitmap pointed to by <CODE>addr</CODE>.
</LI>
<LI> <B>int test_and_set_bit(int nr, volatile void *addr)</B>:
atomically set bit <CODE>nr</CODE> and return the old bit value.
</LI>
<LI> <B>int test_and_clear_bit(int nr, volatile void *addr)</B>:
atomically clear bit <CODE>nr</CODE> and return the old bit value.
</LI>
<LI> <B>int test_and_change_bit(int nr, volatile void *addr)</B>:
atomically toggle bit <CODE>nr</CODE> and return the old bit value.</LI>
</UL>
</P>
<P>These operations use the <CODE>LOCK_PREFIX</CODE> macro, which on SMP kernels evaluates to
bus lock instruction prefix and to nothing on UP. This guarantees atomicity
of access in SMP environment.</P>
<P>Sometimes bit manipulations are not convenient, but instead we need to perform
arithmetic operations - add, subtract, increment decrement. The typical cases
are reference counts (e.g. for inodes). This facility is provided by the
<CODE>atomic_t</CODE> data type and the following operations:</P>
<P>
<UL>
<LI> <B>atomic_read(&amp;v)</B>: read the value of <CODE>atomic_t</CODE> variable <CODE>v</CODE>.
</LI>
<LI> <B>atomic_set(&amp;v, i)</B>: set the value of <CODE>atomic_t</CODE> variable
<CODE>v</CODE> to integer <CODE>i</CODE>.
</LI>
<LI> <B>void atomic_add(int i, volatile atomic_t *v)</B>: add integer
<CODE>i</CODE> to the value of atomic variable pointed to by <CODE>v</CODE>.
</LI>
<LI> <B>void atomic_sub(int i, volatile atomic_t *v)</B>: subtract
integer <CODE>i</CODE> from the value of atomic variable pointed to by <CODE>v</CODE>.
</LI>
<LI> <B>int atomic_sub_and_test(int i, volatile atomic_t *v)</B>:
subtract integer <CODE>i</CODE> from the value of atomic variable pointed to by
<CODE>v</CODE>; return 1 if the new value is 0, return 0 otherwise.
</LI>
<LI> <B>void atomic_inc(volatile atomic_t *v)</B>: increment the value
by 1.
</LI>
<LI> <B>void atomic_dec(volatile atomic_t *v)</B>: decrement the value
by 1.
</LI>
<LI> <B>int atomic_dec_and_test(volatile atomic_t *v)</B>: decrement
the value; return 1 if the new value is 0, return 0 otherwise.
</LI>
<LI> <B>int atomic_inc_and_test(volatile atomic_t *v)</B>: increment
the value; return 1 if the new value is 0, return 0 otherwise.
</LI>
<LI> <B>int atomic_add_negative(int i, volatile atomic_t *v)</B>: add
the value of <CODE>i</CODE> to <CODE>v</CODE> and return 1 if the result is negative. Return
0 if the result is greater than or equal to 0. This operation is used
for implementing semaphores.</LI>
</UL>
</P>

<H2><A NAME="ss2.13">2.13 Spinlocks, Read-write Spinlocks and Big-Reader Spinlocks</A>
</H2>


<P>Since the early days of Linux support (early 90s, this century),
developers were faced with the classical problem of accessing shared data
between different types of context (user process vs
interrupt) and different instances of the same context from multiple cpus.</P>
<P>SMP support was added to Linux 1.3.42 on 15 Nov 1995 (the original patch
was made to 1.3.37 in October the same year).</P>
<P>If the critical region of code may be executed by either process context
and interrupt context, then the way to protect it using <CODE>cli/sti</CODE> instructions
on UP is:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
unsigned long flags;

save_flags(flags);
cli();
/* critical code */
restore_flags(flags);
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>While this is ok on UP, it obviously is of no use on SMP because the same
code sequence may be executed simultaneously on another cpu, and while <CODE>cli()</CODE>
provides protection against races with interrupt context on each CPU individually, it
provides no protection at all against races between contexts running on different
CPUs. This is where spinlocks are useful for.</P>
<P>There are three types of spinlocks: vanilla (basic), read-write and
big-reader spinlocks. Read-write spinlocks should be used when there is a
natural tendency of 'many readers and few writers'. Example of this is
access to the list of registered filesystems (see <CODE>fs/super.c</CODE>). The list is
guarded by the <CODE>file_systems_lock</CODE> read-write spinlock because one needs exclusive
access only when registering/unregistering a filesystem, but any process can
read the file <CODE>/proc/filesystems</CODE> or use the <B>sysfs(2)</B> system call to force a
read-only scan of the file_systems list. This makes it sensible to use
read-write spinlocks. With read-write spinlocks, one can have multiple
readers at a time but only one writer and there can be no readers while
there is
a writer. Btw, it would be nice if new readers would not get a lock while
there
is a writer trying to get a lock, i.e. if Linux could correctly deal with
the issue of potential writer starvation by multiple readers. 
This would mean that readers must be blocked while there is a writer
attempting to get the lock. This is not
currently the case and it is not obvious whether this should be fixed - the
argument to the contrary is - readers usually take the lock for a very short
time so should they really be starved while the writer takes the lock for
potentially longer periods?</P>
<P>Big-reader spinlocks are a form of read-write spinlocks
heavily optimised for very light read access, with a penalty for writes.
There is a limited number of big-reader spinlocks - currently only two exist,
of which one is used only on sparc64 (global irq) and the other is used for
networking. In all other cases where the access pattern does not fit into
any of these two scenarios, one should use basic spinlocks. You cannot block
while holding any kind of spinlock.</P>
<P>Spinlocks come in three flavours: plain, <CODE>_irq()</CODE> and <CODE>_bh()</CODE>.</P>
<P>
<OL>
<LI> Plain <CODE>spin_lock()/spin_unlock()</CODE>: if you know the interrupts are always
disabled or if you do not race with interrupt context (e.g. from
within interrupt handler), then you can use this one. It does not
touch interrupt state on the current CPU.
</LI>
<LI> <CODE>spin_lock_irq()/spin_unlock_irq()</CODE>: if you know that interrupts are
always enabled then you can use this version, which simply disables 
(on lock) and re-enables (on unlock) interrupts on the current CPU.
For example, <CODE>rtc_read()</CODE> uses
<CODE>spin_lock_irq(&amp;rtc_lock)</CODE> (interrupts are always enabled inside
<CODE>read()</CODE>) whilst <CODE>rtc_interrupt()</CODE> uses
<CODE>spin_lock(&amp;rtc_lock)</CODE> (interrupts are always disabled inside 
interrupt handler). Note that <CODE>rtc_read()</CODE> uses <CODE>spin_lock_irq()</CODE> and not
the more generic <CODE>spin_lock_irqsave()</CODE> because on entry to any system
call interrupts are always enabled.
</LI>
<LI> <CODE>spin_lock_irqsave()/spin_unlock_irqrestore()</CODE>: the strongest form,
to be used when the interrupt state is not known, but only if
interrupts matter at all, i.e. there is no point in using it if
our interrupt handlers don't execute any critical code.</LI>
</OL>
</P>
<P>The reason you cannot use plain <CODE>spin_lock()</CODE> if you race against interrupt handlers is because if you take it and then
an interrupt comes in on the same CPU, it will busy wait for the lock forever:
the lock holder, having been interrupted, will not continue until the
interrupt handler returns.</P>
<P>The most common usage of a spinlock is to access a data structure shared
between user process context and interrupt handlers:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
spinlock_t my_lock = SPIN_LOCK_UNLOCKED;

my_ioctl()
{
        spin_lock_irq(&amp;my_lock);
        /* critical section */
        spin_unlock_irq(&amp;my_lock);
}

my_irq_handler()
{
        spin_lock(&amp;lock);
        /* critical section */
        spin_unlock(&amp;lock);
}
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>There are a couple of things to note about this example:</P>
<P>
<OL>
<LI> The process context, represented here as a typical driver method -
<CODE>ioctl()</CODE> (arguments and return values omitted for clarity), must
use <CODE>spin_lock_irq()</CODE> because it knows that interrupts are always
enabled while executing the device <CODE>ioctl()</CODE> method.
</LI>
<LI> Interrupt context, represented here by <CODE>my_irq_handler()</CODE> (again
arguments omitted for clarity) can use plain <CODE>spin_lock()</CODE> form because
interrupts are disabled inside an interrupt handler.</LI>
</OL>
</P>

<H2><A NAME="ss2.14">2.14 Semaphores and read/write Semaphores</A>
</H2>


<P>Sometimes, while accessing a shared data structure, one must perform operations
that can block, for example copy data to userspace. The locking primitive
available for such scenarios under Linux is called a semaphore. There are two
types of semaphores: basic and read-write semaphores. Depending on the
initial value of the semaphore, they can be used for either mutual exclusion
(initial value of 1) or to provide more sophisticated type of access.</P>
<P>Read-write semaphores differ from basic semaphores in the same way as
read-write spinlocks differ from basic spinlocks: one can have multiple
readers at a time but only one writer and there can be no readers while there are
writers - i.e. the writer blocks all readers and new readers block while a
writer is waiting.</P>
<P>Also, basic semaphores can be interruptible - just use the operations
<CODE>down/up_interruptible()</CODE> instead of the plain <CODE>down()/up()</CODE> and check the
value returned from <CODE>down_interruptible()</CODE>: it will be non zero if the operation was
interrupted.</P>
<P>Using semaphores for mutual exclusion is ideal in situations where a critical
code section may call by reference unknown functions registered by other
subsystems/modules, i.e. the caller cannot know apriori whether the function
blocks or not.</P>
<P>A simple example of semaphore usage is in <CODE>kernel/sys.c</CODE>, implementation of
<B>gethostname(2)/sethostname(2)</B> system calls.</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
asmlinkage long sys_sethostname(char *name, int len)
{
        int errno;

        if (!capable(CAP_SYS_ADMIN))
                return -EPERM;
        if (len &lt; 0 || len > __NEW_UTS_LEN)
                return -EINVAL;
        down_write(&amp;uts_sem);
        errno = -EFAULT;
        if (!copy_from_user(system_utsname.nodename, name, len)) {
                system_utsname.nodename[len] = 0;
                errno = 0;
        }
        up_write(&amp;uts_sem);
        return errno;
}

asmlinkage long sys_gethostname(char *name, int len)
{
        int i, errno;

        if (len &lt; 0)
                return -EINVAL;
        down_read(&amp;uts_sem);
        i = 1 + strlen(system_utsname.nodename);
        if (i > len)
                i = len;
        errno = 0;
        if (copy_to_user(name, system_utsname.nodename, i))
                errno = -EFAULT;
        up_read(&amp;uts_sem);
        return errno;
}
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The points to note about this example are:</P>
<P>
<OL>
<LI> The functions may block while copying data from/to userspace in
<CODE>copy_from_user()/copy_to_user()</CODE>. Therefore they could not use any form
of spinlock here.
</LI>
<LI> The semaphore type chosen is read-write as opposed to basic because
there may be lots of concurrent <B>gethostname(2)</B> requests which need not
be mutually exclusive.</LI>
</OL>
</P>
<P>Although Linux implementation of semaphores and read-write semaphores is
very sophisticated, there are possible scenarios one can think of which are
not yet implemented, for example there is no concept of interruptible
read-write semaphores. This is obviously because there are no real-world
situations which require these exotic flavours of the primitives.</P>

<H2><A NAME="ss2.15">2.15 Kernel Support for Loading Modules</A>
</H2>


<P>Linux is a monolithic operating system and despite all the modern hype about
some "advantages" offered by operating systems based on micro-kernel design,
the truth remains (quoting Linus Torvalds himself):</P>
<P>
<BLOCKQUOTE><CODE>
... message passing as the fundamental operation of the OS is just an
exercise in computer science masturbation. It may feel good, but you
don't actually get anything DONE.
</CODE></BLOCKQUOTE>
</P>
<P>Therefore, Linux is and will always be based on a monolithic design, which
means that all subsystems run in the same privileged mode and share the same
address space; communication between them is achieved by the usual C function
call means.</P>
<P>However, although separating kernel functionality into separate "processes"
as is done in micro-kernels is definitely a bad idea, separating it into
dynamically loadable on demand kernel modules is desirable in some
circumstances (e.g. on machines with low memory or for installation kernels
which could otherwise contain ISA auto-probing device drivers that are
mutually exclusive). The decision whether to include support for loadable
modules is made at compile time and is determined by the <CODE>CONFIG_MODULES</CODE>
option. Support for module autoloading via <CODE>request_module()</CODE> mechanism is
a separate compilation option (<CODE>CONFIG_KMOD</CODE>).</P>
<P>The following functionality can be implemented as loadable modules under
Linux:</P>
<P>
<OL>
<LI> Character and block device drivers, including misc device drivers.
</LI>
<LI> Terminal line disciplines.
</LI>
<LI> Virtual (regular) files in <CODE>/proc</CODE> and in devfs (e.g. <CODE>/dev/cpu/microcode</CODE>
vs <CODE>/dev/misc/microcode</CODE>).
</LI>
<LI> Binary file formats (e.g. ELF, aout, etc).
</LI>
<LI> Execution domains (e.g. Linux, UnixWare7, Solaris, etc).
</LI>
<LI> Filesystems.
</LI>
<LI> System V IPC.</LI>
</OL>
</P>
<P>There a few things that cannot be implemented as modules under Linux
(probably because it makes no sense for them to be modularised):</P>
<P>
<OL>
<LI> Scheduling algorithms.
</LI>
<LI> VM policies.
</LI>
<LI> Buffer cache, page cache and other caches.</LI>
</OL>
</P>
<P>Linux provides several system calls to assist in loading modules:</P>
<P>
<OL>
<LI><CODE>caddr_t create_module(const char *name, size_t size)</CODE>: allocates 
<CODE>size</CODE> bytes using <CODE>vmalloc()</CODE> and maps a module structure at the 
beginning thereof. This new module is then linked into the list headed
by module_list. Only a process with <CODE>CAP_SYS_MODULE</CODE> can invoke this
system call, others will get <CODE>EPERM</CODE> returned.
</LI>
<LI><CODE>long init_module(const char *name, struct module *image)</CODE>: loads the
relocated module image and causes the module's initialisation routine
to be invoked. Only a process with <CODE>CAP_SYS_MODULE</CODE> can invoke this
system call, others will get <CODE>EPERM</CODE> returned.
</LI>
<LI><CODE>long delete_module(const char *name)</CODE>: attempts to unload the module.
If <CODE>name == NULL</CODE>, attempt is made to unload all unused modules.
</LI>
<LI><CODE>long query_module(const char *name, int which, void *buf, size_t bufsize, size_t *ret)</CODE>: returns information about a module
(or about all modules).</LI>
</OL>
</P>
<P>The command interface available to users consists of:</P>
<P>
<UL>
<LI><B>insmod</B>: insert a single module.
</LI>
<LI><B>modprobe</B>: insert a module including all other modules it depends
on.
          </LI>
<LI><B>rmmod</B>: remove a module.
</LI>
<LI><B>modinfo</B>: print some information about a module, e.g. author, 
description, parameters the module accepts, etc.</LI>
</UL>
</P>
<P>Apart from being able to load a module manually using either <B>insmod</B> or <B>modprobe</B>,
it is also possible to have the module inserted automatically by the kernel
when a particular functionality is required. The kernel interface for this
is the function called <CODE>request_module(name)</CODE> which is exported to modules,
so that modules can load other modules as well. The <CODE>request_module(name)</CODE>
internally creates a kernel thread which execs the userspace command
<B>modprobe -s -k module_name</B>, using the standard <CODE>exec_usermodehelper()</CODE> kernel
interface (which is also exported to modules). The function returns 0 on
success, however it is usually not worth checking the return code from
<CODE>request_module()</CODE>. Instead, the programming idiom is:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
if (check_some_feature() == NULL)
    request_module(module);
if (check_some_feature() == NULL)
    return -ENODEV;
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>For example, this is done by <CODE>fs/block_dev.c:get_blkfops()</CODE> to load a module
<CODE>block-major-N</CODE> when attempt is made to open a block device with major <CODE>N</CODE>.
Obviously, there is no such module called <CODE>block-major-N</CODE> (Linux developers
only chose sensible names for their modules) but it is mapped to a proper
module name using the file <CODE>/etc/modules.conf</CODE>. However, for most well-known
major numbers (and other kinds of modules) the <B>modprobe/insmod</B> commands
know which real module to load without needing an explicit alias statement
in <CODE>/etc/modules.conf</CODE>.</P>
<P>A good example of loading a module is inside the <B>mount(2)</B> system call. The
<B>mount(2)</B> system call accepts the filesystem type as a string which
<CODE>fs/super.c:do_mount()</CODE> then passes on to <CODE>fs/super.c:get_fs_type()</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
static struct file_system_type *get_fs_type(const char *name)
{
        struct file_system_type *fs;

        read_lock(&amp;file_systems_lock);
        fs = *(find_filesystem(name));
        if (fs &amp;&amp; !try_inc_mod_count(fs->owner))
                fs = NULL;
        read_unlock(&amp;file_systems_lock);
        if (!fs &amp;&amp; (request_module(name) == 0)) {
                read_lock(&amp;file_systems_lock);
                fs = *(find_filesystem(name));
                if (fs &amp;&amp; !try_inc_mod_count(fs->owner))
                        fs = NULL;
                read_unlock(&amp;file_systems_lock);
        }
        return fs;
}
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>A few things to note in this function:</P>
<P>
<OL>
<LI> First we attempt to find the filesystem with the given name amongst
those already registered. This is done under protection of
<CODE>file_systems_lock</CODE> taken for read (as we are not modifying the list
of registered filesystems).
</LI>
<LI> If such a filesystem is found then we attempt to get a new reference
to it by trying to increment its module's hold count. This always
returns 1 for statically linked filesystems or for modules not
presently being deleted. If <CODE>try_inc_mod_count()</CODE> returned 0 then
we consider it a failure - i.e. if the module is there but is being
deleted, it is as good as if it were not there at all.
 </LI>
<LI> We drop the <CODE>file_systems_lock</CODE> because what we are about to do next
(<CODE>request_module()</CODE>) is a blocking operation, and therefore we can't
hold a spinlock over it. Actually, in this specific case, we would
have to drop <CODE>file_systems_lock</CODE> anyway, even if <CODE>request_module()</CODE> were
guaranteed to be non-blocking and the module loading were executed
in the same context atomically. The reason for this is that the module's
initialisation function will try to call <CODE>register_filesystem()</CODE>, which will 
take the same <CODE>file_systems_lock</CODE> read-write spinlock for write.
</LI>
<LI> If the attempt to load was successful, then we take the
<CODE>file_systems_lock</CODE> spinlock and try to locate the newly registered
filesystem in the list. Note that this is slightly wrong because
it is in principle possible for a bug in modprobe command to cause
it to coredump after it successfully loaded the requested module, in
which case <CODE>request_module()</CODE> will fail even though the new filesystem will be
registered, and yet <CODE>get_fs_type()</CODE> won't find it.
</LI>
<LI> If the filesystem is found and we are able to get a reference to it,
we return it. Otherwise we return NULL.</LI>
</OL>
</P>
<P>When a module is loaded into the kernel, it can refer to any symbols that
are exported as public by the kernel using <CODE>EXPORT_SYMBOL()</CODE> macro or by
other currently loaded modules. If the module uses symbols from another
module, it is marked as depending on that module during dependency
recalculation, achieved by running <B>depmod -a</B> command on boot (e.g. after
installing a new kernel).</P>
<P>Usually, one must match the set of modules with the version of the kernel
interfaces they use, which under Linux simply means the "kernel version" as
there is no special kernel interface versioning mechanism in general.
However, there is a limited functionality called "module versioning" or
<CODE>CONFIG_MODVERSIONS</CODE> which allows to avoid recompiling modules when switching
to a new kernel. What happens here is that the kernel symbol table is treated
differently for internal access and for access from modules. The elements of
public (i.e. exported) part of the symbol table are built by 32bit
checksumming the C declaration. So, in order to resolve a symbol used by a
module during loading, the loader must match the full representation of the
symbol that includes the checksum; it will refuse to load the module if these
symbols differ. This
only happens when both the kernel and the module are compiled with module
versioning enabled. If either one of them uses the original symbol names,
the loader simply tries to match the kernel version declared by the module
and the one exported by the kernel and refuses to load if they differ.</P>

<H2><A NAME="s3">3. Virtual Filesystem (VFS)</A></H2>



<H2><A NAME="ss3.1">3.1 Inode Caches and Interaction with Dcache</A>
</H2>


<P>In order to support multiple filesystems, Linux contains a special kernel
interface level called VFS (Virtual Filesystem Switch). This is similar
to the vnode/vfs interface found in SVR4 derivatives (originally it came from
BSD and Sun original implementations).</P>
<P>Linux inode cache is implemented in a single file, <CODE>fs/inode.c</CODE>, which consists
of 977 lines of code. It is interesting to note that not many changes have been
made to it for the last 5-7 years: one can still recognise some of the code
comparing the latest version with, say, 1.3.42.</P>
<P>The structure of Linux inode cache is as follows:</P>
<P>
<OL>
<LI> A global hashtable, <CODE>inode_hashtable</CODE>, where each inode is hashed by the
value of the superblock pointer and 32bit inode number. Inodes without a
superblock (<CODE>inode->i_sb == NULL</CODE>) are added to a doubly linked list
headed by <CODE>anon_hash_chain</CODE> instead. Examples of anonymous inodes
are sockets created by <CODE>net/socket.c:sock_alloc()</CODE>, by calling
<CODE>fs/inode.c:get_empty_inode()</CODE>.
</LI>
<LI> A global type in_use list (<CODE>inode_in_use</CODE>), which contains valid inodes
with <CODE>i_count>0</CODE> and <CODE>i_nlink>0</CODE>. Inodes newly allocated by
<CODE>get_empty_inode()</CODE> and <CODE>get_new_inode()</CODE> are added to the <CODE>inode_in_use</CODE> list.
</LI>
<LI> A global type unused list (<CODE>inode_unused</CODE>), which contains valid inodes
with <CODE>i_count = 0</CODE>.
</LI>
<LI> A per-superblock type dirty list (<CODE>sb->s_dirty</CODE>) which contains valid
inodes with <CODE>i_count>0</CODE>, <CODE>i_nlink>0</CODE> and <CODE>i_state &amp; I_DIRTY</CODE>.
When inode is marked
dirty, it is added to the <CODE>sb->s_dirty</CODE> list if it is also hashed.
Maintaining a per-superblock dirty list of inodes allows to quickly
sync inodes.
</LI>
<LI> Inode cache proper - a SLAB cache called <CODE>inode_cachep</CODE>. As inode
objects are allocated and freed, they are taken from and returned to
this SLAB cache.</LI>
</OL>
</P>
<P>The type lists are anchored from <CODE>inode->i_list</CODE>, the hashtable from
<CODE>inode->i_hash</CODE>. Each inode can be on a hashtable and one and only one type
(in_use, unused or dirty) list.</P>
<P>All these lists are protected by a single spinlock: <CODE>inode_lock</CODE>.</P>
<P>The inode cache subsystem is initialised when <CODE>inode_init()</CODE> function is called from
<CODE>init/main.c:start_kernel()</CODE>. The function is marked as <CODE>__init</CODE>, which means
its code is thrown away later on. It is passed a single argument - the
number of physical pages on the system. This is so that the inode cache can
configure itself depending on how much memory is available, i.e. create
a larger hashtable if there is enough memory.</P>
<P>The only stats information about inode cache is the number of unused inodes,
stored in <CODE>inodes_stat.nr_unused</CODE> and accessible to user programs via files
<CODE>/proc/sys/fs/inode-nr</CODE> and <CODE>/proc/sys/fs/inode-state</CODE>.</P>
<P>We can examine one of the lists from <B>gdb</B> running on a live kernel thus:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
(gdb) printf "%d\n", (unsigned long)(&amp;((struct inode *)0)->i_list)
8
(gdb) p inode_unused
$34 = 0xdfa992a8
(gdb) p (struct list_head)inode_unused
$35 = {next = 0xdfa992a8, prev = 0xdfcdd5a8}
(gdb) p ((struct list_head)inode_unused).prev
$36 = (struct list_head *) 0xdfcdd5a8
(gdb) p (((struct list_head)inode_unused).prev)->prev
$37 = (struct list_head *) 0xdfb5a2e8
(gdb) set $i = (struct inode *)0xdfb5a2e0
(gdb) p $i->i_ino
$38 = 0x3bec7
(gdb) p $i->i_count
$39 = {counter = 0x0}
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>Note that we deducted 8 from the address 0xdfb5a2e8 to obtain the address of
the <CODE>struct inode</CODE> (0xdfb5a2e0) according to the definition of <CODE>list_entry()</CODE>
macro from <CODE>include/linux/list.h</CODE>.</P>
<P>To understand how inode cache works, let us trace a lifetime of an inode
of a regular file on ext2 filesystem as it is opened and closed:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
fd = open("file", O_RDONLY);
close(fd);
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The <B>open(2)</B> system call is implemented in <CODE>fs/open.c:sys_open</CODE> function and
the real work is done by <CODE>fs/open.c:filp_open()</CODE> function, which is split into
two parts:</P>
<P>
<OL>
<LI> <CODE>open_namei()</CODE>: fills in the nameidata structure containing the dentry
and vfsmount structures.
</LI>
<LI> <CODE>dentry_open()</CODE>: given a dentry and vfsmount, this function allocates a new
<CODE>struct file</CODE> and links them together; it also invokes the filesystem
specific <CODE>f_op->open()</CODE> method which was set in <CODE>inode->i_fop</CODE> when inode
was read in <CODE>open_namei()</CODE> (which provided inode via <CODE>dentry->d_inode</CODE>).</LI>
</OL>
</P>
<P>The <CODE>open_namei()</CODE> function interacts with dentry cache via <CODE>path_walk()</CODE>, which
in turn calls <CODE>real_lookup()</CODE>, which invokes the filesystem specific <CODE>inode_operations->lookup()</CODE> method.
The role of this method is to find the entry in the parent
directory with the matching name and then do <CODE>iget(sb, ino)</CODE> to get the
corresponding inode - which brings us to the inode cache. When the inode is
read in, the dentry is instantiated by means of <CODE>d_add(dentry, inode)</CODE>. While
we are at it, note that for UNIX-style filesystems which have the concept of
on-disk inode number, it is the lookup method's job to map its endianness
to current CPU format, e.g. if the inode number in raw (fs-specific) dir
entry is in little-endian 32 bit format one could do:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
unsigned long ino = le32_to_cpu(de->inode);
inode = iget(sb, ino);
d_add(dentry, inode);
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>So, when we open a file we hit <CODE>iget(sb, ino)</CODE> which is really
<CODE>iget4(sb, ino, NULL, NULL)</CODE>, which does:</P>
<P>
<OL>
<LI> Attempt to find an inode with matching superblock and inode number
in the hashtable under protection of <CODE>inode_lock</CODE>. If inode is found,
its reference count (<CODE>i_count</CODE>) is incremented; if it
was 0 prior to incrementation and the inode is not dirty, it is removed from whatever
type list (<CODE>inode->i_list</CODE>) it is currently on (it has to be
<CODE>inode_unused</CODE> list, of course) and inserted into
<CODE>inode_in_use</CODE> type list; finally, <CODE>inodes_stat.nr_unused</CODE> is decremented.
</LI>
<LI> If inode is currently locked, we wait until it is unlocked so that
<CODE>iget4()</CODE> is guaranteed to return an unlocked inode.
</LI>
<LI> If inode was not found in the hashtable then it is the first time we
encounter this inode, so we call <CODE>get_new_inode()</CODE>, passing it the pointer
to the place in the hashtable where it should be inserted to.
</LI>
<LI> <CODE>get_new_inode()</CODE> allocates a new inode from the <CODE>inode_cachep</CODE> SLAB
cache but this operation can block (<CODE>GFP_KERNEL</CODE> allocation), so it
must drop the <CODE>inode_lock</CODE> spinlock which guards the hashtable. Since it
has dropped the spinlock, it must retry searching the inode in the
hashtable afterwards; if it is found this time, it returns (after incrementing
the reference by <CODE>__iget</CODE>) the one found in the hashtable and destroys
the newly allocated one. If it is still not found in the hashtable,
then the new inode we have just allocated is the one to be used;
therefore it is initialised to the required values and the fs-specific
<CODE>sb->s_op->read_inode()</CODE> method is invoked to populate the rest of the
inode. This brings us from inode cache back to the filesystem code -
remember that we came to the inode cache when filesystem-specific
<CODE>lookup()</CODE> method invoked <CODE>iget()</CODE>. While the <CODE>s_op->read_inode()</CODE> method
is reading the inode from disk, the inode is locked (<CODE>i_state = I_LOCK</CODE>);
it is unlocked after the <CODE>read_inode()</CODE> method returns and all the waiters for it are
woken up.</LI>
</OL>
</P>
<P>Now, let's see what happens when we close this file descriptor. The <B>close(2)</B>
system call is implemented in <CODE>fs/open.c:sys_close()</CODE> function, which calls
<CODE>do_close(fd, 1)</CODE> which rips (replaces with NULL) the descriptor of the
process' file descriptor table and invokes the <CODE>filp_close()</CODE> function which does
most of the work. The interesting things happen in <CODE>fput()</CODE>, which checks if
this was the last reference to the file, and if so calls
<CODE>fs/file_table.c:_fput()</CODE> which calls <CODE>__fput()</CODE> which is where interaction with
dcache (and therefore with inode cache - remember dcache is a Master of inode
cache!) happens. The <CODE>fs/dcache.c:dput()</CODE> does <CODE>dentry_iput()</CODE> which brings us
back to inode cache via <CODE>iput(inode)</CODE> so let us understand
<CODE>fs/inode.c:iput(inode)</CODE>:</P>
<P>
<OL>
<LI> If parameter passed to us is NULL, we do absolutely nothing and return.
</LI>
<LI> if there is a fs-specific <CODE>sb->s_op->put_inode()</CODE> method, it is invoked
immediately with no spinlocks held (so it can block).
</LI>
<LI> <CODE>inode_lock</CODE> spinlock is taken and <CODE>i_count</CODE> is decremented. If this was
NOT the last reference to this inode then we simply check if
there are too many references to it and so <CODE>i_count</CODE> can wrap around
the 32 bits allocated to it and if so we print a warning and return.
Note that we call <CODE>printk()</CODE> while holding the <CODE>inode_lock</CODE> spinlock -
this is fine because <CODE>printk()</CODE> can never block, therefore it may be called in
absolutely any context (even from interrupt handlers!).
</LI>
<LI> If this was the last active reference then some work needs to be done.</LI>
</OL>
</P>
<P>The work performed by <CODE>iput()</CODE> on the last inode reference is rather complex
so we separate it into a list of its own:</P>
<P>
<OL>
<LI> If <CODE>i_nlink == 0</CODE> (e.g. the file was unlinked while we held it open)
then the inode is removed from hashtable and from its type list; if
there are any data pages held in page cache for this inode, they are
removed by means of <CODE>truncate_all_inode_pages(&amp;inode->i_data)</CODE>. Then
the filesystem-specific <CODE>s_op->delete_inode()</CODE> method is invoked,
which typically deletes the on-disk copy of the inode. If there is no
<CODE>s_op->delete_inode()</CODE> method registered by the filesystem (e.g. ramfs)
then we call <CODE>clear_inode(inode)</CODE>, which invokes <CODE>s_op->clear_inode()</CODE> if
registered and if inode corresponds to a block device, this device's
reference count is dropped by <CODE>bdput(inode->i_bdev)</CODE>.
</LI>
<LI> if <CODE>i_nlink != 0</CODE> then we check if there are other inodes in the same
hash bucket and if there is none, then if inode is not dirty we delete
it from its type list and add it to <CODE>inode_unused</CODE> list, incrementing
<CODE>inodes_stat.nr_unused</CODE>. If there are inodes in the same hashbucket
then we delete it from the type list and add to <CODE>inode_unused</CODE> list.
If this was an anonymous inode (NetApp .snapshot) then we delete it
from the type list and clear/destroy it completely.</LI>
</OL>
</P>


<H2><A NAME="ss3.2">3.2 Filesystem Registration/Unregistration</A>
</H2>


<P>The Linux kernel provides a mechanism for new filesystems to be written with
minimum effort. The historical reasons for this are:</P>
<P>
<OL>
<LI> In the world where people still use non-Linux operating systems
to protect their investment in legacy software, Linux had to provide
interoperability by supporting a great multitude of different
filesystems - most of which would not deserve to exist on their own
but only for compatibility with existing non-Linux operating systems.
</LI>
<LI> The interface for filesystem writers had to be very simple so that
people could try to reverse engineer existing proprietary filesystems
by writing read-only versions of them. Therefore Linux VFS makes it
very easy to implement read-only filesystems; 95% of the work is
to finish them by adding full write-support. As a concrete example,
I wrote read-only BFS filesystem for Linux in about 10 hours, but it
took several weeks to complete it to have full write support (and
even today some purists claim that it is not complete because "it
doesn't have compactification support").
</LI>
<LI> The VFS interface is exported, and therefore all Linux filesystems can
be implemented as modules.
</LI>
</OL>
</P>
<P>Let us consider the steps required to implement a filesystem under Linux.
The code to implement a filesystem can be either a dynamically loadable
module or statically linked into the kernel, and the way it is done under
Linux is very transparent. All that is needed is to fill in a
<CODE>struct file_system_type</CODE> structure and register it with the VFS using
the <CODE>register_filesystem()</CODE> function as in the following example from
<CODE>fs/bfs/inode.c</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
#include &lt;linux/module.h>
#include &lt;linux/init.h>

static struct super_block *bfs_read_super(struct super_block *, void *, int);

static DECLARE_FSTYPE_DEV(bfs_fs_type, "bfs", bfs_read_super);

static int __init init_bfs_fs(void)
{
        return register_filesystem(&amp;bfs_fs_type);
}

static void __exit exit_bfs_fs(void)
{
        unregister_filesystem(&amp;bfs_fs_type);
}

module_init(init_bfs_fs)
module_exit(exit_bfs_fs)
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The <CODE>module_init()/module_exit()</CODE> macros ensure that, when BFS is compiled as a
module, the functions <CODE>init_bfs_fs()</CODE> and <CODE>exit_bfs_fs()</CODE> turn into <CODE>init_module()</CODE>
and <CODE>cleanup_module()</CODE> respectively; if BFS is statically linked into the kernel,
the <CODE>exit_bfs_fs()</CODE> code vanishes as it is unnecessary.</P>
<P>The <CODE>struct file_system_type</CODE> is declared in <CODE>include/linux/fs.h</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct file_system_type {
        const char *name;
        int fs_flags;
        struct super_block *(*read_super) (struct super_block *, void *, int);
        struct module *owner;
        struct vfsmount *kern_mnt; /* For kernel mount, if it's FS_SINGLE fs */
        struct file_system_type * next;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The fields thereof are explained thus:</P>
<P>
<UL>
<LI><B>name</B>: human readable name, appears in <CODE>/proc/filesystems</CODE> file
and is used as a key to find a filesystem by its name; this same name is
used for the filesystem type in <B>mount(2)</B>, and it should be unique:  there
can (obviously) be only one filesystem with a given name. For modules,
name points to module's address spaces and not copied: this means <B>cat
/proc/filesystems</B> can oops if the module was unloaded but filesystem is
still registered.
</LI>
<LI><B>fs_flags</B>: one or more (ORed) of the flags: <CODE>FS_REQUIRES_DEV</CODE>
for filesystems that can only be mounted on a block device, <CODE>FS_SINGLE</CODE>
for filesystems that can have only one superblock, <CODE>FS_NOMOUNT</CODE> for
filesystems that cannot be mounted from userspace by means of <B>mount(2)</B>
system call: they can however be mounted internally using <CODE>kern_mount()</CODE>
interface, e.g. pipefs.
</LI>
<LI><B>read_super</B>: a pointer to the function that reads the super
block during mount operation. This function is required: if it is not
provided, mount operation (whether from userspace or inkernel) will
always fail except in <CODE>FS_SINGLE</CODE> case where it will Oops in
<CODE>get_sb_single()</CODE>, trying to dereference a NULL pointer in
<CODE>fs_type->kern_mnt->mnt_sb</CODE> with (<CODE>fs_type->kern_mnt = NULL</CODE>).
</LI>
<LI><B>owner</B>: pointer to the module that implements this filesystem.
If the filesystem is statically linked into the kernel then this is
NULL. You don't need to set this manually as the macro <CODE>THIS_MODULE</CODE>
does the right thing automatically.
</LI>
<LI><B>kern_mnt</B>: for <CODE>FS_SINGLE</CODE> filesystems only. This is set by
<CODE>kern_mount()</CODE> (TODO: <CODE>kern_mount()</CODE> should refuse to mount filesystems
if <CODE>FS_SINGLE</CODE> is not set).
</LI>
<LI><B>next</B>: linkage into singly-linked list headed by <CODE>file_systems</CODE>
(see <CODE>fs/super.c</CODE>). The list is protected by the <CODE>file_systems_lock</CODE>
read-write spinlock and functions <CODE>register/unregister_filesystem()</CODE>
modify it by linking and unlinking the entry from the list.</LI>
</UL>
</P>
<P>The job of the <CODE>read_super()</CODE> function is to fill in the fields of the superblock,
allocate root inode and initialise any fs-private information associated with
this mounted instance of the filesystem. So, typically the <CODE>read_super()</CODE> would
do:</P>
<P>
<OL>
<LI> Read the superblock from the device specified via <CODE>sb->s_dev</CODE> argument,
using buffer cache <CODE>bread()</CODE> function. If it anticipates to read a few
more subsequent metadata blocks immediately then it makes sense to
use <CODE>breada()</CODE> to schedule reading extra blocks asynchronously.
</LI>
<LI> Verify that superblock contains the valid magic number and overall
"looks" sane.
</LI>
<LI> Initialise <CODE>sb->s_op</CODE> to point to <CODE>struct super_block_operations</CODE>
structure. This structure contains filesystem-specific functions
implementing operations like "read inode", "delete inode", etc.
</LI>
<LI> Allocate root inode and root dentry using <CODE>d_alloc_root()</CODE>.
</LI>
<LI> If the filesystem is not mounted read-only then set <CODE>sb->s_dirt</CODE> to 1
and mark the buffer containing superblock dirty (TODO: why do we
do this? I did it in BFS because MINIX did it...)</LI>
</OL>
</P>

<H2><A NAME="ss3.3">3.3 File Descriptor Management</A>
</H2>


<P>Under Linux there are several levels of indirection between user file
descriptor and the kernel inode structure. When a process makes <B>open(2)</B>
system call, the kernel returns a small non-negative integer which can be
used for subsequent I/O operations on this file. This integer is an index
into an array of pointers to <CODE>struct file</CODE>. Each file structure points to
a dentry via <CODE>file->f_dentry</CODE>. And each dentry points to an inode via
<CODE>dentry->d_inode</CODE>. </P>
<P>Each task contains a field <CODE>tsk->files</CODE> which is a pointer to
<CODE>struct files_struct</CODE> defined in <CODE>include/linux/sched.h</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/*
 * Open file table structure
 */
struct files_struct {
        atomic_t count;
        rwlock_t file_lock;
        int max_fds;
        int max_fdset;
        int next_fd;
        struct file ** fd;      /* current fd array */
        fd_set *close_on_exec;
        fd_set *open_fds;
        fd_set close_on_exec_init;
        fd_set open_fds_init;
        struct file * fd_array[NR_OPEN_DEFAULT];
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The <CODE>file->count</CODE> is a reference count, incremented by <CODE>get_file()</CODE> (usually
called by <CODE>fget()</CODE>) and decremented by <CODE>fput()</CODE> and by <CODE>put_filp()</CODE>. The difference
between <CODE>fput()</CODE> and <CODE>put_filp()</CODE> is that <CODE>fput()</CODE> does more work usually needed
for regular files, such as releasing flock locks, releasing dentry, etc, while
<CODE>put_filp()</CODE> is only manipulating file table structures, i.e. decrements the
count, removes the file from the <CODE>anon_list</CODE> and adds it to the <CODE>free_list</CODE>,
under protection of <CODE>files_lock</CODE> spinlock.</P>
<P>The <CODE>tsk->files</CODE> can be shared between parent and child if the child thread
was created using <CODE>clone()</CODE> system call with <CODE>CLONE_FILES</CODE> set in the clone flags
argument. This can be seen in <CODE>kernel/fork.c:copy_files()</CODE> (called by
<CODE>do_fork()</CODE>) which only increments the <CODE>file->count</CODE> if <CODE>CLONE_FILES</CODE> is set
instead of the usual copying file descriptor table in time-honoured
tradition of classical UNIX <B>fork(2)</B>.</P>
<P>When a file is opened, the file structure allocated for it is installed into
<CODE>current->files->fd[fd]</CODE> slot and a <CODE>fd</CODE> bit is set in the bitmap
<CODE>current->files->open_fds</CODE> . All this is done under the write protection of 
<CODE>current->files->file_lock</CODE> read-write spinlock. When the descriptor is
closed a <CODE>fd</CODE> bit is cleared in <CODE>current->files->open_fds</CODE> and
<CODE>current->files->next_fd</CODE> is set equal to <CODE>fd</CODE> as a hint for finding the
first unused descriptor next time this process wants to open a file.</P>

<H2><A NAME="ss3.4">3.4 File Structure Management</A>
</H2>


<P>The file structure is declared in <CODE>include/linux/fs.h</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct fown_struct {
        int pid;                /* pid or -pgrp where SIGIO should be sent */
        uid_t uid, euid;        /* uid/euid of process setting the owner */
        int signum;             /* posix.1b rt signal to be delivered on IO */
};

struct file {
        struct list_head        f_list;
        struct dentry           *f_dentry;
        struct vfsmount         *f_vfsmnt;
        struct file_operations  *f_op;
        atomic_t                f_count;
        unsigned int            f_flags;
        mode_t                  f_mode;
        loff_t                  f_pos;
        unsigned long           f_reada, f_ramax, f_raend, f_ralen, f_rawin;
        struct fown_struct      f_owner;
        unsigned int            f_uid, f_gid;
        int                     f_error;

        unsigned long           f_version;
  
        /* needed for tty driver, and maybe others */
        void                    *private_data; 
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>Let us look at the various fields of <CODE>struct file</CODE>:</P>
<P>
<OL>
<LI><B>f_list</B>: this field links file structure on one (and only one)
of the lists: a) <CODE>sb->s_files</CODE> list of all open files on this filesystem,
if the corresponding inode is not anonymous, then <CODE>dentry_open()</CODE> (called
by <CODE>filp_open()</CODE>) links the file into this list; 
b) <CODE>fs/file_table.c:free_list</CODE>, containing unused file structures;
c) <CODE>fs/file_table.c:anon_list</CODE>, when a new file structure is created by
<CODE>get_empty_filp()</CODE> it is placed on this list. All these lists are
protected by the <CODE>files_lock</CODE> spinlock.
</LI>
<LI><B>f_dentry</B>: the dentry corresponding to this file. The dentry
is created at nameidata lookup time by <CODE>open_namei()</CODE> (or
rather <CODE>path_walk()</CODE>
which it calls) but the actual <CODE>file->f_dentry</CODE> field is set by
<CODE>dentry_open()</CODE> to contain the dentry thus found.
</LI>
<LI><B>f_vfsmnt</B>: the pointer to <CODE>vfsmount</CODE> structure of the filesystem
containing the file. This is set by <CODE>dentry_open()</CODE> but is found as part
of nameidata lookup by <CODE>open_namei()</CODE> (or rather <CODE>path_init()</CODE> which it
calls).
</LI>
<LI><B>f_op</B>: the pointer to <CODE>file_operations</CODE> which contains various
methods that can be invoked on the file. This is copied from
<CODE>inode->i_fop</CODE> which is placed there by filesystem-specific
<CODE>s_op->read_inode()</CODE> method during nameidata lookup. We will look at
<CODE>file_operations</CODE> methods in detail later on in this section.
</LI>
<LI><B>f_count</B>: reference count manipulated by
<CODE>get_file/put_filp/fput</CODE>.
</LI>
<LI><B>f_flags</B>: <CODE>O_XXX</CODE> flags from <B>open(2)</B> system call copied there
(with slight modifications by <CODE>filp_open()</CODE>) by <CODE>dentry_open()</CODE> and after
clearing <CODE>O_CREAT</CODE>, <CODE>O_EXCL</CODE>, <CODE>O_NOCTTY</CODE>, <CODE>O_TRUNC</CODE> - there is no point in
storing these flags permanently since they cannot be modified by 
<CODE>F_SETFL</CODE> (or queried by <CODE>F_GETFL</CODE>) <B>fcntl(2)</B> calls.
</LI>
<LI><B>f_mode</B>: a combination of userspace flags and mode, set
by <CODE>dentry_open()</CODE>. The point of the conversion is to store read and
write access in separate bits so one could do easy checks like
<CODE>(f_mode &amp; FMODE_WRITE)</CODE> and <CODE>(f_mode &amp; FMODE_READ)</CODE>.
</LI>
<LI><B>f_pos</B>: a current file position for next read or write to
the file. Under i386 it is of type <CODE>long long</CODE>, i.e. a 64bit value.
</LI>
<LI><B>f_reada, f_ramax, f_raend, f_ralen, f_rawin</B>: to support
readahead - too complex to be discussed by mortals ;)
</LI>
<LI><B>f_owner</B>: owner of file I/O to receive asynchronous I/O 
notifications via <CODE>SIGIO</CODE> mechanism (see <CODE>fs/fcntl.c:kill_fasync()</CODE>).
</LI>
<LI><B>f_uid, f_gid</B> - set to user id and group id of the process that
opened the file, when the file structure is created in
<CODE>get_empty_filp()</CODE>. If the file is a socket, used by ipv4 netfilter.
</LI>
<LI><B>f_error</B>: used by NFS client to return write errors. It is
set in <CODE>fs/nfs/file.c</CODE> and checked in <CODE>mm/filemap.c:generic_file_write()</CODE>.
</LI>
<LI><B>f_version</B> - versioning mechanism for invalidating caches,
incremented (using global <CODE>event</CODE>) whenever <CODE>f_pos</CODE> changes.
</LI>
<LI><B>private_data</B>: private per-file data which can be used by
filesystems (e.g. coda stores credentials here) or by device drivers.
Device drivers (in the presence of devfs) could use this field to
differentiate between multiple instances instead of the classical
minor number encoded in <CODE>file->f_dentry->d_inode->i_rdev</CODE>.
          </LI>
</OL>
</P>
<P>Now let us look at <CODE>file_operations</CODE> structure which contains the methods that
can be invoked on files. Let us recall that it is copied from <CODE>inode->i_fop</CODE>
where it is set by <CODE>s_op->read_inode()</CODE> method. It is declared in
<CODE>include/linux/fs.h</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct file_operations {
        struct module *owner;
        loff_t (*llseek) (struct file *, loff_t, int);
        ssize_t (*read) (struct file *, char *, size_t, loff_t *);
        ssize_t (*write) (struct file *, const char *, size_t, loff_t *);
        int (*readdir) (struct file *, void *, filldir_t);
        unsigned int (*poll) (struct file *, struct poll_table_struct *);
        int (*ioctl) (struct inode *, struct file *, unsigned int, unsigned long);
        int (*mmap) (struct file *, struct vm_area_struct *);
        int (*open) (struct inode *, struct file *);
        int (*flush) (struct file *);
        int (*release) (struct inode *, struct file *);
        int (*fsync) (struct file *, struct dentry *, int datasync);
        int (*fasync) (int, struct file *, int);
        int (*lock) (struct file *, int, struct file_lock *);
        ssize_t (*readv) (struct file *, const struct iovec *, unsigned long, loff_t *);
        ssize_t (*writev) (struct file *, const struct iovec *, unsigned long, loff_t *);
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>
<OL>
<LI><B>owner</B>: a pointer to the module that owns the subsystem in
question. Only drivers need to set it to <CODE>THIS_MODULE</CODE>, filesystems can
happily ignore it because their module counts are controlled at
mount/umount time whilst the drivers need to control it at open/release
time.
</LI>
<LI><B>llseek</B>: implements the <B>lseek(2)</B> system call. Usually it is
omitted and <CODE>fs/read_write.c:default_llseek()</CODE> is used, which does the
right thing (TODO: force all those who set it to NULL currently to use
default_llseek - that way we save an <CODE>if()</CODE> in <CODE>llseek()</CODE>)
</LI>
<LI><B>read</B>: implements <CODE>read(2)</CODE> system call. Filesystems can use
<CODE>mm/filemap.c:generic_file_read()</CODE> for regular files and
<CODE>fs/read_write.c:generic_read_dir()</CODE> (which simply returns <CODE>-EISDIR</CODE>)
for directories here.
</LI>
<LI><B>write</B>: implements <B>write(2)</B> system call. Filesystems can use
<CODE>mm/filemap.c:generic_file_write()</CODE> for regular files and ignore it for
directories here.
      </LI>
<LI><B>readdir</B>: used by filesystems. Ignored for regular files
and implements <B>readdir(2)</B> and <B>getdents(2)</B> system calls for directories.
</LI>
<LI><B>poll</B>: implements <B>poll(2)</B> and <B>select(2)</B> system calls.
</LI>
<LI><B>ioctl</B>: implements driver or filesystem-specific
ioctls. Note that generic file ioctls like <CODE>FIBMAP</CODE>, <CODE>FIGETBSZ</CODE>, <CODE>FIONREAD</CODE>
are implemented by higher levels so they never read <CODE>f_op->ioctl()</CODE>
method.
</LI>
<LI><B>mmap</B>: implements the <B>mmap(2)</B> system call. Filesystems can use
<B>generic_file_mmap</B> here for regular files and ignore it on directories.
</LI>
<LI><B>open</B>: called at <B>open(2)</B> time by <CODE>dentry_open()</CODE>. Filesystems
rarely use this, e.g. coda tries to cache the file locally at open
time.
</LI>
<LI><B>flush</B>: called at each <B>close(2)</B> of this file, not necessarily
the last one (see <CODE>release()</CODE> method below). The only filesystem that
uses this is NFS client to flush all dirty pages. Note that this can
return an error which will be passed back to userspace which made the
<B>close(2)</B> system call.
</LI>
<LI><B>release</B>: called at the last <B>close(2)</B> of this file, i.e. when
<CODE>file->f_count</CODE> reaches 0. Although defined as returning int, the return
value is ignored by VFS (see <CODE>fs/file_table.c:__fput()</CODE>).
</LI>
<LI><B>fsync</B>: maps directly to <B>fsync(2)/fdatasync(2)</B> system calls,
with the last argument specifying whether it is fsync or fdatasync.
Almost no work is done by VFS around this, except to map file
descriptor to a file structure (<CODE>file = fget(fd)</CODE>) and down/up
<CODE>inode->i_sem</CODE> semaphore. Ext2 filesystem currently ignores the last
argument and does exactly the same for <B>fsync(2)</B> and <B>fdatasync(2)</B>.
</LI>
<LI><B>fasync</B>: this method is called when <CODE>file->f_flags &amp; FASYNC</CODE>
changes.
</LI>
<LI><B>lock</B>: the filesystem-specific portion of the POSIX <B>fcntl(2)</B>
file region locking mechanism. The only bug here is that because it is
called before fs-independent portion (<CODE>posix_lock_file()</CODE>), if it
succeeds but the standard POSIX lock code fails then it will never be
unlocked on fs-dependent level..
      </LI>
<LI><B>readv</B>: implements <B>readv(2)</B> system call.
</LI>
<LI><B>writev</B>: implements <B>writev(2)</B> system call.</LI>
</OL>
</P>

<H2><A NAME="ss3.5">3.5 Superblock and Mountpoint Management</A>
</H2>


<P>Under Linux, information about mounted filesystems is kept in two separate
structures - <CODE>super_block</CODE> and <CODE>vfsmount</CODE>. The reason for this is that Linux
allows to mount the same filesystem (block device) under multiple mount
points, which means that the same <CODE>super_block</CODE> can correspond to multiple
<CODE>vfsmount</CODE> structures.</P>
<P>Let us look at <CODE>struct super_block</CODE> first, declared in <CODE>include/linux/fs.h</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct super_block {
        struct list_head        s_list;         /* Keep this first */
        kdev_t                  s_dev;
        unsigned long           s_blocksize;
        unsigned char           s_blocksize_bits;
        unsigned char           s_lock;
        unsigned char           s_dirt;
        struct file_system_type *s_type;
        struct super_operations *s_op;
        struct dquot_operations *dq_op;
        unsigned long           s_flags;
        unsigned long           s_magic;
        struct dentry           *s_root;
        wait_queue_head_t       s_wait;

        struct list_head        s_dirty;        /* dirty inodes */
        struct list_head        s_files;

        struct block_device     *s_bdev;
        struct list_head        s_mounts;       /* vfsmount(s) of this one */
        struct quota_mount_options s_dquot;     /* Diskquota specific options */

       union {
                struct minix_sb_info    minix_sb;
                struct ext2_sb_info     ext2_sb;
                ..... all filesystems that need sb-private info ...
                void                    *generic_sbp;
        } u;
       /*
         * The next field is for VFS *only*. No filesystems have any business
         * even looking at it. You had been warned.
         */
        struct semaphore s_vfs_rename_sem;      /* Kludge */

        /* The next field is used by knfsd when converting a (inode number based)
         * file handle into a dentry. As it builds a path in the dcache tree from
         * the bottom up, there may for a time be a subpath of dentrys which is not
         * connected to the main tree.  This semaphore ensure that there is only ever
         * one such free path per filesystem.  Note that unconnected files (or other
         * non-directories) are allowed, but not unconnected diretories.
         */
        struct semaphore s_nfsd_free_path_sem;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The various fields in the <CODE>super_block</CODE> structure are:</P>
<P>
<OL>
<LI><B>s_list</B>: a doubly-linked list of all active superblocks; note
I don't say "of all mounted filesystems" because under Linux one can
have multiple instances of a mounted filesystem corresponding to a
single superblock.
</LI>
<LI><B>s_dev</B>: for filesystems which require a block to be mounted
on, i.e. for <CODE>FS_REQUIRES_DEV</CODE> filesystems, this is the <CODE>i_dev</CODE> of the
block device. For others (called anonymous filesystems) this is an
integer <CODE>MKDEV(UNNAMED_MAJOR, i)</CODE> where <CODE>i</CODE> is the first unset bit in
<CODE>unnamed_dev_in_use</CODE> array, between 1 and 255 inclusive. See 
<CODE>fs/super.c:get_unnamed_dev()/put_unnamed_dev()</CODE>. It has been suggested
many times that anonymous filesystems should not use <CODE>s_dev</CODE> field.
      </LI>
<LI><B>s_blocksize, s_blocksize_bits</B>: blocksize and log2(blocksize).
</LI>
<LI><B>s_lock</B>: indicates whether superblock is currently locked by
<CODE>lock_super()/unlock_super()</CODE>.
</LI>
<LI><B>s_dirt</B>: set when superblock is changed, and cleared whenever
it is written back to disk.
</LI>
<LI><B>s_type</B>: pointer to <CODE>struct file_system_type</CODE> of the
corresponding filesystem. Filesystem's <CODE>read_super()</CODE> method doesn't need
to set it as VFS <CODE>fs/super.c:read_super()</CODE> sets it for you if
fs-specific <CODE>read_super()</CODE> succeeds and resets to NULL if it fails.
</LI>
<LI><B>s_op</B>: pointer to <CODE>super_operations</CODE> structure which contains
fs-specific methods to read/write inodes etc. It is the job of
filesystem's <CODE>read_super()</CODE> method to initialise <CODE>s_op</CODE> correctly.
</LI>
<LI><B>dq_op</B>: disk quota operations.
</LI>
<LI><B>s_flags</B>: superblock flags.
</LI>
<LI><B>s_magic</B>: filesystem's magic number. Used by minix filesystem
to differentiate between multiple flavours of itself.
</LI>
<LI><B>s_root</B>: dentry of the filesystem's root. It is the job of
<CODE>read_super()</CODE> to read the root inode from the disk and pass it to
<CODE>d_alloc_root()</CODE> to allocate the dentry and instantiate it. Some
filesystems spell "root" other than "/" and so use more generic
<CODE>d_alloc()</CODE> function to bind the dentry to a name, e.g. pipefs mounts
itself on "pipe:" as its own root instead of "/".
</LI>
<LI><B>s_wait</B>: waitqueue of processes waiting for superblock to be
unlocked.
</LI>
<LI><B>s_dirty</B>: a list of all dirty inodes. Recall that if inode
is dirty (<CODE>inode->i_state &amp; I_DIRTY</CODE>) then it is on superblock-specific
dirty list linked via <CODE>inode->i_list</CODE>.
</LI>
<LI><B>s_files</B>: a list of all open files on this superblock. Useful
for deciding whether filesystem can be remounted read-only, see
<CODE>fs/file_table.c:fs_may_remount_ro()</CODE> which goes through <CODE>sb->s_files</CODE> list
and denies remounting if there are files opened for write
(<CODE>file->f_mode &amp; FMODE_WRITE</CODE>) or files with pending 
unlink (<CODE>inode->i_nlink == 0</CODE>).
</LI>
<LI><B>s_bdev</B>: for <CODE>FS_REQUIRES_DEV</CODE>, this points to the block_device
structure describing the device the filesystem is mounted on.
</LI>
<LI><B>s_mounts</B>: a list of all <CODE>vfsmount</CODE> structures, one for each
mounted instance of this superblock.
</LI>
<LI><B>s_dquot</B>: more diskquota stuff.</LI>
</OL>
</P>
<P>The superblock operations are described in the <CODE>super_operations</CODE> structure
declared in <CODE>include/linux/fs.h</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct super_operations {
        void (*read_inode) (struct inode *);
        void (*write_inode) (struct inode *, int);
        void (*put_inode) (struct inode *);
        void (*delete_inode) (struct inode *);
        void (*put_super) (struct super_block *);
        void (*write_super) (struct super_block *);
        int (*statfs) (struct super_block *, struct statfs *);
        int (*remount_fs) (struct super_block *, int *, char *);
        void (*clear_inode) (struct inode *);
        void (*umount_begin) (struct super_block *);
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>
<OL>
<LI><B>read_inode</B>: reads the inode from the filesystem. It is only
called from <CODE>fs/inode.c:get_new_inode()</CODE> from <CODE>iget4()</CODE> (and therefore
<CODE>iget()</CODE>). If a filesystem wants to use <CODE>iget()</CODE> then <CODE>read_inode()</CODE> must be
implemented - otherwise <CODE>get_new_inode()</CODE> will panic.
While inode is being read it is locked (<CODE>inode->i_state = I_LOCK</CODE>). When
the function returns, all waiters on <CODE>inode->i_wait</CODE> are woken up. The job
of the filesystem's <CODE>read_inode()</CODE> method is to locate the disk block which
contains the inode to be read and use buffer cache <CODE>bread()</CODE> function to
read it in and initialise the various fields of inode structure, for
example the <CODE>inode->i_op</CODE> and <CODE>inode->i_fop</CODE> so that VFS level knows what
operations can be performed on the inode or corresponding file. 
Filesystems that don't implement <CODE>read_inode()</CODE> are ramfs and
pipefs. For example, ramfs has its own inode-generating function
<CODE>ramfs_get_inode()</CODE> with all the inode operations calling it as needed.
</LI>
<LI><B>write_inode</B>: write inode back to disk. Similar to
<CODE>read_inode()</CODE> in that it needs to locate the relevant block on
disk and interact with buffer cache by calling
<CODE>mark_buffer_dirty(bh)</CODE>. This method is called on dirty inodes
(those marked dirty with <CODE>mark_inode_dirty()</CODE>) when the inode needs
to be sync'd either individually or as part of syncing the
entire filesystem.
</LI>
<LI><B>put_inode</B>: called whenever the reference count is decreased.
</LI>
<LI><B>delete_inode</B>: called whenever both <CODE>inode->i_count</CODE> and
<CODE>inode->i_nlink</CODE> reach 0. Filesystem deletes the on-disk copy of the
inode and calls <CODE>clear_inode()</CODE> on VFS inode to "terminate it with
extreme prejudice".
</LI>
<LI><B>put_super</B>: called at the last stages of <B>umount(2)</B> system
call to notify the filesystem that any private information held by
the filesystem about this instance should be freed. Typically this
would <CODE>brelse()</CODE> the block containing the superblock and <CODE>kfree()</CODE> any
bitmaps allocated for free blocks, inodes, etc.
</LI>
<LI><B>write_super</B>: called when superblock needs to be
written back to disk. It should find the block containing the
superblock (usually kept in <CODE>sb-private</CODE> area) and
<CODE>mark_buffer_dirty(bh)</CODE> . It should also clear <CODE>sb->s_dirt</CODE> flag.
</LI>
<LI><B>statfs</B>: implements <B>fstatfs(2)/statfs(2)</B> system calls. Note
that the pointer to <CODE>struct statfs</CODE> passed as argument is a kernel
pointer, not a user pointer so we don't need to do any I/O to
userspace. If not implemented then <CODE>statfs(2)</CODE> will fail with <CODE>ENOSYS</CODE>.
</LI>
<LI><B>remount_fs</B>: called whenever filesystem is being remounted.
</LI>
<LI><B>clear_inode</B>: called from VFS level <CODE>clear_inode()</CODE>. Filesystems
that attach private data to inode structure (via <CODE>generic_ip</CODE> field) must
free it here.
</LI>
<LI><B>umount_begin</B>: called during forced umount to notify the
filesystem beforehand, so that it can do its best to make sure that
nothing keeps the filesystem busy. Currently used only by NFS. This
has nothing to do with the idea of generic VFS level forced umount
support.</LI>
</OL>
</P>
<P>So, let us look at what happens when we mount a on-disk (<CODE>FS_REQUIRES_DEV</CODE>)
filesystem. The implementation of the <B>mount(2)</B> system call is in
<CODE>fs/super.c:sys_mount()</CODE> which is the just a wrapper that copies the options,
filesystem type and device name for the <CODE>do_mount()</CODE> function which does the
real work:</P>
<P>
<OL>
<LI>Filesystem driver is loaded if needed and its module's reference count
is incremented. Note that during mount operation, the filesystem
module's reference count is incremented twice - once by <CODE>do_mount()</CODE>
calling <CODE>get_fs_type()</CODE> and once by <CODE>get_sb_dev()</CODE> calling <CODE>get_filesystem()</CODE>
if <CODE>read_super()</CODE> was successful. The first increment is to prevent
module unloading while we are inside <CODE>read_super()</CODE> method and the second
increment is to indicate that the module is in use by this mounted
instance. Obviously, <CODE>do_mount()</CODE> decrements the count before returning, so
overall the count only grows by 1 after each mount.
</LI>
<LI>Since, in our case, <CODE>fs_type->fs_flags &amp; FS_REQUIRES_DEV</CODE> is true, the
superblock is initialised by a call to <CODE>get_sb_bdev()</CODE> which obtains
the reference to the block device and interacts with the filesystem's
<CODE>read_super()</CODE> method to fill in the superblock. If all goes well, the
<CODE>super_block</CODE> structure is initialised and we have an extra reference 
to the filesystem's module and a reference to the underlying block
device.
</LI>
<LI>A new <CODE>vfsmount</CODE> structure is allocated and linked to <CODE>sb->s_mounts</CODE> list
and to the global <CODE>vfsmntlist</CODE> list. The <CODE>vfsmount</CODE> field <CODE>mnt_instances</CODE>
allows to find all instances mounted on the same superblock as this
one. The <CODE>mnt_list</CODE> field allows to find all instances for all
superblocks system-wide.  The <CODE>mnt_sb</CODE> field
points to this superblock and <CODE>mnt_root</CODE> has a new reference to the
<CODE>sb->s_root</CODE> dentry.</LI>
</OL>
</P>

<H2><A NAME="ss3.6">3.6 Example Virtual Filesystem: pipefs</A>
</H2>


<P>As a simple example of Linux filesystem that does not require a block device
for mounting, let us consider pipefs from <CODE>fs/pipe.c</CODE>. The filesystem's preamble
is rather straightforward and requires little explanation:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
static DECLARE_FSTYPE(pipe_fs_type, "pipefs", pipefs_read_super,
        FS_NOMOUNT|FS_SINGLE);

static int __init init_pipe_fs(void)
{
        int err = register_filesystem(&amp;pipe_fs_type);
        if (!err) {
                pipe_mnt = kern_mount(&amp;pipe_fs_type);
                err = PTR_ERR(pipe_mnt);
                if (!IS_ERR(pipe_mnt))
                        err = 0;
        }
        return err;
}

static void __exit exit_pipe_fs(void)
{
        unregister_filesystem(&amp;pipe_fs_type);
        kern_umount(pipe_mnt);
}

module_init(init_pipe_fs)
module_exit(exit_pipe_fs)
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>The filesystem is of type <CODE>FS_NOMOUNT|FS_SINGLE</CODE>, which means it cannot be
mounted from userspace and can only have one superblock system-wide. The
<CODE>FS_SINGLE</CODE> file also means that it must be mounted via <CODE>kern_mount()</CODE> after
it is successfully registered via <CODE>register_filesystem()</CODE>, which is exactly
what happens in <CODE>init_pipe_fs()</CODE>. The only bug in this function is that if
<CODE>kern_mount()</CODE> fails (e.g. because <CODE>kmalloc()</CODE> failed in <CODE>add_vfsmnt()</CODE>) then the
filesystem is left as registered but module initialisation fails. This will
cause <B>cat /proc/filesystems</B> to Oops. (have just sent a patch to Linus
mentioning that although this is not a real bug today as pipefs can't be
compiled as a module, it should be written with the view that in the future
it may become modularised).</P>
<P>The result of <CODE>register_filesystem()</CODE> is that <CODE>pipe_fs_type</CODE> is linked into
the <CODE>file_systems</CODE> list so one can read <CODE>/proc/filesystems</CODE> and find "pipefs"
entry in there with "nodev" flag indicating that <CODE>FS_REQUIRES_DEV</CODE> was not set.
The <CODE>/proc/filesystems</CODE> file should really be enhanced to support all the new
<CODE>FS_</CODE> flags (and I made a patch to do so) but it cannot be done because it will
break all the user applications that use it. Despite Linux kernel interfaces
changing every minute (only for the better) when it comes to the userspace
compatibility, Linux is a very conservative operating system which allows
many applications to be used for a long time without being recompiled.</P>
<P>The result of <CODE>kern_mount()</CODE> is that:</P>
<P>
<OL>
<LI>A new unnamed (anonymous) device number is allocated by setting a bit in
<CODE>unnamed_dev_in_use</CODE> bitmap; if there are no more bits then <CODE>kern_mount()</CODE>
fails with <CODE>EMFILE</CODE>.
</LI>
<LI>A new superblock structure is allocated by means of <CODE>get_empty_super()</CODE>.
The <CODE>get_empty_super()</CODE> function walks the list of superblocks headed
by <CODE>super_block</CODE> and looks for empty entry, i.e. <CODE>s->s_dev == 0</CODE>. If no
such empty superblock is found then a new one is allocated using
<CODE>kmalloc()</CODE> at <CODE>GFP_USER</CODE> priority. The maximum system-wide number of
superblocks is checked in <CODE>get_empty_super()</CODE> so if it starts failing,
one can adjust the tunable <CODE>/proc/sys/fs/super-max</CODE>.
</LI>
<LI>A filesystem-specific <CODE>pipe_fs_type->read_super()</CODE> method, i.e.
<CODE>pipefs_read_super()</CODE>, is invoked which allocates root inode and root
dentry <CODE>sb->s_root</CODE>, and sets <CODE>sb->s_op</CODE> to be <CODE>&amp;pipefs_ops</CODE>.
</LI>
<LI>Then <CODE>kern_mount()</CODE> calls <CODE>add_vfsmnt(NULL, sb->s_root, "none")</CODE> which
allocates a new <CODE>vfsmount</CODE> structure and links it into <CODE>vfsmntlist</CODE> and
<CODE>sb->s_mounts</CODE>.
</LI>
<LI>The <CODE>pipe_fs_type->kern_mnt</CODE> is set to this new <CODE>vfsmount</CODE> structure and
it is returned. The reason why the return value of <CODE>kern_mount()</CODE> is a
<CODE>vfsmount</CODE> structure is because even <CODE>FS_SINGLE</CODE> filesystems can be mounted
multiple times and so their <CODE>mnt->mnt_sb</CODE> will point to the same thing
which would be silly to return from multiple calls to <CODE>kern_mount()</CODE>.</LI>
</OL>
</P>
<P>Now that the filesystem is registered and inkernel-mounted we can use it.
The entry point into the pipefs filesystem is the <B>pipe(2)</B> system call,
implemented in arch-dependent function <CODE>sys_pipe()</CODE> but the real work is done
by a portable <CODE>fs/pipe.c:do_pipe()</CODE> function. Let us look at <CODE>do_pipe()</CODE> then.
The interaction with pipefs happens when <CODE>do_pipe()</CODE> calls <CODE>get_pipe_inode()</CODE>
to allocate a new pipefs inode. For this inode, <CODE>inode->i_sb</CODE> is set to
pipefs' superblock <CODE>pipe_mnt->mnt_sb</CODE>, the file operations <CODE>i_fop</CODE> is set to
<CODE>rdwr_pipe_fops</CODE> and the number of readers and writers (held in <CODE>inode->i_pipe</CODE>)
is set to 1. The reason why there is a separate inode field <CODE>i_pipe</CODE> instead
of keeping it in the <CODE>fs-private</CODE> union is that pipes and FIFOs share the same
code and FIFOs can exist on other filesystems which use the other access
paths within the same union which is very bad C and can work only by pure
luck. So, yes, 2.2.x kernels work only by pure luck and will stop working
as soon as you slightly rearrange the fields in the inode.</P>
<P>Each <B>pipe(2)</B> system call increments a reference count on the <CODE>pipe_mnt</CODE>
mount instance.</P>
<P>Under Linux, pipes are not symmetric (bidirection or STREAM pipes), i.e.
two sides of the file have different <CODE>file->f_op</CODE> operations - the
<CODE>read_pipe_fops</CODE> and <CODE>write_pipe_fops</CODE> respectively. The write on read side
returns <CODE>EBADF</CODE> and so does read on write side.</P>


<H2><A NAME="ss3.7">3.7 Example Disk Filesystem: BFS</A>
</H2>


<P>As a simple example of ondisk Linux filesystem, let us consider BFS. The
preamble of the BFS module is in <CODE>fs/bfs/inode.c</CODE>:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
static DECLARE_FSTYPE_DEV(bfs_fs_type, "bfs", bfs_read_super);

static int __init init_bfs_fs(void)
{
        return register_filesystem(&amp;bfs_fs_type);
}

static void __exit exit_bfs_fs(void)
{
        unregister_filesystem(&amp;bfs_fs_type);
}

module_init(init_bfs_fs)
module_exit(exit_bfs_fs)
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>A special fstype declaration macro <CODE>DECLARE_FSTYPE_DEV()</CODE> is used which
sets the <CODE>fs_type->flags</CODE> to <CODE>FS_REQUIRES_DEV</CODE> to signify that BFS requires a
real block device to be mounted on.</P>
<P>The module's initialisation function registers the filesystem with VFS and
the cleanup function (only present when BFS is configured to be a module)
unregisters it.</P>
<P>With the filesystem registered, we can proceed to mount it, which would
invoke out <CODE>fs_type->read_super()</CODE> method which is implemented in
<CODE>fs/bfs/inode.c:bfs_read_super().</CODE> It does the following:</P>
<P>
<OL>
<LI><CODE>set_blocksize(s->s_dev, BFS_BSIZE)</CODE>: since we are about to interact
with the block device layer via the buffer cache, we must initialise a few
things, namely set the block size and also inform VFS via fields
<CODE>s->s_blocksize</CODE> and <CODE>s->s_blocksize_bits</CODE>.
</LI>
<LI><CODE>bh = bread(dev, 0, BFS_BSIZE)</CODE>: we read block 0 of the device
passed via <CODE>s->s_dev</CODE>. This block is the filesystem's superblock.
</LI>
<LI>Superblock is validated against <CODE>BFS_MAGIC</CODE> number and, if valid, stored
in the sb-private field <CODE>s->su_sbh</CODE> (which is really <CODE>s->u.bfs_sb.si_sbh</CODE>).
</LI>
<LI>Then we allocate inode bitmap using <CODE>kmalloc(GFP_KERNEL)</CODE> and clear all
bits to 0 except the first two which we set to 1 to indicate that we
should never allocate inodes 0 and 1. Inode 2 is root and the
corresponding bit will be set to 1 a few lines later anyway - the
filesystem should have a valid root inode at mounting time!
</LI>
<LI>Then we initialise <CODE>s->s_op</CODE>, which means that we can from this point
invoke inode cache via <CODE>iget()</CODE> which results in <CODE>s_op->read_inode()</CODE> to
be invoked. This finds the block that contains the specified (by
<CODE>inode->i_ino</CODE> and <CODE>inode->i_dev</CODE>) inode and reads it in. If we fail to
get root inode then we free the inode bitmap and release superblock
buffer back to buffer cache and return NULL. If root inode was read OK,
then we allocate a dentry with name <CODE>/</CODE> (as becometh root) and
instantiate it with this inode.
</LI>
<LI>Now we go through all inodes on the filesystem and read them all in
order to set the corresponding bits in our internal inode bitmap and
also to calculate some other internal parameters like the offset of
last inode and the start/end blocks of last file. Each inode we read
is returned back to inode cache via <CODE>iput()</CODE> - we don't hold a reference
to it longer than needed.
</LI>
<LI>If the filesystem was not mounted read-only, we mark the superblock
buffer dirty and set <CODE>s->s_dirt</CODE> flag (TODO: why do I do this?
Originally, I did it because <CODE>minix_read_super()</CODE> did but neither minix
nor BFS seem to modify superblock in the <CODE>read_super()</CODE>).
</LI>
<LI>All is well so we return this initialised superblock back to the caller
at VFS level, i.e. <CODE>fs/super.c:read_super()</CODE>.</LI>
</OL>
</P>
<P>After the <CODE>read_super()</CODE> function returns successfully, VFS obtains the
reference to the filesystem module via call to <CODE>get_filesystem(fs_type)</CODE> in
<CODE>fs/super.c:get_sb_bdev()</CODE> and a reference to the block device.</P>
<P>Now, let us examine what happens when we do I/O on the filesystem. We already
examined how inodes are read when <CODE>iget()</CODE> is called and how they are released
on <CODE>iput().</CODE> Reading inodes sets up, among other things, <CODE>inode->i_op</CODE> and
<CODE>inode->i_fop</CODE>; opening a file will propagate <CODE>inode->i_fop</CODE> into <CODE>file->f_op</CODE>.</P>
<P>Let us examine the code path of the <B>link(2)</B> system call. The implementation
of the system call is in <CODE>fs/namei.c:sys_link()</CODE>:</P>
<P>
<OL>
<LI>The userspace names are copied into kernel space by means of <CODE>getname()</CODE>
function which does the error checking.
</LI>
<LI>These names are nameidata converted using <CODE>path_init()/path_walk()</CODE>
interaction with dcache. The result is stored in <CODE>old_nd</CODE> and <CODE>nd</CODE>
structures.
</LI>
<LI>If <CODE>old_nd.mnt != nd.mnt</CODE> then "cross-device link" <CODE>EXDEV</CODE> is returned - 
one cannot link between filesystems, in Linux this translates into -
one cannot link between mounted instances of a filesystem (or, in
particular between filesystems).
</LI>
<LI>A new dentry is created corresponding to <CODE>nd</CODE> by <CODE>lookup_create()</CODE> .
</LI>
<LI>A generic <CODE>vfs_link()</CODE> function is called which checks if we can
create a new entry in the directory and invokes the <CODE>dir->i_op->link()</CODE>
method which brings us back to filesystem-specific
<CODE>fs/bfs/dir.c:bfs_link()</CODE> function.
</LI>
<LI>Inside <CODE>bfs_link()</CODE>, we check if we are trying to link a directory and
if so, refuse with <CODE>EPERM</CODE> error. This is the same behaviour as standard (ext2).
</LI>
<LI>We attempt to add a new directory entry to the specified directory
by calling the helper function <CODE>bfs_add_entry()</CODE> which goes through all
entries looking for unused slot (<CODE>de->ino == 0</CODE>) and, when found, writes
out the name/inode pair into the corresponding block and marks it
dirty (at non-superblock priority).
</LI>
<LI>If we successfully added the directory entry then there is no way
to fail the operation so we increment <CODE>inode->i_nlink</CODE>, update
<CODE>inode->i_ctime</CODE> and mark this inode dirty as well as instantiating the
new dentry with the inode.</LI>
</OL>
</P>
<P>Other related inode operations like <CODE>unlink()/rename()</CODE> etc work in a similar
way, so not much is gained by examining them all in details.</P>

<H2><A NAME="ss3.8">3.8 Execution Domains and Binary Formats</A>
</H2>


<P>Linux supports loading user application binaries from disk. More
interestingly, the binaries can be stored in different formats and the
operating system's response to programs via system calls can deviate from
norm (norm being the Linux behaviour) as required, in order to emulate
formats found in other flavours of UNIX (COFF, etc) and also to emulate
system calls behaviour of other flavours (Solaris, UnixWare, etc). This is
what execution domains and binary formats are for.</P>
<P>Each Linux task has a personality stored in its <CODE>task_struct</CODE> (<CODE>p->personality</CODE>).
The currently existing (either in the official kernel or as addon patch)
personalities include support for FreeBSD, Solaris, UnixWare, OpenServer and
many other popular operating systems. 
The value of <CODE>current->personality</CODE> is split into two parts:</P>
<P>
<OL>
<LI>high three bytes - bug emulation: <CODE>STICKY_TIMEOUTS</CODE>, <CODE>WHOLE_SECONDS</CODE>, etc.</LI>
<LI>low byte - personality proper, a unique number.</LI>
</OL>
</P>
<P>By changing the personality, we can change
the way the operating system treats certain system calls, for example
adding a <CODE>STICKY_TIMEOUT</CODE> to <CODE>current->personality</CODE> makes <B>select(2)</B> system call
preserve the value of last argument (timeout) instead of storing the
unslept time. Some buggy programs rely on buggy operating systems (non-Linux)
and so Linux provides a way to emulate bugs in cases where the source code
is not available and so bugs cannot be fixed.</P>
<P>Execution domain is a contiguous range of personalities implemented by a
single module. Usually a single execution domain implements a single
personality but sometimes it is possible to implement "close" personalities
in a single module without too many conditionals.</P>
<P>Execution domains are implemented in <CODE>kernel/exec_domain.c</CODE> and were completely
rewritten for 2.4 kernel, compared with 2.2.x. The list of execution domains
currently supported by the kernel, along with the range of personalities
they support, is available by reading the <CODE>/proc/execdomains</CODE> file. Execution
domains, except the <CODE>PER_LINUX</CODE> one, can be implemented as dynamically
loadable modules.</P>
<P>The user interface is via <B>personality(2)</B> system call, which sets the current
process' personality or returns the value of <CODE>current->personality</CODE> if the
argument is set to impossible personality 0xffffffff. Obviously, the
behaviour of this system call itself does not depend on personality..</P>
<P>The kernel interface to execution domains registration consists of two
functions:</P>
<P>
<UL>
<LI><CODE>int register_exec_domain(struct exec_domain *)</CODE>: registers the
execution domain by linking it into single-linked list <CODE>exec_domains</CODE>
under the write protection of the read-write spinlock <CODE>exec_domains_lock</CODE>.
Returns 0 on success, non-zero on failure.
</LI>
<LI><CODE>int unregister_exec_domain(struct exec_domain *)</CODE>: unregisters the
execution domain by unlinking it from the <CODE>exec_domains</CODE> list, again using
<CODE>exec_domains_lock</CODE> spinlock in write mode. Returns 0 on success.</LI>
<LI></LI>
</UL>
</P>
<P>The reason why <CODE>exec_domains_lock</CODE> is a read-write is that only registration
and unregistration requests modify the list, whilst doing
<B>cat /proc/filesystems</B> calls <CODE>fs/exec_domain.c:get_exec_domain_list()</CODE>, which
needs only read access to the list. Registering a new execution domain
defines a "lcall7 handler" and a signal number conversion map. Actually,
ABI patch extends this concept of exec domain to include extra information
(like socket options, socket types, address family and errno maps).</P>
<P>The binary formats are implemented in a similar manner, i.e. a single-linked
list formats is defined in <CODE>fs/exec.c</CODE> and is protected by a read-write lock
<CODE>binfmt_lock</CODE>. As with <CODE>exec_domains_lock</CODE>, the <CODE>binfmt_lock</CODE> is taken read on
most occasions except for registration/unregistration of binary formats.
Registering a new binary format enhances the <B>execve(2)</B> system call with new
<CODE>load_binary()/load_shlib()</CODE> functions as well as ability to <CODE>core_dump()</CODE> . The
<CODE>load_shlib()</CODE> method is used only by the old <B>uselib(2)</B> system call while
the <CODE>load_binary()</CODE> method is called by the <CODE>search_binary_handler()</CODE> from
<CODE>do_execve()</CODE> which implements <B>execve(2)</B> system call.</P>
<P>The personality of the process is determined at binary format loading by
the corresponding format's <CODE>load_binary()</CODE> method using some heuristics.
For example to determine UnixWare7 binaries one first marks the binary
using the <B>elfmark(1)</B> utility, which sets the ELF header's <CODE>e_flags</CODE> to the magic
value 0x314B4455 which is detected at ELF loading time and
<CODE>current->personality</CODE> is set to PER_UW7. If this heuristic fails, then a more
generic one, such as treat ELF interpreter paths like <CODE>/usr/lib/ld.so.1</CODE> or
<CODE>/usr/lib/libc.so.1</CODE> to
indicate a SVR4 binary, is used and personality is set to PER_SVR4. One
could write a little utility program that uses Linux's <B>ptrace(2)</B> capabilities
to single-step the code and force a running program into any personality.</P>
<P>Once personality (and therefore <CODE>current->exec_domain</CODE>) is known, the system
calls are handled as follows. Let us assume that a process makes a system
call by means of lcall7 gate instruction. This transfers control to
<CODE>ENTRY(lcall7)</CODE> of <CODE>arch/i386/kernel/entry.S</CODE> because it was prepared in
<CODE>arch/i386/kernel/traps.c:trap_init()</CODE>. After appropriate stack layout
conversion, <CODE>entry.S:lcall7</CODE> obtains the pointer to <CODE>exec_domain</CODE> from <CODE>current</CODE>
and then an offset of lcall7 handler within the <CODE>exec_domain</CODE> (which is 
hardcoded as 4 in asm code so you can't shift the <CODE>handler</CODE> field around in
C declaration of <CODE>struct exec_domain</CODE>) and jumps to it. So, in C, it would
look like this:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
static void UW7_lcall7(int segment, struct pt_regs * regs)
{
       abi_dispatch(regs, &amp;uw7_funcs[regs->eax &amp; 0xff], 1);
}
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>where <CODE>abi_dispatch()</CODE> is a wrapper around the table of function pointers that
implement this personality's system calls <CODE>uw7_funcs</CODE>.</P>

<H2><A NAME="s4">4. Linux Page Cache</A></H2>


<P>In this chapter we describe the Linux 2.4 pagecache.  The pagecache
is - as the name suggests - a cache of physical pages. In the UNIX world the
concept of a pagecache became popular with the introduction of SVR4 UNIX,
where it replaced the buffercache for data IO operations.</P>
<P>While the SVR4 pagecache is only used for filesystem data cache and thus uses
the struct vnode and an offset into the file as hash parameters, the Linux page
cache is designed to be more generic, and therefore uses a struct address_space
(explained below) as first parameter.  Because the Linux pagecache is tightly
coupled to the notation of address spaces, you will need at least a basic
understanding of adress_spaces to understand the way the pagecache works.
An address_space is some kind of software MMU that maps all pages of one object
(e.g. inode) to an other concurrency (typically physical disk blocks).
The struct address_space is defined in <CODE>include/linux/fs.h</CODE> as:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
        struct address_space {
                struct list_head        clean_pages;
                struct list_head        dirty_pages;
                struct list_head        locked_pages;
                unsigned long           nrpages;
                struct address_space_operations *a_ops;
                struct inode            *host;
                struct vm_area_struct   *i_mmap;
                struct vm_area_struct   *i_mmap_shared;
                spinlock_t              i_shared_lock;
 
        };
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>To understand the way address_spaces works, we only need to look at a few of this fields:
<CODE>clean_pages</CODE>, <CODE>dirty_pages</CODE> and <CODE>locked_pages</CODE> are double linked lists
of all clean, dirty and locked pages that belong to this address_space, <CODE>nrpages</CODE>
is the total number of pages in this address_space.  <CODE>a_ops</CODE> defines the methods of
this object and <CODE>host</CODE> is an pointer to the inode this address_space belongs to -
it may also be NULL, e.g. in the case of the swapper address_space
(<CODE>mm/swap_state.c,</CODE>).</P>
<P>The usage of <CODE>clean_pages</CODE>, <CODE>dirty_pages</CODE>, <CODE>locked_pages</CODE> and
<CODE>nrpages</CODE> is obvious, so we will take a tighter look at the
<CODE>address_space_operations</CODE> structure, defined in the same header:</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
        struct address_space_operations {
                int (*writepage)(struct page *);
                int (*readpage)(struct file *, struct page *);
                int (*sync_page)(struct page *);
                int (*prepare_write)(struct file *, struct page *, unsigned, unsigned);
                int (*commit_write)(struct file *, struct page *, unsigned, unsigned);
                int (*bmap)(struct address_space *, long);
        };
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<P>For a basic view at the principle of address_spaces (and the pagecache) we need
to take a look at -><CODE>writepage</CODE> and -><CODE>readpage</CODE>, but in practice we need
to take a look at -><CODE>prepare_write</CODE> and -><CODE>commit_write</CODE>, too.</P>
<P>You can probably guess what the address_space_operations methods do
by virtue of their names alone; nevertheless, they do require some
explanation.  Their use in the course of filesystem data I/O, by
far the most common path through the pagecache, provides a good
way of understanding them.
Unlike most other UNIX-like operating systems, Linux has generic file
operations (a subset of the SYSVish vnode operations) for data IO through the
pagecache.  This means that the data will not directly interact with the file-
system on read/write/mmap, but will be read/written from/to the pagecache
whenever possible.  The pagecache has to get data from the actual low-level
filesystem in case the user wants to read from a page not yet in memory,
or write data to disk in case memory gets low.</P>
<P>In the read path the generic methods will first try to find a page that
matches the wanted inode/index tuple.</P>
<P>
<BLOCKQUOTE><CODE>
hash = page_hash(inode->i_mapping, index);
</CODE></BLOCKQUOTE>
</P>
<P>Then we test whether the page actually exists.</P>
<P>
<BLOCKQUOTE><CODE>
hash = page_hash(inode->i_mapping, index);
page = __find_page_nolock(inode->i_mapping, index, *hash);
</CODE></BLOCKQUOTE>
</P>
<P>When it does not exist, we allocate a new free page, and add it to the page-
cache hash.</P>
<P>
<BLOCKQUOTE><CODE>
page = page_cache_alloc();
__add_to_page_cache(page, mapping, index, hash);
</CODE></BLOCKQUOTE>
</P>
<P>After the page is hashed we use the -><CODE>readpage</CODE> address_space operation to
actually fill the page with data. (file is an open instance of inode).</P>
<P>
<BLOCKQUOTE><CODE>
error = mapping->a_ops->readpage(file, page);
</CODE></BLOCKQUOTE>
</P>
<P>Finally we can copy the data to userspace.</P>
<P>For writing to the filesystem two pathes exist: one for writable mappings
(mmap) and one for the write(2) family of syscalls. The mmap case is very
simple, so it will be discussed first.
When a user modifies mappings, the VM subsystem marks the page dirty.</P>
<P>
<BLOCKQUOTE><CODE>
SetPageDirty(page);
</CODE></BLOCKQUOTE>
</P>
<P>The bdflush kernel thread that is trying to free pages, either as background
activity or because memory gets low will try to call -><CODE>writepage</CODE> on the pages
that are explicitly marked dirty.  The -><CODE>writepage</CODE> method does now have to
write the pages content back to disk and free the page.</P>
<P>The second write path is _much_ more complicated.  For each page the user
writes to, we are basically doing the following:
(for the full code see <CODE>mm/filemap.c:generic_file_write()</CODE>).</P>
<P>
<BLOCKQUOTE><CODE>
page = __grab_cache_page(mapping, index, &amp;cached_page);
mapping->a_ops->prepare_write(file, page, offset, offset+bytes);
copy_from_user(kaddr+offset, buf, bytes);
mapping->a_ops->commit_write(file, page, offset, offset+bytes);
</CODE></BLOCKQUOTE>
</P>
<P>So first we try to find the hashed page or allocate a new one, then we call the
-><CODE>prepare_write</CODE> address_space method, copy the user buffer to kernel memory and
finally call the -><CODE>commit_write</CODE> method.  As you probably have seen
->prepare_write and -><CODE>commit_write</CODE> are fundamentally different from -><CODE>readpage</CODE>
and -><CODE>writepage</CODE>, because they are not only called when physical IO is actually
wanted but everytime the user modifies the file.
There are two (or more?) ways to handle this, the first one uses the Linux
buffercache to delay the physical IO, by filling a <CODE>page->buffers</CODE> pointer with
buffer_heads, that will be used in try_to_free_buffers (<CODE>fs/buffers.c</CODE>) to
request IO once memory gets low, and is used very widespread in the current
kernel.  The other way just sets the page dirty and relies on -><CODE>writepage</CODE> to do
all the work.  Due to the lack of a validitity bitmap in struct page this does
not work with filesystem that have a smaller granuality then <CODE>PAGE_SIZE</CODE>.</P>

<H2><A NAME="s5">5. IPC mechanisms</A></H2>

<P>This chapter describes the semaphore, shared memory, and
message queue IPC mechanisms as implemented in the Linux 2.4
kernel. It is organized into four sections. The
first three sections cover the interfaces and support functions
for 
<A HREF="#semaphores">semaphores</A>,
<A HREF="#message">message queues</A>,
and 
<A HREF="#sharedmem">shared memory</A> respectively.
The 
<A HREF="#ipc_primitives">last</A> section describes
a set of common functions and data structures that are shared by
all three mechanisms.</P>

<H2><A NAME="semaphores"></A> <A NAME="ss5.1">5.1 Semaphores</A>
</H2>

<P>The functions described in this section implement the user level
semaphore mechanisms. Note that this implementation relies on the
use of kernel splinlocks and kernel semaphores. To avoid confusion,
the term "kernel semaphore" will be used in reference to kernel
semaphores. All other uses of the word "sempahore" will be in
reference to the user level semaphores.</P>

<H3><A NAME="sem_apis"></A> Semaphore System Call Interfaces</H3>



<H3><A NAME="sys_semget"></A> sys_semget()</H3>

<P>The entire call to sys_semget() is protected by the
global 
<A HREF="#struct_ipc_ids">sem_ids.sem</A>
kernel semaphore.</P>
<P>In the case where a new set of semaphores must be
created, the 
<A HREF="#newary">newary()</A> function is
called to create and initialize a new semaphore set. The ID of
the new set is returned to the caller.</P>
<P>In the case where a key value is provided for an existing
semaphore set, 
<A HREF="#ipc_findkey">ipc_findkey()</A>
is invoked to look up the corresponding semaphore descriptor
array index.  The parameters and permissions of the caller are
verified before returning the semaphore set ID.</P>
<H3><A NAME="sys_semctl"></A> sys_semctl()</H3>

<P>For the 
<A HREF="#IPC_INFO_and_SEM_INFO">IPC_INFO</A>,
<A HREF="#IPC_INFO_and_SEM_INFO">SEM_INFO</A>, and
<A HREF="#SEM_STAT">SEM_STAT</A> commands,
<A HREF="#semctl_nolock">semctl_nolock()</A>
is called to perform the necessary functions.</P>
<P>For the 
<A HREF="#GETALL">GETALL</A>, 
<A HREF="#GETVAL">GETVAL</A>,
<A HREF="#GETPID">GETPID</A>, 
<A HREF="#GETNCNT">GETNCNT</A>,
<A HREF="#GETZCNT">GETZCNT</A>, 
<A HREF="#IPC_STAT">IPC_STAT</A>,
<A HREF="#SETVAL">SETVAL</A>,and 
<A HREF="#SETALL">SETALL</A> commands,
<A HREF="#semctl_main">semctl_main()</A> is called to perform the
necessary functions.</P>
<P>For the 
<A HREF="#semctl_ipc_rmid">IPC_RMID</A>
and 
<A HREF="#semctl_ipc_set">IPC_SET</A> command,
<A HREF="#semctl_down">semctl_down()</A> is called
to perform the necessary functions. Throughout both of these
operations, the global 
<A HREF="#struct_ipc_ids">sem_ids.sem</A>
kernel semaphore is held.</P>
<H3><A NAME="sys_semop"></A> sys_semop()</H3>

<P>After validating the call parameters, the semaphore
operations data is copied from user space to a temporary buffer.
If a small temporary buffer is sufficient, then a stack buffer is
used. Otherwise, a larger buffer is allocated. After copying in the
semaphore operations data, the global semaphores spinlock is
locked, and the user-specified semaphore set ID is validated.
Access permissions for the semaphore set are also validated.</P>
<P>All of the user-specified semaphore operations are parsed.
During this process, a count is maintained of all the operations that
have the SEM_UNDO flag set. A <CODE>decrease</CODE> flag is set if any of the
operations subtract from a semaphore value, and an <CODE>alter</CODE> flag is set
if any of the semaphore values are modified (i.e. increased or
decreased). The number of each
semaphore to be modified is validated.</P>
<P>If SEM_UNDO was asserted for any of the semaphore operations,
then the undo list for the current task is searched for an undo
structure associated with this semaphore set. During this search,
if the semaphore set ID of any of the undo structures is found
to be -1, then 
<A HREF="#freeundos">freeundos()</A>
is called to free the undo structure
and remove it from the list. If no undo structure is found for
this semaphore set then 
<A HREF="#alloc_undo">alloc_undo()</A>
is called to allocate and initialize one.</P>
<P>The 
<A HREF="#try_atomic_semop">try_atomic_semop()</A>
function is called with the <CODE>do_undo</CODE>
parameter equal to 0 in order to execute the sequence of
operations. The return value indicates that either the
operations passed, failed, or were not executed because
they need to block. Each of these cases are further described below:</P>

<H3><A NAME="Non-blocking_Semaphore_Operations"></A> Non-blocking Semaphore Operations</H3>

<P>The 
<A HREF="#try_atomic_semop">try_atomic_semop()</A>
function returns zero to indicate that all operations in the
sequence succeeded. In this case,
<A HREF="#update_queue">update_queue()</A>
is called to traverse the queue of pending semaphore
operations for the semaphore set and awaken any
sleeping tasks that no longer need to block. This completes the
execution of the sys_semop() system call for this case.</P>
<H3><A NAME="Failing_Semaphore_Operations"></A> Failing Semaphore Operations</H3>

<P>If 
<A HREF="#try_atomic_semop">try_atomic_semop()</A>
returns a negative value, then a failure condition was encountered.
In this case, none of the operations have been executed.
This occurs when either a semaphore operation would cause an
invalid semaphore value, or an operation marked IPC_NOWAIT is
unable to complete.  The error condition is then returned to the
caller of sys_semop().</P>
<P>Before sys_semop() returns, a call is made to
<A HREF="#update_queue">update_queue()</A> to traverse
the queue of pending semaphore operations for the semaphore set
and awaken any sleeping tasks that no longer need to block.</P>
<H3><A NAME="Blocking_Semaphore_Operations"></A> Blocking Semaphore Operations</H3>

<P>The 
<A HREF="#try_atomic_semop">try_atomic_semop()</A>
function returns 1 to indicate that the
sequence of semaphore operations was not executed because
one of the semaphores would block. For this case, a new
<A HREF="#struct_sem_queue">sem_queue</A> element is
initialized containing these semaphore operations. If any of
these operations would alter the state of the semaphore, then
the new queue element is added at the tail of the queue.
Otherwise, the new queue element is added at the head of the queue.</P>
<P>The <CODE>semsleeping</CODE> element of the current
task is set to indicate that the task is sleeping on this
<A HREF="#struct_sem_queue">sem_queue</A> element.
The current task is marked as TASK_INTERRUPTIBLE, and the
<CODE>sleeper</CODE> element of the
<A HREF="#struct_sem_queue">sem_queue</A>
is set to identify this task as the sleeper. The
global semaphore spinlock is then unlocked, and schedule() is called
to put the current task to sleep.</P>
<P>When awakened, the task re-locks the global semaphore spinlock,
determines why it was awakened, and how it should
respond.  The following cases are handled:</P>
<P>
<UL>
<LI>  If the semaphore set has been removed, then
the system call fails with EIDRM.
</LI>
<LI>  If the <CODE>status</CODE> element of the
<A HREF="#struct_sem_queue">sem_queue</A> structure
is set to 1, then the task was awakened in order to retry the
semaphore operations. Another call to
<A HREF="#try_atomic_semop">try_atomic_semop()</A> is
made to execute the sequence of semaphore operations.  If
try_atomic_sweep() returns 1, then the task must block again
as described above. Otherwise, 0 is returned for success,
or an appropriate error code is returned in case of failure.

Before sys_semop() returns, current->semsleeping is cleared,
and the 
<A HREF="#struct_sem_queue">sem_queue</A>
is removed from the queue.  If any of the specified semaphore
operations were altering operations (increase or decrease),
then 
<A HREF="#update_queue">update_queue()</A> is
called to traverse the queue of pending semaphore operations
for the semaphore set and awaken any sleeping tasks that no
longer need to block.
</LI>
<LI>  If the <CODE>status</CODE> element of the
<A HREF="#struct_sem_queue">sem_queue</A> structure is
NOT set to 1, and the
<A HREF="#struct_sem_queue">sem_queue</A> element has
not been dequeued, then the task was awakened by an interrupt.
In this case, the system call fails with EINTR.  Before
returning, current->semsleeping is cleared, and the
<A HREF="#struct_sem_queue">sem_queue</A> is removed
from the queue.  Also,
<A HREF="#update_queue">update_queue()</A> is called
if any of the operations were altering operations.
</LI>
<LI>  If the <CODE>status</CODE> element of the
<A HREF="#struct_sem_queue">sem_queue</A> structure is
NOT set to 1, and the
<A HREF="#struct_sem_queue">sem_queue</A> element
has been dequeued,
then the semaphore operations have already been executed by
<A HREF="#update_queue">update_queue()</A>.  The
queue <CODE>status</CODE>, which could be 0 for success
or a negated error code for failure, becomes the return value of
the system call.
</LI>
</UL>
</P>
<H3><A NAME="sem_structures"></A> Semaphore Specific Support Structures</H3>

<P>The following structures are used specifically for semaphore support:</P>

<H3><A NAME="struct_sem_array"></A> struct sem_array</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/* One sem_array data structure for each set of semaphores in the system. */
struct sem_array {
    struct kern_ipc_perm sem_perm; /* permissions .. see ipc.h */
    time_t sem_otime; /* last semop time */
    time_t sem_ctime; /* last change time */
    struct sem *sem_base; /* ptr to first semaphore in array */
    struct sem_queue *sem_pending; /* pending operations to be processed */
    struct sem_queue **sem_pending_last; /* last pending operation */
    struct sem_undo *undo; /* undo requests on this array * /
    unsigned long sem_nsems; /* no. of semaphores in array */
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_sem"></A> struct sem</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/* One semaphore structure for each semaphore in the system. */
struct sem {
        int     semval;         /* current value */
        int     sempid;         /* pid of last operation */
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_seminfo"></A> struct seminfo</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct  seminfo {
        int semmap;
        int semmni;
        int semmns;
        int semmnu;
        int semmsl;
        int semopm;
        int semume;
        int semusz;
        int semvmx;
        int semaem;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_semid64_ds"></A> struct semid64_ds</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct semid64_ds {
        struct ipc64_perm sem_perm;             /* permissions .. see
ipc.h */
        __kernel_time_t sem_otime;              /* last semop time */
        unsigned long   __unused1;
        __kernel_time_t sem_ctime;              /* last change time */
        unsigned long   __unused2;
        unsigned long   sem_nsems;              /* no. of semaphores in
array */
        unsigned long   __unused3;
        unsigned long   __unused4;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_sem_queue"></A> struct sem_queue</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/* One queue for each sleeping process in the system. */
struct sem_queue {
        struct sem_queue *      next;    /* next entry in the queue */
        struct sem_queue **     prev;    /* previous entry in the queue, *(q->pr
ev) == q */
        struct task_struct*     sleeper; /* this process */
        struct sem_undo *       undo;    /* undo structure */
        int                     pid;     /* process id of requesting process */
        int                     status;  /* completion status of operation */
        struct sem_array *      sma;     /* semaphore array for operations */
        int                     id;      /* internal sem id */
        struct sembuf *         sops;    /* array of pending operations */
        int                     nsops;   /* number of operations */
        int                     alter;   /* operation will alter semaphore */
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_sembuf"></A> struct sembuf</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/* semop system calls takes an array of these. */
struct sembuf {
        unsigned short  sem_num;        /* semaphore index in array */
        short           sem_op;         /* semaphore operation */
        short           sem_flg;        /* operation flags */
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_sem_undo"></A> struct sem_undo</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/* Each task has a list of undo requests. They are executed automatically
 * when the process exits.
 */
struct sem_undo {
        struct sem_undo *       proc_next;      /* next entry on this process */
        struct sem_undo *       id_next;        /* next entry on this semaphore set */
        int                     semid;          /* semaphore set identifier */
        short *                 semadj;         /* array of adjustments, one per
 semaphore */
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="sem_primitives"></A> Semaphore Support Functions</H3>

<P>The following functions are used specifically in support of
semaphores:</P>

<H3><A NAME="newary"></A> newary()</H3>

<P>newary() relies on the 
<A HREF="#ipc_alloc">ipc_alloc()</A>
function to allocate the memory
required for the new semaphore set. It allocates enough memory
for the semaphore set descriptor and for each of the semaphores
in the set.  The allocated memory is cleared, and the address of the
first element of the semaphore set descriptor is passed to
<A HREF="#ipc_addid">ipc_addid()</A>.
<A HREF="#ipc_addid">ipc_addid()</A> reserves an array entry
for the new semaphore set descriptor and initializes the
(
<A HREF="#struct_kern_ipc_perm">struct kern_ipc_perm</A>) data for the set.
The global <CODE>used_sems</CODE> variable is updated by the number of
semaphores in the new set and the initialization of the
(
<A HREF="#struct_kern_ipc_perm">struct kern_ipc_perm</A>)
data for the new set is completed. Other
initialization for this set performed are listed below:</P>
<P>
<UL>
<LI>  The <CODE>sem_base</CODE> element for the set is initialized
to the address immediately following the
(
<A HREF="#struct_sem_array">struct sem_array</A>)
portion of the newly allocated data. This corresponds to
the location of the first semaphore in the set.
</LI>
<LI>  The <CODE>sem_pending</CODE> queue is initialized as empty.</LI>
</UL>
</P>
<P>All of the operations following the call to 
<A HREF="#ipc_addid">ipc_addid()</A>
are performed while holding the global semaphores spinlock. After
unlocking the global semaphores spinlock, newary() calls
<A HREF="#ipc_buildid">ipc_buildid()</A>
(via sem_buildid()). This function uses the index
of the semaphore set descriptor to create a unique ID, that is then
returned to the caller of newary().</P>

<H3><A NAME="freeary"></A> freeary()</H3>

<P>freeary() is called by 
<A HREF="#semctl_down">semctl_down()</A> to perform the
functions listed below. It is called with the global semaphores
spinlock locked and it returns with the spinlock unlocked</P>
<P>
<UL>
<LI>  The 
<A HREF="#func_ipc_rmid">ipc_rmid()</A> function
is called (via the
sem_rmid() wrapper) to delete the ID for the semaphore
set and to retrieve a pointer to the semaphore set.
</LI>
<LI>  The undo list for the semaphore set is invalidated.</LI>
<LI>  All pending processes are awakened and caused to fail
with EIDRM.
</LI>
<LI>  The number of used semaphores is reduced by the number
of semaphores in the removed set.
</LI>
<LI>  The memory associated with the semaphore set is freed.</LI>
</UL>
</P>
<H3><A NAME="semctl_down"></A> semctl_down()</H3>

<P>semctl_down() provides the 
<A HREF="#semctl_ipc_rmid">IPC_RMID</A> and
<A HREF="#semctl_ipc_set">IPC_SET</A> operations of the
semctl() system call.  The semaphore set ID and the access permissions
are verified prior to either of these operations, and in either
case, the global semaphore spinlock is held throughout the
operation.</P>

<H3><A NAME="semctl_ipc_rmid"></A> IPC_RMID</H3>

<P>The IPC_RMID operation calls 
<A HREF="#freeary">freeary()</A> to remove the semaphore set.</P>
<H3><A NAME="semctl_ipc_set"></A> IPC_SET</H3>

<P>The IPC_SET operation updates the <CODE>uid</CODE>, <CODE>gid</CODE>,
<CODE>mode</CODE>, and <CODE>ctime</CODE> elements of the semaphore set.</P>
<H3><A NAME="semctl_nolock"></A> semctl_nolock()</H3>

<P>semctl_nolock() is called by 
<A HREF="#sys_semctl">sys_semctl()</A>
to perform the IPC_INFO, SEM_INFO and SEM_STAT functions.</P>

<H3><A NAME="IPC_INFO_and_SEM_INFO"></A> IPC_INFO and SEM_INFO</H3>

<P>IPC_INFO and SEM_INFO cause a temporary 
<A HREF="#struct_seminfo">seminfo</A>
buffer to be initialized and loaded with unchanging semaphore
statistical data. Then, while holding the global <CODE>sem_ids.sem</CODE>
kernel semaphore, the <CODE>semusz</CODE> and <CODE>semaem</CODE> elements of
the 
<A HREF="#struct_seminfo">seminfo</A> structure are
updated according to the given command (IPC_INFO or SEM_INFO).
The return value of the system call is set to the maximum
semaphore set ID.</P>
<H3><A NAME="SEM_STAT"></A> SEM_STAT</H3>

<P>SEM_STAT causes a temporary 
<A HREF="#struct_semid64_ds">semid64_ds</A>
buffer to be initialized.  The global
semaphore spinlock is then held while copying the <CODE>sem_otime</CODE>,
<CODE>sem_ctime</CODE>, and <CODE>sem_nsems</CODE> values into the buffer. This data is
then copied to user space.</P>
<H3><A NAME="semctl_main"></A> semctl_main()</H3>

<P>semctl_main() is called by 
<A HREF="#sys_semctl">sys_semctl()</A> to perform many
of the supported functions, as described in the subsections below.
Prior to performing any of the following operations, semctl_main()
locks the global semaphore spinlock and validates the
semaphore set ID and the permissions. The spinlock is released
before returning.</P>

<H3><A NAME="GETALL"></A> GETALL</H3>

<P>The GETALL operation loads the current semaphore values into
a temporary kernel buffer and copies
them out to user space. The small stack buffer is used if the
semaphore set is small. Otherwise, the spinlock is temporarily
dropped in order to allocate a larger buffer. The spinlock is
held while copying the semaphore values in to the temporary buffer.</P>
<H3><A NAME="SETALL"></A> SETALL</H3>

<P>The SETALL operation copies semaphore values from user space into a temporary buffer,
and then into the semaphore set. The spinlock is dropped while
copying the values from user space into the temporary buffer,
and while verifying reasonable values. If the semaphore set
is small, then a stack buffer is used, otherwise a larger buffer
is allocated. The spinlock is regained and held while the
following operations are performed on the semaphore set:</P>
<P>
<UL>
<LI>  The semaphore values are copied into the semaphore set.</LI>
<LI>  The semaphore adjustments of the undo queue for
the semaphore set are cleared.
</LI>
<LI>  The <CODE>sem_ctime</CODE> value for the semaphore set is set.
</LI>
<LI>  The 
<A HREF="#update_queue">update_queue()</A>
function is called to traverse
the queue of pending semops and look for any tasks that
can be completed as a result of the SETALL operation. Any
pending tasks that are no longer blocked are awakened.</LI>
</UL>
</P>
<H3><A NAME="IPC_STAT"></A> IPC_STAT</H3>

<P>In the IPC_STAT operation, the <CODE>sem_otime</CODE>,
<CODE>sem_ctime</CODE>, and <CODE>sem_nsems</CODE> value are copied into
a stack buffer. The data is then copied to user space after
dropping the spinlock.</P>
<H3><A NAME="GETVAL"></A> GETVAL</H3>

<P>For GETVAL in the non-error case, the return value for the system call is
set to the value of the specified semaphore.</P>
<H3><A NAME="GETPID"></A> GETPID</H3>

<P>For GETPID in the non-error case, the return value for the system call is
set to the <CODE>pid</CODE> associated with the last operation on the
semaphore.</P>
<H3><A NAME="GETNCNT"></A> GETNCNT</H3>

<P>For GETNCNT in the non-error case, the return value for the system call
is set to the number of processes waiting on the semaphore
being less than zero. This number is calculated by the
<A HREF="#count_semncnt">count_semncnt()</A> function.</P>
<H3><A NAME="GETZCNT"></A> GETZCNT</H3>

<P>For GETZCNT in the non-error case, the return value for the system call
is set to the number of processes waiting on the semaphore
being set to zero. This number is calculated by the
<A HREF="#count_semzcnt">count_semzcnt()</A> function.</P>
<H3><A NAME="SETVAL"></A> SETVAL</H3>

<P>After validating the new semaphore value, the following
functions are performed:</P>
<P>
<UL>
<LI>  The undo queue is searched for any adjustments to
this semaphore. Any adjustments that are found are reset to
zero.
</LI>
<LI>  The semaphore value is set to the value provided.</LI>
<LI>  The <CODE>sem_ctime</CODE> value for the semaphore set is updated.</LI>
<LI>  The 
<A HREF="#update_queue">update_queue()</A>
function is called to traverse
the queue of pending semops and look for any tasks that
can be completed as a result of the
<A HREF="#SETALL">SETALL</A> operation. Any
pending tasks that are no longer blocked are awakened.</LI>
</UL>
</P>
<H3><A NAME="count_semncnt"></A> count_semncnt()</H3>

<P>count_semncnt() counts the number of tasks waiting on the value of a semaphore
to be less than zero.</P>
<H3><A NAME="count_semzcnt"></A> count_semzcnt()</H3>

<P>count_semzcnt() counts the number of tasks waiting on the value of a semaphore
to be zero.</P>
<H3><A NAME="update_queue"></A> update_queue()</H3>

<P>update_queue() traverses the queue of pending semops for
a semaphore set and calls
<A HREF="#try_atomic_semop">try_atomic_semop()</A>
to determine which sequences of semaphore operations
would succeed. If the status of the queue element
indicates that blocked tasks have already
been awakened, then the queue element is skipped over. For other
elements of the queue, the <CODE>q-alter</CODE> flag
is passed as the undo parameter to
<A HREF="#try_atomic_semop">try_atomic_semop()</A>,
indicating that any
altering operations should be undone before returning.</P>
<P>If the sequence of operations would block, then
update_queue() returns without making any changes.</P>
<P>A sequence of operations can fail if one of the semaphore
operations would cause an invalid semaphore value, or an
operation marked IPC_NOWAIT is unable to complete. In such a
case, the task that is blocked on the sequence of semaphore
operations is awakened, and the queue status is set with an
appropriate error code. The queue element is also dequeued.</P>
<P>If the sequence of operations is non-altering, then
they would have passed a zero value as the undo parameter to
<A HREF="#try_atomic_semop">try_atomic_semop()</A>.
If these operations succeeded, then they
are considered complete and are removed from the queue.
The blocked task is awakened, and the queue element
<CODE>status</CODE> is set to indicate success.</P>
<P>If the sequence of operations would alter the semaphore
values, but can succeed, then sleeping tasks that no longer
need to be blocked are awakened. The queue status is set to
1 to indicate that the blocked task has been awakened. The
operations have not been performed, so the queue element is not
removed from the queue. The semaphore operations would be
executed by the awakened task.</P>
<H3><A NAME="try_atomic_semop"></A> try_atomic_semop()</H3>

<P>try_atomic_semop() is called by 
<A HREF="#sys_semop">sys_semop()</A>
and 
<A HREF="#update_queue">update_queue()</A>
to determine if a sequence of semaphore operations will all
succeed. It determines this by attempting to perform each of the
operations.</P>
<P>If a blocking operation is encountered, then the process
is aborted and all operations are reversed. -EAGAIN is returned
if IPC_NOWAIT is set. Otherwise 1 is returned to indicate that
the sequence of semaphore operations is blocked.</P>
<P>If a semaphore value is adjusted beyond system limits, then
then all operations are reversed, and -ERANGE is returned.</P>
<P>If all operations in the sequence succeed, and the <CODE>do_undo</CODE>
parameter is non-zero, then all operations are reversed, and 0
is returned. If the <CODE>do_undo</CODE> parameter is zero, then all operations
succeeded and remain in force, and the <CODE>sem_otime</CODE>, field of the
semaphore set is updated.</P>
<H3><A NAME="sem_revalidate"></A> sem_revalidate()</H3>

<P>sem_revalidate() is called when the global semaphores spinlock
has been temporarily dropped and needs to be locked again. It is
called by 
<A HREF="#semctl_main">semctl_main()</A>
and 
<A HREF="#alloc_undo">alloc_undo()</A>.  It validates the
semaphore ID and permissions and on success, returns with the
global semaphores spinlock locked.</P>
<H3><A NAME="freeundos"></A> freeundos()</H3>

<P>freeundos() traverses the process undo list in search of
the desired undo structure. If found, the undo structure is removed from the
list and freed. A pointer to the next undo structure on the
process list is returned.</P>
<H3><A NAME="alloc_undo"></A> alloc_undo()</H3>

<P>alloc_undo() expects to be called with the global semaphores
spinlock locked. In the case of an error, it returns with it
unlocked.</P>
<P>The global semaphores spinlock is unlocked, and kmalloc() is
called to allocate sufficient memory for both the
<A HREF="#struct_sem_undo">sem_undo</A>
structure, and also an array of one adjustment value for each
semaphore in the set. On success, the global spinlock is regained
with a call to 
<A HREF="#sem_revalidate">sem_revalidate()</A>.</P>
<P>The new semundo structure is then initialized, and the address
of this structure is placed at the address provided by the
caller. The new undo structure is then placed at the head of undo
list for the current task.</P>
<H3><A NAME="sem_exit"></A> sem_exit()</H3>

<P>sem_exit() is called by do_exit(), and is responsible for
executing all of the undo adjustments for the exiting task.</P>
<P>If the current process was blocked on a semaphore, then it is
removed from the 
<A HREF="#struct_sem_queue">sem_queue</A>
list while holding the global semaphores spinlock.</P>
<P>The undo list for the current task is then traversed, and the
following operations are performed while holding and releasing the
the global semaphores spinlock around the processing of each
element of the list. The following operations are performed for
each of the undo elements:</P>
<P>
<UL>
<LI>  The undo structure and the semaphore set ID are validated.</LI>
<LI>  The undo list of the corresponding semaphore set is
searched to find a reference to the same undo structure and to
remove it from that list.</LI>
<LI>  The adjustments indicated in the undo structure are
applied to the semaphore set.</LI>
<LI>  The <CODE>sem_otime</CODE> parameter of the semaphore set is updated.</LI>
<LI>  
<A HREF="#update_queue">update_queue()</A> is called
to traverse the queue of
pending semops and awaken any sleeping tasks that no longer
need to be blocked as a result of executing the undo
operations.</LI>
<LI>  The undo structure is freed.</LI>
</UL>
</P>
<P>When the processing of the list is complete, the
current->semundo value is cleared.</P>
<H2><A NAME="message"></A> <A NAME="ss5.2">5.2 Message queues</A>
</H2>


<H3><A NAME="Message_System_Call_Interfaces"></A> Message System Call Interfaces</H3>


<H3><A NAME="sys_msgget"></A> sys_msgget()</H3>

<P>The entire call to sys_msgget() is protected by
the global message queue semaphore
(
<A HREF="#struct_ipc_ids">msg_ids.sem</A>).</P>
<P>In the case where a new message queue must be created,
the 
<A HREF="#newque">newque()</A> function is
called to create and initialize
a new message queue, and the new queue ID is returned to
the caller.</P>
<P>If a key value is provided for an existing message queue,
then 
<A HREF="#ipc_findkey">ipc_findkey()</A> is called
to look up the corresponding index in the global message queue
descriptor array (msg_ids.entries). The
parameters and permissions of the caller are verified before
returning the message queue ID. The look up operation and
verification are performed while the global message queue
spinlock(msg_ids.ary) is held.</P>
<H3><A NAME="sys_msgctl"></A> sys_msgctl()</H3>

<P>The parameters passed to sys_msgctl() are: a message
queue ID (<CODE>msqid</CODE>), the operation
(<CODE>cmd</CODE>), and a pointer to a user space buffer of type
<A HREF="#struct_msqid_ds">msgid_ds</A>
(<CODE>buf</CODE>).  Six operations are
provided in this function: IPC_INFO, MSG_INFO,IPC_STAT,
MSG_STAT, IPC_SET and IPC_RMID.  The message queue
ID and the operation parameters are validated; then, the operation(cmd)
is performed as follows:</P>

<H3><A NAME="msgctl_IPCINFO"></A> IPC_INFO ( or MSG_INFO)</H3>

<P>The global message queue information is copied to user space.</P>
<H3><A NAME="msgctl_IPCSTAT"></A> IPC_STAT ( or MSG_STAT)</H3>

<P>A temporary buffer of type 
<A HREF="#struct_msqid64_ds">struct msqid64_ds</A>
is initialized and the global message queue spinlock is locked.
After verifying the access permissions of the calling process,
the message queue information associated with the message
queue ID is loaded into the temporary buffer, the global
message queue spinlock is unlocked, and the contents of
the temporary buffer are copied out to user space by
<A HREF="#copy_msqid_to_user">copy_msqid_to_user()</A>.</P>
<H3><A NAME="msgctl_IPCSET"></A> IPC_SET</H3>

<P>The user data is copied in via
<A HREF="#copy_msqid_to_user">copy_msqid_to_user()</A>.  The global
message queue semaphore and spinlock are obtained and released
at the end.  After the message queue ID and the current
process access permissions are validated, the message queue
information is updated with the user provided data.  Later,
<A HREF="#expunge_all">expunge_all()</A> and
<A HREF="#ss_wakeup">ss_wakeup()</A>
are called to wake up all
processes sleeping on the receiver and sender waiting queues
of the message queue. This is because some receivers may now
be excluded by stricter access permissions and some senders
may now be able to send the message due to an increased
queue size.</P>
<H3><A NAME="msgctl_IPCRMID"></A> IPC_RMID</H3>

<P>The global message queue semaphore
is obtained and the global message queue spinlock is locked.
After validating the message queue ID and the current task
access permissions, 
<A HREF="#freeque">freeque()</A>
is called to free the resources related to the message queue ID.
The global message queue semaphore and spinlock are released.</P>
<H3><A NAME="sys_msgsnd"></A> sys_msgsnd()</H3>

<P>sys_msgsnd() receives as parameters a message queue ID
(<CODE>msqid</CODE>), a pointer to a buffer of type
<A HREF="#struct_msg_msg">struct msg_msg</A>
(<CODE>msgp</CODE>), the size of the message to be sent
(<CODE>msgsz</CODE>), and a flag indicating wait vs.
not wait (<CODE>msgflg</CODE>). There are two task waiting
queues and one message waiting queue associated with the message
queue ID. If there is a task in the receiver waiting queue
that is waiting for this message, then the message is
delivered directly to the receiver, and the receiver is
awakened. Otherwise, if there is enough space available in
the message waiting queue, the message is saved in this
queue. As a last resort, the sending task enqueues itself
on the sender waiting queue. A more in-depth discussion of the
operations performed by sys_msgsnd() follows:</P>
<P>
<OL>
<LI>  Validates the user buffer address and the message
type, then invokes
<A HREF="#load_msg">load_msg()</A> to load the
contents of the user message into a temporary object
<CODE>
<A NAME="msg"></A> msg</CODE> of type
<A HREF="#struct_msg_msg">struct msg_msg</A>.
The message type and message size fields
of <CODE>msg</CODE> are also initialized.</LI>
<LI>  Locks the global message queue spinlock and gets
the message queue descriptor associated with the
message queue ID. If no such message queue exists,
returns EINVAL.</LI>
<LI>
<A NAME="sndretry"></A> 
Invokes 
<A HREF="#ipc_checkid">ipc_checkid()</A>
(via msg_checkid())to verify that the message
queue ID is valid and calls
<A HREF="#ipcperms">ipcperms()</A> to check the
calling process' access permissions.</LI>
<LI>  Checks the message size and the space left in
the message waiting queue to see if there is enough
room to store the message. If not, the following
substeps are performed:

<OL>
<LI>  If IPC_NOWAIT is specified in
<CODE>msgflg</CODE> the global message
queue spinlock is unlocked, the memory
resources for the message are freed, and EAGAIN
is returned.</LI>
<LI>  Invokes
<A HREF="#ss_add">ss_add()</A> to
enqueue the current
task in the sender waiting queue. It also unlocks
the global message queue spinlock and invokes
schedule() to put the current task to sleep.</LI>
<LI>  When awakened, obtains the global spinlock
again and verifies that the message queue ID
is still valid. If the message queue ID is not valid,
ERMID is returned.</LI>
<LI>  Invokes 
<A HREF="#ss_del">ss_del()</A>
to remove the sending task from the sender
waiting queue. If there is any signal pending
for the task, sys_msgsnd() unlocks the
global spinlock,
invokes 
<A HREF="#free_msg">free_msg()</A>
to free the message buffer,
and returns EINTR. Otherwise, the function goes
<A HREF="#sndretry">back</A>
to check again whether there is enough space
in the message waiting queue.</LI>
</OL>
</LI>
<LI>  Invokes
<A HREF="#pipelined_send">pipelined_send()</A>
to try to send the message to the waiting receiver directly.</LI>
<LI>  If there is no receiver waiting for this message,
enqueues <CODE>msg</CODE> into the message waiting
queue(msq->q_messages). Updates the
<CODE>q_cbytes</CODE> and
the <CODE>q_qnum</CODE> fields of the message
queue descriptor, as well as the global variables
<CODE>msg_bytes</CODE> and
<CODE>msg_hdrs</CODE>, which indicate the total
number of bytes used for messages and the total number
of messages system wide.</LI>
<LI>  If the message has been successfully sent or
enqueued, updates the <CODE>q_lspid</CODE>
and the <CODE>q_stime</CODE> fields
of the message queue descriptor and releases the global
message queue spinlock.</LI>
</OL>
</P>
<H3><A NAME="sys_msgrcv"></A> sys_msgrcv()</H3>

<P>The sys_msgrcv() function receives as parameters
a message queue ID
(<CODE>msqid</CODE>), a pointer to a buffer of type
<A HREF="#struct_msg_msg">msg_msg</A>
(<CODE>msgp</CODE>), the desired
message size(<CODE>msgsz</CODE>), the message type
(<CODE>msgtyp</CODE>), and the flags
(<CODE>msgflg</CODE>). It searches the message waiting queue
associated with the message queue ID, finds the first
message in the queue which matches the request type, and
copies it into the given user buffer. If no such message
is found in the message waiting queue, the requesting task
is enqueued into the receiver waiting queue until the
desired message is available. A more in-depth discussion of the
operations performed by sys_msgrcv() follows:</P>
<P>
<OL>
<LI>  First, invokes
<A HREF="#convert_mode">convert_mode()</A>
to derive the search mode from
<CODE>msgtyp</CODE>.  sys_msgrcv() then locks
the global message
queue spinlock and obtains the message queue descriptor
associated with the message queue ID. If no such
message queue exists, it returns EINVAL.</LI>
<LI>  Checks whether the current task has the correct
permissions to access the message queue.</LI>
<LI>
<A NAME="rcvretry"></A> 
Starting from the first message in the message
waiting queue, invokes
<A HREF="#testmsg">testmsg()</A> to check whether
the message type matches the required type.  sys_msgrcv()
continues searching until a matched message is found or the whole
waiting queue is exhausted. If the search mode is
SEARCH_LESSEQUAL, then the first message on the queue
with the lowest type less than or equal to
<CODE>msgtyp</CODE> is searched.</LI>
<LI>  If a message is found, sys_msgrcv() performs
the following substeps:
<OL>
<LI>  If the message size is larger than
the desired size and <CODE>msgflg</CODE>
indicates no error allowed, unlocks the global
message queue spinlock and returns E2BIG.</LI>
<LI>  Removes the message from the message
waiting queue and updates the message queue
statistics.</LI>
<LI>  Wakes up all tasks sleeping on the senders
waiting queue. The removal of a message from
the queue in the previous step makes it possible
for one of the senders to progress. Goes to
the 
<A HREF="#laststep">last step</A></LI>
</OL>
</LI>
<LI>  If no message matching the receivers criteria is found
in the message waiting queue, then <CODE>msgflg</CODE>
is checked. If IPC_NOWAIT is set, then the global message
queue spinlock is unlocked and ENOMSG is returned. Otherwise,
the receiver is enqueued on the receiver waiting queue as
follows:
<OL>
<LI>  A 
<A HREF="#struct_msg_receiver">msg_receiver</A> data structure
<CODE>msr</CODE> is allocated and is
added to the head of waiting queue.</LI>
<LI>  The <CODE>r_tsk</CODE> field of <CODE>msr</CODE>
is set to current task.</LI>
<LI>  The <CODE>r_msgtype</CODE> and
<CODE>r_mode</CODE> fields are
initialized with the desired message type and
mode respectively.</LI>
<LI>  If <CODE>msgflg</CODE> indicates
MSG_NOERROR, then the r_maxsize field of
<CODE>msr</CODE> is set to be the
value of <CODE>msgsz</CODE> otherwise
it is set to be INT_MAX.</LI>
<LI>  The <CODE>r_msg</CODE> field
is initialized to indicate that
no message has been received yet.</LI>
<LI>  After the initialization is complete,
the status of the receiving task is set to
TASK_INTERRUPTIBLE, the global message queue
spinlock is unlocked, and schedule() is invoked.</LI>
</OL>
</LI>
<LI>  After the receiver is awakened,
the <CODE>r_msg</CODE> field of
<CODE>msr</CODE> is checked.  This field is used to
store the pipelined message or in the case of an error,
to store the error status.
If the <CODE>r_msg</CODE> field is filled
with the desired message, then go to the
<A HREF="#laststep">last step</A>  Otherwise,
the global message queue spinlock is locked again.</LI>
<LI>  After obtaining the spinlock,
the <CODE>r_msg</CODE> field is
re-checked to see if the message was received while
waiting for the spinlock. If the message has been
received, the 
<A HREF="#laststep">last step</A>
occurs.</LI>
<LI>  If the <CODE>r_msg</CODE> field remains
unchanged, then the task was
awakened in order to retry.  In this case,
<CODE>msr</CODE> is dequeued. If there is a
signal pending for the task, then the global message
queue spinlock is unlocked and EINTR is returned.
Otherwise, the function needs to go
<A HREF="#rcvretry">back</A> and retry.</LI>
<LI>  If the <CODE>r_msg</CODE> field shows
that an error occurred
while sleeping, the global message queue spinlock
is unlocked and the error is returned.</LI>
<LI>
<A NAME="laststep"></A> 
After validating that the address of the user buffer
<CODE>msp</CODE> is valid, message type is loaded
into the <CODE>mtype</CODE> field of
<CODE>msp</CODE>,and
<A HREF="#store_msg">store_msg()</A>
is invoked to copy the message contents to
the <CODE>mtext</CODE> field of
<CODE>msp</CODE>. Finally the memory for the message is
freed by function 
<A HREF="#free_msg">free_msg()</A>.</LI>
</OL>
</P>
<H3><A NAME="datastructs"></A> Message Specific Structures</H3>

<P>Data structures for message queues are defined in msg.c.</P>
<H3><A NAME="struct_msg_queue"></A> struct msg_queue</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/* one msq_queue structure for each present queue on the system */
struct msg_queue {
        struct kern_ipc_perm q_perm;
        time_t q_stime;                 /* last msgsnd time */
        time_t q_rtime;                 /* last msgrcv time */
        time_t q_ctime;                 /* last change time */
        unsigned long q_cbytes;         /* current number of bytes on queue */
        unsigned long q_qnum;           /* number of messages in queue */
        unsigned long q_qbytes;         /* max number of bytes on queue */
        pid_t q_lspid;                  /* pid of last msgsnd */
        pid_t q_lrpid;                  /* last receive pid */

        struct list_head q_messages;
        struct list_head q_receivers;
        struct list_head q_senders;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_msg_msg"></A> struct msg_msg</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/* one msg_msg structure for each message */
struct msg_msg {
        struct list_head m_list;
        long  m_type;
        int m_ts;           /* message text size */
        struct msg_msgseg* next;
        /* the actual message follows immediately */
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_msg_msgseg"></A> struct msg_msgseg</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/* message segment for each message */
struct msg_msgseg {
        struct msg_msgseg* next;
        /* the next part of the message follows immediately */
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_msg_sender"></A> struct msg_sender</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/* one msg_sender for each sleeping sender */
struct msg_sender {
        struct list_head list;
        struct task_struct* tsk;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_msg_receiver"></A> struct msg_receiver</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/* one msg_receiver structure for each sleeping receiver */
struct msg_receiver {
        struct list_head r_list;
        struct task_struct* r_tsk;

        int r_mode;
        long r_msgtype;
        long r_maxsize;

        struct msg_msg* volatile r_msg;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_msqid64_ds"></A> struct msqid64_ds</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct msqid64_ds {
        struct ipc64_perm msg_perm;
        __kernel_time_t msg_stime;      /* last msgsnd time */
        unsigned long   __unused1;
        __kernel_time_t msg_rtime;      /* last msgrcv time */
        unsigned long   __unused2;
        __kernel_time_t msg_ctime;      /* last change time */
        unsigned long   __unused3;
        unsigned long  msg_cbytes;      /* current number of bytes on queue */
        unsigned long  msg_qnum;        /* number of messages in queue */
        unsigned long  msg_qbytes;      /* max number of bytes on queue */
        __kernel_pid_t msg_lspid;       /* pid of last msgsnd */
        __kernel_pid_t msg_lrpid;       /* last receive pid */
        unsigned long  __unused4;
        unsigned long  __unused5;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_msqid_ds"></A> struct msqid_ds</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
 struct msqid_ds {
        struct ipc_perm msg_perm;
        struct msg *msg_first;          /* first message on queue,unused  */
        struct msg *msg_last;           /* last message in queue,unused */
        __kernel_time_t msg_stime;      /* last msgsnd time */
        __kernel_time_t msg_rtime;      /* last msgrcv time */
        __kernel_time_t msg_ctime;      /* last change time */
        unsigned long  msg_lcbytes;     /* Reuse junk fields for 32 bit */
        unsigned long  msg_lqbytes;     /* ditto */
        unsigned short msg_cbytes;      /* current number of bytes on queue */
        unsigned short msg_qnum;        /* number of messages in queue */
        unsigned short msg_qbytes;      /* max number of bytes on queue */
        __kernel_ipc_pid_t msg_lspid;   /* pid of last msgsnd */
        __kernel_ipc_pid_t msg_lrpid;   /* last receive pid */
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="msg_setbuf"></A> msg_setbuf</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct msq_setbuf {
        unsigned long   qbytes;
        uid_t           uid;
        gid_t           gid;
        mode_t          mode;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="msgfuncs"></A> Message Support Functions</H3>


<H3><A NAME="newque"></A> newque()</H3>

<P>newque() allocates the memory for a new message queue
descriptor (
<A HREF="#struct_msg_queue">struct msg_queue</A>)
and then calls 
<A HREF="#ipc_addid">ipc_addid()</A>, which
reserves a message queue array entry for the new message queue
descriptor.  The message queue descriptor is initialized as
follows:</P>
<P>
<UL>
<LI>  The 
<A HREF="#struct_kern_ipc_perm">kern_ipc_perm</A>
structure is initialized.</LI>
<LI>  The <CODE>q_stime</CODE> and <CODE>q_rtime</CODE> fields of the message
queue descriptor are initialized as 0. The <CODE>q_ctime</CODE>
field is set to be CURRENT_TIME.</LI>
<LI>  The maximum number of bytes allowed in this
queue message (<CODE>q_qbytes</CODE>) is set to be MSGMNB,
and the number of bytes currently used by the queue
(<CODE>q_cbytes</CODE>) is initialized as 0.</LI>
<LI>  The message waiting queue (<CODE>q_messages</CODE>),
the receiver waiting queue (<CODE>q_receivers</CODE>),
and the sender waiting queue (<CODE>q_senders</CODE>)
are each initialized as empty.</LI>
</UL>
</P>
<P>All the operations following the call to
<A HREF="#ipc_addid">ipc_addid()</A> are
performed while holding the global message queue spinlock.
After unlocking the spinlock, newque() calls msg_buildid(),
which maps directly to 
<A HREF="#ipc_buildid">ipc_buildid()</A>.
<A HREF="#ipc_buildid">ipc_buildid()</A>
uses the index of the message queue descriptor to create a unique
message queue ID that is then returned to the caller of newque().</P>
<H3><A NAME="freeque"></A> freeque()</H3>

<P>When a message queue is going to be removed, the freeque() function is
called.  This function assumes that the global message queue spinlock
is already locked by the calling function.  It frees all kernel
resources associated with that message queue. First, it calls
<A HREF="#func_ipc_rmid">ipc_rmid()</A> (via msg_rmid())
to remove the message queue descriptor from the array of global
message queue descriptors. Then it calls
<A HREF="#expunge_all">expunge_all</A> to wake up
all receivers and 
<A HREF="#ss_wakeup">ss_wakeup()</A>
to wake up all senders sleeping on this message queue. Later
the global message queue spinlock is released.
All messages stored in this message queue are freed and the
memory for the message queue descriptor is freed.</P>
<H3><A NAME="ss_wakeup"></A> ss_wakeup()</H3>

<P>ss_wakeup() wakes up all the tasks waiting in the given
message sender waiting queue. If this function is called by
<A HREF="#freeque">freeque()</A>, then all senders
in the queue are dequeued.</P>
<H3><A NAME="ss_add"></A> ss_add()</H3>

<P>ss_add() receives as parameters a message queue descriptor
and a message sender data structure.  It fills the
<CODE>tsk</CODE> field of the message sender data
structure with the current process, changes the status of
current process to TASK_INTERRUPTIBLE,
then inserts the message sender data structure at the head of
the sender waiting queue of the given message queue.</P>
<H3><A NAME="ss_del"></A> ss_del()</H3>

<P>If the given message sender data structure
(<CODE>mss</CODE>) is still in the associated sender
waiting queue, then ss_del() removes
<CODE>mss</CODE> from the queue.</P>
<H3><A NAME="expunge_all"></A> expunge_all()</H3>

<P>expunge_all() receives as parameters a message queue
descriptor(<CODE>msq</CODE>) and an integer value
(<CODE>res</CODE>) indicating the reason for waking up the
receivers. For each sleeping receiver associated with
<CODE>msq</CODE>, the <CODE>r_msg</CODE>
field is set to the indicated
wakeup reason (<CODE>res</CODE>), and the associated receiving
task is awakened. This function is called when a message queue is
removed or a message control operation has been performed.</P>
<H3><A NAME="load_msg"></A> load_msg()</H3>

<P>When a process sends a message, the
<A HREF="#sys_msgsnd">sys_msgsnd()</A> function
first invokes the load_msg() function to load the message
from user space to kernel space.  The message is represented in
kernel memory as a linked list of data blocks. Associated with
the first data block is a 
<A HREF="#struct_msg_msg">msg_msg</A>
structure that describes the overall message. The datablock
associated with the msg_msg structure is limited to a size of
DATA_MSG_LEN. The data block and the structure are allocated in one
contiguous memory block that can be as large as one page in memory.
If the full message will not fit into this first data block, then
additional data blocks are allocated and are organized into a
linked list.  These additional data blocks are limited to a size
of DATA_SEG_LEN, and each include an associated
<A HREF="#struct_msg_msgseg">msg_msgseg)</A> structure. The
msg_msgseg structure and the associated data block are allocated in
one contiguous memory block that can be as large as one page in
memory.  This function returns the address of the new
<A HREF="#struct_msg_msg">msg_msg</A> structure on success.</P>
<H3><A NAME="store_msg"></A> store_msg()</H3>

<P>The store_msg() function is called by
<A HREF="#sys_msgrcv">sys_msgrcv()</A> to reassemble a received
message into the user space buffer provided by the caller. The data
described by the 
<A HREF="#struct_msg_msg">msg_msg</A>
structure and any 
<A HREF="#struct_msg_msgseg">msg_msgseg</A>
structures are sequentially copied to the user space buffer.</P>
<H3><A NAME="free_msg"></A> free_msg()</H3>

<P>The free_msg() function releases the memory for a message
data structure 
<A HREF="#struct_msg_msg">msg_msg</A>,
and the message segments.</P>
<H3><A NAME="convert_mode"></A> convert_mode()</H3>

<P>convert_mode() is called by 
<A HREF="#sys_msgrcv">sys_msgrcv()</A>.
It receives as parameters the address of the specified message
type (<CODE>msgtyp</CODE>) and a flag (<CODE>msgflg</CODE>).
It returns the search mode to the caller based on the value of
<CODE>msgtyp</CODE> and <CODE>msgflg</CODE>.  If
<CODE>msgtyp</CODE> is null, then SEARCH_ANY is returned.
If <CODE>msgtyp</CODE> is less than 0, then <CODE>msgtyp</CODE> is
set to it's absolute value and SEARCH_LESSEQUAL is returned.
If MSG_EXCEPT is specified in <CODE>msgflg</CODE>, then SEARCH_NOTEQUAL is returned.
Otherwise SEARCH_EQUAL is returned.</P>
<H3><A NAME="testmsg"></A> testmsg()</H3>

<P>The testmsg() function checks whether a message meets the
criteria specified by the receiver.  It returns 1 if one of the
following conditions is true:</P>
<P>
<UL>
<LI>  The search mode indicates searching any message (SEARCH_ANY).</LI>
<LI>  The search mode is SEARCH_LESSEQUAL and the message type
is less than or equal to desired type.</LI>
<LI>  The search mode is SEARCH_EQUAL and the message type is
the same as desired type.</LI>
<LI>  Search mode is SEARCH_NOTEQUAL and the message type is
not equal to the specified type.</LI>
</UL>
</P>
<H3><A NAME="pipelined_send"></A> pipelined_send()</H3>

<P>pipelined_send() allows a process to directly send a message
to a waiting receiver rather than deposit the message in the
associated message waiting queue. The
<A HREF="#testmsg">testmsg()</A> function is
invoked to find the first receiver which is waiting for the
given message. If found, the waiting receiver is removed from
the receiver waiting queue, and the associated receiving task is
awakened. The message is stored in the <CODE>r_msg</CODE>
field of the receiver, and 1 is returned. In the case where no
receiver is waiting for the message, 0 is returned.</P>
<P>In the process of searching for a receiver, potential
receivers may be found which have requested a size that is too small
for the given message. Such receivers are removed from the queue,
and are awakened with an error status of E2BIG, which is stored in the
<CODE>r_msg</CODE> field. The search then continues until
either a valid receiver is found, or the queue is exhausted.</P>
<H3><A NAME="copy_msqid_to_user"></A> copy_msqid_to_user()</H3>

<P>copy_msqid_to_user() copies the contents of a kernel buffer to
the user buffer.  It receives as parameters a user buffer, a
kernel buffer of type
<A HREF="#struct_msqid64_ds">msqid64_ds</A>, and a
version flag indicating
the new IPC version vs. the old IPC version.  If the version
flag equals IPC_64, then copy_to_user() is invoked to copy from
the kernel buffer to the user buffer directly.  Otherwise a
temporary buffer of type struct msqid_ds is initialized, and the
kernel data is translated to this temporary buffer.  Later
copy_to_user() is called to copy the contents of the temporary
buffer to the user buffer.</P>
<H3><A NAME="copy_msqid_from_user"></A> copy_msqid_from_user()</H3>

<P>The function copy_msqid_from_user() receives as parameters
a kernel message buffer of type struct msq_setbuf, a user buffer
and a version flag indicating the new IPC version vs. the old IPC
version.  In the case of the new IPC version, copy_from_user()
is called to copy the contents of the user buffer
to a temporary buffer of type 
<A HREF="#struct_msqid64_ds">msqid64_ds</A>.
Then, the <CODE>qbytes</CODE>,<CODE>uid</CODE>, <CODE>gid</CODE>,
and <CODE>mode</CODE> fields of the kernel buffer are
filled with the values of the
corresponding fields from the temporary buffer.  In the case of the
old IPC version, a temporary buffer of type struct
<A HREF="#struct_msqid_ds">msqid_ds</A> is used instead.</P>
<H2><A NAME="sharedmem"></A> <A NAME="ss5.3">5.3 Shared Memory</A>
</H2>


<H3><A NAME="Shared_Memory_System_Call_Interfaces"></A> Shared Memory System Call Interfaces</H3>


<H3><A NAME="sys_shmget"></A> sys_shmget()</H3>

<P>The entire call to sys_shmget() is protected by the
global shared memory semaphore.</P>
<P>In the case where a new shared memory segment must
be created, the 
<A HREF="#newseg">newseg()</A> function is called to create
and initialize a new shared memory segment.  The ID of
the new segment is returned to the caller.</P>
<P>In the case where a key value is provided for an
existing shared memory segment, the corresponding index
in the shared memory descriptors array is looked up, and
the parameters and permissions of the caller are verified
before returning the shared memory segment ID. The look up
operation and verification are performed while the global
shared memory spinlock is held.</P>
<H3><A NAME="sys_shmctl"></A> sys_shmctl()</H3>


<H3><A NAME="IPC_INFO"></A> IPC_INFO</H3>

<P>A temporary 
<A HREF="#struct_shminfo64">shminfo64</A>
buffer is loaded with system-wide
shared memory parameters and is copied out to user space for
access by the calling application.</P>
<H3><A NAME="SHM_INFO"></A> SHM_INFO</H3>

<P>The global shared memory semaphore and the global shared
memory spinlock are held while gathering system-wide statistical
information for shared memory.  The
<A HREF="#shm_get_stat">shm_get_stat()</A> function is called
to calculate both the number of shared memory pages that are
resident in memory and the number of shared memory pages that are
swapped out. Other statistics include the total number of shared
memory pages and the number of shared memory segments in use.
The counts of <CODE>swap_attempts</CODE> and <CODE>swap_successes</CODE>
are hard-coded to zero. These statistics are stored in a temporary
<A HREF="#struct_shm_info">shm_info</A> buffer and copied out
to user space for the calling application.</P>
<H3><A NAME="SHM_STAT_IPC_STAT"></A> SHM_STAT, IPC_STAT</H3>

<P>For SHM_STAT and IPC_STATA, a temporary buffer of type
<A HREF="#struct_shmid64_ds">struct shmid64_ds</A> is
initialized, and the global shared memory spinlock is locked.</P>
<P>For the SHM_STAT case, the shared memory segment ID parameter is
expected to be a straight index (i.e. 0 to n where n is the
number of shared memory IDs in the system). After validating
the index, 
<A HREF="#ipc_buildid">ipc_buildid()</A>
is called (via shm_buildid()) to
convert the index into a shared memory ID. In the passing case
of SHM_STAT, the shared memory ID will be the return value.
Note that this is an undocumented feature, but is maintained
for the ipcs(8) program.</P>
<P>For the IPC_STAT case, the shared memory segment ID parameter is
expected to be an ID that was generated by a call to
<A HREF="#sys_shmget">shmget()</A>.
The ID is validated before proceeding. In the passing case of
IPC_STAT, 0 will be the return value.</P>
<P>For both SHM_STAT and IPC_STAT, the access permissions of
the caller are verified. The desired statistics are loaded into
the temporary buffer and then copied out to the calling
application.</P>
<H3><A NAME="SHM_LOCK_SHM_UNLOCK"></A> SHM_LOCK, SHM_UNLOCK</H3>

<P>After validating access permissions, the global shared
memory spinlock is locked, and the shared memory segment ID
is validated.  For both SHM_LOCK and SHM_UNLOCK,
<A HREF="#shmem_lock">shmem_lock()</A>
is called to perform the function. The parameters for
<A HREF="#shmem_lock">shmem_lock()</A>
identify the function to be performed.</P>
<H3><A NAME="IPC_RMID"></A> IPC_RMID</H3>

<P>During IPC_RMID the global shared memory semaphore and
the global shared memory spinlock are held throughout this
function. The Shared Memory ID is validated, and then if
there are no current attachments, 
<A HREF="#shm_destroy">shm_destroy()</A>
is called to destroy the shared memory segment.
Otherwise, the SHM_DEST flag is set to mark it for destruction,
and the IPC_PRIVATE flag is set to prevent other processes from
being able to reference the shared memory ID.</P>
<H3><A NAME="IPC_SET"></A> IPC_SET</H3>

<P>After validating the shared memory segment ID and the user
access permissions, the <CODE>uid</CODE>, <CODE>gid</CODE>, and <CODE>mode</CODE> flags of the
shared memory segment are updated with the user data. The
<CODE>shm_ctime</CODE> field is also updated.  These changes are made
while holding the global shared memory semaphore and the
global share memory spinlock.</P>
<H3><A NAME="sys_shmat"></A> sys_shmat()</H3>

<P>sys_shmat() takes as parameters, a shared memory segment ID,
an address at which the shared memory segment should be
attached(<CODE>shmaddr</CODE>), and flags which will be described below.</P>
<P>If <CODE>shmaddr</CODE> is non-zero, and the SHM_RND flag is
specified, then <CODE>shmaddr</CODE> is rounded down to a multiple of
SHMLBA. If <CODE>shmaddr</CODE> is not a multiple of SHMLBA and SHM_RND
is not specified, then EINVAL is returned.</P>
<P>The access permissions of the caller are validated and
the <CODE>shm_nattch</CODE> field for the shared memory segment is
incremented. Note that this increment guarantees that the
attachment count is non-zero and prevents the shared memory
segment from being destroyed during the process of attaching
to the segment.  These operations are performed while holding the
global shared memory spinlock.</P>
<P>The do_mmap() function is called to create a virtual memory
mapping to the shared memory segment pages. This is done while
holding the <CODE>mmap_sem</CODE> semaphore of the current task. The
MAP_SHARED flag is passed to do_mmap(). If an address was
provided by the caller, then the MAP_FIXED flag is also passed
to do_mmap(). Otherwise, do_mmap() will select the virtual
address at which to map the shared memory segment.</P>
<P>NOTE 
<A HREF="#shm_inc">shm_inc()</A> will be invoked within the do_mmap()
function call via the <CODE>shm_file_operations</CODE> structure. This
function is called to set the PID, to set the current time, and
to increment the number of attachments to this shared memory
segment.</P>
<P>After the call to do_mmap(), the global shared memory
semaphore and the global shared memory spinlock are both
obtained.  The attachment count is then decremented.  The the net
change to the attachment count is 1 for a call
to shmat() because of the call to 
<A HREF="#shm_inc">shm_inc()</A>. If, after
decrementing the attachment count, the resulting count is found
to be zero, and if the segment is marked for destruction
(SHM_DEST), then 
<A HREF="#shm_destroy">shm_destroy()</A> is
called to release the shared memory segment resources.</P>
<P>Finally, the virtual address at which the shared memory is
mapped is returned to the caller at the user specified address.
If an error code had been returned by do_mmap(), then this
failure code is passed on as the return value for the system call.</P>
<H3><A NAME="sys_shmdt"></A> sys_shmdt()</H3>

<P>The global shared memory semaphore is held while performing
sys_shmdt(). The <CODE>mm_struct</CODE> of the current
process is searched for the <CODE>vm_area_struct</CODE> associated with
the shared memory address. When it is found, do_munmap() is
called to undo the virtual address mapping for the shared memory segment.</P>
<P>Note also that do_munmap() performs a call-back to
<A HREF="#shm_close">shm_close()</A>,
which performs the shared-memory book keeping functions, and
releases the shared memory segment resources if there are no other
attachments.</P>
<P>sys_shmdt() unconditionally returns 0.</P>
<H3><A NAME="shm_structures"></A> Shared Memory Support Structures</H3>


<H3><A NAME="struct_shminfo64"></A> struct shminfo64</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct shminfo64 {
        unsigned long   shmmax;
        unsigned long   shmmin;
        unsigned long   shmmni;
        unsigned long   shmseg;
        unsigned long   shmall;
        unsigned long   __unused1;
        unsigned long   __unused2;
        unsigned long   __unused3;
        unsigned long   __unused4;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_shm_info"></A> struct shm_info</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct shm_info {
        int used_ids;
        unsigned long shm_tot;  /* total allocated shm */
        unsigned long shm_rss;  /* total resident shm */
        unsigned long shm_swp;  /* total swapped shm */
        unsigned long swap_attempts;
        unsigned long swap_successes;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_shmid_kernel"></A> struct shmid_kernel</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct shmid_kernel /* private to the kernel */
{
        struct kern_ipc_perm    shm_perm;
        struct file *           shm_file;
        int                     id;
        unsigned long           shm_nattch;
        unsigned long           shm_segsz;
        time_t                  shm_atim;
        time_t                  shm_dtim;
        time_t                  shm_ctim;
        pid_t                   shm_cprid;
        pid_t                   shm_lprid;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_shmid64_ds"></A> struct shmid64_ds</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct shmid64_ds {
        struct ipc64_perm       shm_perm;       /* operation perms */
        size_t                  shm_segsz;      /* size of segment (bytes) */
        __kernel_time_t         shm_atime;      /* last attach time */
        unsigned long           __unused1;
        __kernel_time_t         shm_dtime;      /* last detach time */
        unsigned long           __unused2;
        __kernel_time_t         shm_ctime;      /* last change time */
        unsigned long           __unused3;
        __kernel_pid_t          shm_cpid;       /* pid of creator */
        __kernel_pid_t          shm_lpid;       /* pid of last operator */
        unsigned long           shm_nattch;     /* no. of current attaches */
        unsigned long           __unused4;
        unsigned long           __unused5;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_shmem_inode_info"></A> struct shmem_inode_info</H3>

<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct shmem_inode_info {
        spinlock_t      lock;
        unsigned long   max_index;
        swp_entry_t     i_direct[SHMEM_NR_DIRECT]; /* for the first blocks */
        swp_entry_t   **i_indirect; /* doubly indirect blocks */
        unsigned long   swapped;
        int             locked;     /* into memory */
        struct list_head        list;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="shm_primitives"></A> Shared Memory Support Functions</H3>


<H3><A NAME="newseg"></A> newseg()</H3>

<P>The newseg() function is called when a new shared memory
segment needs to be created.  It acts on three parameters for
the new segment the key, the flag, and the size.  After
validating that the size of the shared memory segment to be
created is between SHMMIN and SHMMAX and that the total number
of shared memory segments does not exceed SHMALL, it allocates
a new shared memory segment descriptor.
The 
<A HREF="#shmem_file_setup">shmem_file_setup()</A>
function is invoked later to create an unlinked file of type
tmpfs.  The returned file pointer is saved in the <CODE>shm_file</CODE> field
of the associated shared memory segment descriptor. The files
size is set to be the same as the size of the segment.  The
new shared memory segment descriptor is initialized and inserted
into the global IPC shared memory descriptors array.  The shared
memory segment ID is created by shm_buildid()
(via 
<A HREF="#ipc_buildid">ipc_buildid()</A>).
This segment ID is saved in the <CODE>id</CODE> field of the shared memory
segment descriptor, as well as in the <CODE>i_ino</CODE> field of the associated
inode.  In addition, the address of the shared memory operations
defined in structure <CODE>shm_file_operation</CODE> is stored in the associated
file.  The value of the global variable <CODE>shm_tot</CODE>, which indicates
the total number of shared memory segments system wide, is also
increased to reflect this change.  On success, the segment ID is
returned to the caller application.</P>
<H3><A NAME="shm_get_stat"></A> shm_get_stat()</H3>

<P>shm_get_stat() cycles through all of the shared memory
structures, and calculates the total number of memory pages in
use by shared memory and the total number of shared memory pages
that are swapped out. There is a file structure and an inode
structure for each shared memory segment.  Since the required
data is obtained via the inode, the spinlock for each inode
structure that is accessed is locked and unlocked in sequence.</P>
<H3><A NAME="shmem_lock"></A> shmem_lock()</H3>

<P>shmem_lock() receives as parameters a pointer to the
shared memory segment descriptor and a flag indicating
lock vs. unlock.The locking state of the shared memory
segment is stored in an associated inode. This state is compared
with the desired locking state; shmem_lock() simply returns if they match.</P>
<P>While holding the semaphore of the associated inode, the
locking state of the inode is set. The following list of items
occur for each page in the shared memory segment:
<UL>
<LI>  find_lock_page() is called to lock the page (setting
PG_locked) and to increment the reference count of the page.
Incrementing the reference count assures that the shared
memory segment remains locked in memory throughout this
operation.</LI>
<LI>  If the desired state is locked, then PG_locked is cleared,
but the reference count remains incremented.</LI>
<LI>  If the desired state is unlocked, then the reference count
is decremented twice once for the current reference, and once
for the existing reference which caused the page to remain
locked in memory. Then PG_locked is cleared.</LI>
</UL>
</P>
<H3><A NAME="shm_destroy"></A> shm_destroy()</H3>

<P>During shm_destroy() the total number of shared memory pages
is adjusted to account for the removal of the shared memory segment.
<A HREF="#func_ipc_rmid">ipc_rmid()</A> is called
(via shm_rmid()) to remove the Shared Memory ID. 
<A HREF="#shmem_lock">shmem_lock</A> is
called to unlock the shared memory pages, effectively decrementing
the reference counts to zero for each page. fput() is called to
decrement the usage counter <CODE>f_count</CODE> for the associated file object,
and if necessary, to release the file object resources.  kfree() is
called to free the shared memory segment descriptor.</P>
<H3><A NAME="shm_inc"></A> shm_inc()</H3>

<P>shm_inc() sets the PID, sets the current time, and increments
the number of attachments for the given shared memory segment.
These operations are performed while holding the global shared
memory spinlock.</P>
<H3><A NAME="shm_close"></A> shm_close()</H3>

<P>shm_close() updates the <CODE>shm_lprid</CODE> and the <CODE>shm_dtim</CODE> fields
and decrements the number of attached shared memory segments. If
there are no other attachments to the shared memory segment,
then 
<A HREF="#shm_destroy">shm_destroy()</A> is called to
release the shared memory segment resources. These operations are
all performed while holding both the global shared memory semaphore
and the global  shared memory spinlock.</P>
<H3><A NAME="shmem_file_setup"></A> shmem_file_setup()</H3>

<P>The function shmem_file_setup() sets up an unlinked file living
in the tmpfs file system with the given name and size.  If there
are enough systen memory resource for this file, it creates a new
dentry under the mount root of tmpfs, and allocates a new file
descriptor and a new inode object of tmpfs type.  Then it associates
the new dentry object with the new inode object by calling
d_instantiate() and saves the address of the dentry object in the
file descriptor. The <CODE>i_size</CODE> field of the inode object is set to
be the file size and the <CODE>i_nlink</CODE> field is set to be 0 in order to
mark the inode unlinked.  Also, shmem_file_setup() stores the
address of the <CODE>shmem_file_operations</CODE> structure in the <CODE>f_op</CODE> field,
and initializes <CODE>f_mode</CODE> and <CODE>f_vfsmnt</CODE> fields of the file descriptor
properly.  The function shmem_truncate() is called to complete the
initialization of the inode object. On success, shmem_file_setup()
returns the new file descriptor.</P>
<H2><A NAME="ipc_primitives"></A> <A NAME="ss5.4">5.4 Linux IPC Primitives</A>
</H2>


<H3><A NAME="Generic_Linux_IPC_Primitives_used_with_Semaphores_Messages_and_Shared_Memory"></A> Generic Linux IPC Primitives used with Semaphores, Messages,and Shared Memory </H3>

<P>The semaphores, messages, and shared memory mechanisms of Linux
are built on a set of common primitives. These primitives are described in the sections below.</P>

<H3><A NAME="ipc_alloc"></A> ipc_alloc()</H3>

<P>If the memory allocation is greater than PAGE_SIZE, then
vmalloc() is used to allocate memory. Otherwise, kmalloc() is
called with GFP_KERNEL to allocate the memory.</P>
<H3><A NAME="ipc_addid"></A> ipc_addid()</H3>

<P>When a new semaphore set, message queue, or shared memory
segment is added,  ipc_addid() first calls 
<A HREF="#grow_ary">grow_ary()</A> to
insure that the size of the corresponding descriptor array is
sufficiently large for the system maximum.  The array of descriptors
is searched for the first unused element. If an unused element
is found, the count of descriptors which are in use is incremented.
The 
<A HREF="#struct_kern_ipc_perm">kern_ipc_perm</A> structure for the new resource descriptor
is then initialized, and the array index for the new descriptor
is returned. When ipc_addid() succeeds, it returns with the global
spinlock for the given IPC type locked.</P>
<H3><A NAME="func_ipc_rmid"></A> ipc_rmid()</H3>

<P>ipc_rmid() removes the IPC descriptor from the global
descriptor array of the IPC type, updates the count of IDs which
are in use, and adjusts the maximum ID in the corresponding
descriptor array if necessary. A pointer to  the IPC
descriptor associated with given IPC ID is returned.</P>
<H3><A NAME="ipc_buildid"></A> ipc_buildid()</H3>

<P>ipc_buildid() creates a unique ID to be associated with
each descriptor within a given IPC type. This ID is created at
the time a new IPC element is added (e.g. a new shared memory
segment or a new semaphore set).  The IPC ID converts
easily into the corresponding descriptor array index. Each
IPC type maintains a sequence number which is incremented
each time a descriptor is added.  An ID is created by
multiplying the sequence number with SEQ_MULTIPLIER and adding
the product to the descriptor array index. The sequence number
used in creating a particular IPC ID is then stored in the
corresponding descriptor. The existence of the sequence number
makes it possible to detect the use of a stale IPC ID.</P>
<H3><A NAME="ipc_checkid"></A> ipc_checkid()</H3>

<P>ipc_checkid() divides the given IPC ID by the SEQ_MULTIPLIER
and compares the quotient with the seq value saved corresponding
descriptor.  If they are equal, then the IPC ID is considered to
be valid and 1 is returned.  Otherwise, 0 is returned.</P>
<H3><A NAME="grow_ary"></A> grow_ary()</H3>

<P>grow_ary() handles the possibility that the maximum
(tunable) number of IDs for a given IPC type can be dynamically
changed. It enforces the current maximum limit so that it is no
greater than the permanent system limit (IPCMNI) and adjusts it down
if necessary. It also insures that the existing descriptor array
is large enough.  If the existing array size is sufficiently large,
then the current maximum limit is returned.  Otherwise, a new larger
array is allocated, the old array is copied into the new array,
and the old array is freed.  The corresponding global
spinlock is held when updating the descriptor array for the
given IPC type.</P>
<H3><A NAME="ipc_findkey"></A> ipc_findkey()</H3>

<P>ipc_findkey() searches through the descriptor array of
the specified 
<A HREF="#struct_ipc_ids">ipc_ids</A> object,
and searches for the specified key. Once found, the index of
the corresponding descriptor is returned. If the key is not found,
then -1 is returned.</P>
<H3><A NAME="ipcperms"></A> ipcperms()</H3>

<P>ipcperms() checks the user, group, and other permissions
for access to the IPC resources. It returns 0 if permission
is granted and -1 otherwise.</P>
<H3><A NAME="ipc_lock"></A> ipc_lock()</H3>

<P>ipc_lock() takes an IPC ID as one of its parameters.
It locks the global spinlock for the given IPC type, and
returns a pointer to the descriptor corresponding to the
specified IPC ID.</P>
<H3><A NAME="ipc_unlock"></A> ipc_unlock()</H3>

<P>ipc_unlock() releases the global spinlock for the indicated IPC
type.</P>
<H3><A NAME="ipc_lockall"></A> ipc_lockall()</H3>

<P>ipc_lockall() locks the global spinlock for the given
IPC mechanism (i.e. shared memory, semaphores, and messaging).</P>
<H3><A NAME="ipc_unlockall"></A> ipc_unlockall()</H3>

<P>ipc_unlockall() unlocks the global spinlock for the given
IPC mechanism (i.e. shared memory, semaphores, and messaging).</P>
<H3><A NAME="ipc_get"></A> ipc_get()</H3>

<P>ipc_get() takes a pointer to a particular IPC type
(i.e. shared memory, semaphores, or message queues) and a
descriptor ID, and returns a pointer to the corresponding
IPC descriptor.  Note that although the descriptors for each
IPC type are of different data types, the common
<A HREF="#struct_kern_ipc_perm">kern_ipc_perm</A>
structure type is embedded as the first entity in every case.
The ipc_get() function returns this common data type. The expected
model is that ipc_get() is called through a wrapper function
(e.g. shm_get()) which casts the data type to the correct
descriptor data type.</P>
<H3><A NAME="ipc_parse_version"></A> ipc_parse_version()</H3>

<P>ipc_parse_version() removes the IPC_64 flag from the command
if it is present and returns either IPC_64 or IPC_OLD.</P>
<H3><A NAME="ipc_structures"></A> Generic IPC Structures used with Semaphores, Messages, and Shared Memory</H3>

<P>The semaphores, messages, and shared memory mechanisms all make
use of the following common structures:</P>

<H3><A NAME="struct_kern_ipc_perm"></A> struct kern_ipc_perm</H3>

<P>Each of the IPC descriptors has a data object of this type
as the first element. This makes it possible to access any
descriptor from any of the generic IPC functions using a pointer
of this data type.</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
/* used by in-kernel data structures */
struct kern_ipc_perm {
    key_t key;
    uid_t uid;
    gid_t gid;
    uid_t cuid;
    gid_t cgid;
    mode_t mode;
    unsigned long seq;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_ipc_ids"></A> struct ipc_ids</H3>

<P>The ipc_ids structure describes the common data for semaphores,
message queues, and shared memory. There are three global instances of
this data structure-- <CODE>semid_ds</CODE>,
<CODE>msgid_ds</CODE> and <CODE>shmid_ds</CODE>-- for
semaphores, messages and shared memory respectively. In each
instance, the <CODE>sem</CODE> semaphore is used to
protect access to the structure.
The <CODE>entries</CODE> field points to an IPC
descriptor array, and the
<CODE>ary</CODE> spinlock protects access to this array.  The
<CODE>seq</CODE> field is a global sequence number which will
be incremented when a new IPC resource is created.</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct ipc_ids {
    int size;
    int in_use;
    int max_id;
    unsigned short seq;
    unsigned short seq_max;
    struct semaphore sem;
    spinlock_t ary;
    struct ipc_id* entries;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
<H3><A NAME="struct_ipc_id"></A> struct ipc_id</H3>

<P>An array of struct ipc_id exists in each instance of
the 
<A HREF="#struct_ipc_ids">ipc_ids</A> structure.
The array is dynamically allocated and may be replaced with
larger array by 
<A HREF="#grow_ary">grow_ary()</A>
as required. The array is
sometimes referred to as the descriptor array, since the
<A HREF="#struct_kern_ipc_perm">kern_ipc_perm</A> data
type is used as the common descriptor data type by the IPC generic
functions.</P>
<P>
<BLOCKQUOTE><CODE>
<HR>
<PRE>
struct ipc_id {
    struct kern_ipc_perm* p;
};
</PRE>
<HR>
</CODE></BLOCKQUOTE>
</P>
</BODY>
</HTML>
